{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Classification and Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Feature extraction with frequencies\n",
    "\n",
    "Example\n",
    "\n",
    "Postive\n",
    "- I am happy because I am learning NLP\n",
    "- I am happy\n",
    "\n",
    "Negative\n",
    "- I am sad, I am not learning NLP\n",
    "- I am sad\n",
    "\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>PosFreq(1)</td>\n",
    "    <td>NegFreq(0)</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "For the word, \"I am sad, I am not learning NLP\"\n",
    "- Freq(w,1) = 3+3+1+1 = 8 (\"happy\" and \"because\" do not appear on the sentence)\n",
    "- Freq(w,0) = 3+3+1+1+2+1 = 8 (\"happy\" and \"because\" do not appear on the sentence)\n",
    "\n",
    "The feature vector becomes [1,8,11] \n",
    "- 1 is the bias\n",
    "- 8 is positive feature\n",
    "- 11 is negative feature\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "- Eliminate \"Stop words\" like \"and, is, are, at, has, for, a\"\n",
    "- Eliminate punctuations\n",
    "- Eliminate handles (starting with @) and URLs\n",
    "- Stemming word \"tune\" has three forms \"tune, tuned, tuning\". Stemmed word becomes \"tun\"\n",
    "- Convert all words to lowercase\n",
    "\n",
    "For the word, \"I am happy Because i am learning NLP @DeepLearning\", we do the preprocessing to get\n",
    "- [happy, learn, nlp]\n",
    "\n",
    "Then, feature vector becomes [1,4,2]\n",
    "- 1 is the bias\n",
    "- happy appears twice, learn and nlp appear once each, thus 4 is the positive feature\n",
    "- learn and nlp appear once each, thus 2 is the negative feature\n",
    "\n",
    "If we have lots of $m$ sentences to construct the feature vectors,\n",
    "$\\begin{bmatrix}\n",
    "    1 & X_{1}^{(1)} & X_{2}^{(1)} \\\\\n",
    "    1 & X_{1}^{(2)} & X_{2}^{(2)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    1 & X_{1}^{(m)} & X_{2}^{(m)}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random                              # pseudo-random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads sample twitter dataset. uncomment the line below if running on a local machine.\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))\n",
    "\n",
    "print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n",
    "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a figure with a custom size\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# labels for the two classes\n",
    "labels = 'Positives', 'Negative'\n",
    "\n",
    "# Sizes for each slide\n",
    "sizes = [len(all_positive_tweets), len(all_negative_tweets)] \n",
    "\n",
    "# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')  \n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our selected sample. Complex enough to exemplify each step\n",
    "tweet = all_positive_tweets[2277]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the stopwords from NLTK\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m' + tweet)\n",
    "print('\\033[94m')\n",
    "\n",
    "# remove old style retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "# remove hyperlinks\n",
    "tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "\n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "print(tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('\\033[92m' + tweet2)\n",
    "print('\\033[94m')\n",
    "\n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize tweets\n",
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "print()\n",
    "print('Tokenized string:')\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the english stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "print('Stop words\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweet_tokens)\n",
    "print('\\033[94m')\n",
    "\n",
    "tweets_clean = []\n",
    "\n",
    "for word in tweet_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stopwords_english and  # remove stopwords\n",
    "        word not in string.punctuation):  # remove punctuation\n",
    "        tweets_clean.append(word)\n",
    "\n",
    "print('removed stop words and punctuation:')\n",
    "print(tweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweets_clean)\n",
    "print('\\033[94m')\n",
    "\n",
    "# Instantiate stemming class\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Create an empty list to store the stems\n",
    "tweets_stem = [] \n",
    "\n",
    "for word in tweets_clean:\n",
    "    stem_word = stemmer.stem(word)  # stemming word\n",
    "    tweets_stem.append(stem_word)  # append to the list\n",
    "\n",
    "print('stemmed words:')\n",
    "print(tweets_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils611 import process_tweet # Import the process_tweet function\n",
    "\n",
    "# choose the same tweet\n",
    "tweet = all_positive_tweets[2277]\n",
    "\n",
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweet)\n",
    "print('\\033[94m')\n",
    "\n",
    "# call the imported function\n",
    "tweets_stem = process_tweet(tweet); # Preprocess a given tweet\n",
    "\n",
    "print('preprocessed tweet:')\n",
    "print(tweets_stem) # Print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                  # Python library for NLP\n",
    "from nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt              # visualization library\n",
    "import numpy as np                           # library for scientific computing and matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the stopwords for the process_tweet function\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# import our convenience functions\n",
    "from utils611 import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the lists of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "\n",
    "# let's see how many tweets we have\n",
    "print(\"Number of tweets: \", len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a numpy array representing labels of the tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(tweets, labels)\n",
    "\n",
    "# check data type\n",
    "print(f'type(freqs) = {type(freqs)}')\n",
    "\n",
    "# check length of the dictionary\n",
    "print(f'len(freqs) = {len(freqs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\n",
    "keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n",
    "        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',\n",
    "        'song', 'idea', 'power', 'play', 'magnific']\n",
    "\n",
    "# list representing our table of word counts.\n",
    "# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n",
    "data = []\n",
    "\n",
    "# loop through our selected words\n",
    "for word in keys:\n",
    "    \n",
    "    # initialize positive and negative counts\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    # retrieve number of positive counts\n",
    "    if (word, 1) in freqs:\n",
    "        pos = freqs[(word, 1)]\n",
    "        \n",
    "    # retrieve number of negative counts\n",
    "    if (word, 0) in freqs:\n",
    "        neg = freqs[(word, 0)]\n",
    "        \n",
    "    # append the word counts to the table\n",
    "    data.append([word, pos, neg])\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n",
    "x = np.log([x[1] + 1 for x in data])  \n",
    "\n",
    "# do the same for the negative counts\n",
    "y = np.log([x[2] + 1 for x in data]) \n",
    "\n",
    "# Plot a dot for each pair of words\n",
    "ax.scatter(x, y)  \n",
    "\n",
    "# assign axis labels\n",
    "plt.xlabel(\"Log Positive count\")\n",
    "plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "# Add the word as the label at the same position as you added the points just before\n",
    "for i in range(0, len(data)):\n",
    "    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
    "\n",
    "ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                         # NLP toolbox\n",
    "from os import getcwd\n",
    "import pandas as pd                 # Library for Dataframes \n",
    "from nltk.corpus import twitter_samples \n",
    "import matplotlib.pyplot as plt     # Library for visualization\n",
    "import numpy as np                  # Library for math functions\n",
    "\n",
    "from utils611 import process_tweet, build_freqs # Our functions for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "tweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \n",
    "labels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "train_pos  = all_positive_tweets[:4000]\n",
    "train_neg  = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "print(\"Number of tweets: \", len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('logistic_features.csv'); # Load a 3 columns csv file using pandas function\n",
    "data.head(10) # Print the first 10 data entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each feature is labeled as bias, positive and negative\n",
    "X = data[['bias', 'positive', 'negative']].values # Get only the numerical values of the dataframe\n",
    "Y = data['sentiment'].values; # Put in Y the corresponding labels or sentiments\n",
    "\n",
    "print(X.shape) # Print the shape of the X part\n",
    "print(X) # Print some rows of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [7e-08, 0.0005239, -0.00055517]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the samples using columns 1 and 2 of the matrix\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "colors = ['red', 'green']\n",
    "\n",
    "# Color based on the sentiment Y\n",
    "ax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\n",
    "plt.xlabel(\"Positive\")\n",
    "plt.ylabel(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation for the separation plane\n",
    "# It give a value in the negative axe as a function of a positive value\n",
    "# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n",
    "# s(pos, W) = (w0 - w1 * pos) / w2\n",
    "def neg(theta, pos):\n",
    "    return (-theta[0] - pos * theta[1]) / theta[2]\n",
    "\n",
    "# Equation for the direction of the sentiments change\n",
    "# We don't care about the magnitude of the change. We are only interested \n",
    "# in the direction. So this direction is just a perpendicular function to the \n",
    "# separation plane\n",
    "# df(pos, W) = pos * w2 / w1\n",
    "def direction(theta, pos):\n",
    "    return    pos * theta[2] / theta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the samples using columns 1 and 2 of the matrix\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "colors = ['red', 'green']\n",
    "\n",
    "# Color base on the sentiment Y\n",
    "ax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\n",
    "plt.xlabel(\"Positive\")\n",
    "plt.ylabel(\"Negative\")\n",
    "\n",
    "# Now lets represent the logistic regression model in this chart. \n",
    "maxpos = np.max(X[:,1])\n",
    "\n",
    "offset = 5000 # The pos value for the direction vectors origin\n",
    "\n",
    "# Plot a gray line that divides the 2 areas.\n",
    "ax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n",
    "\n",
    "# Plot a green line pointing to the positive direction\n",
    "ax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n",
    "# Plot a red line pointing to the negative direction\n",
    "ax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    num_rows, num_cols = x.shape\n",
    "    m = num_rows\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = (-1/m) * ( np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)) )\n",
    "                      \n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha/m) * np.dot(x.T, h-y)\n",
    "        \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        key_pos = (word,1.0)\n",
    "        count_pos = freqs[key_pos] if key_pos in freqs else 0\n",
    "        x[0,1] += count_pos\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        key_neg = (word,0.0)\n",
    "        count_neg = freqs[key_neg] if key_neg in freqs else 0\n",
    "        x[0,2] += count_neg\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    num_row, num_col = test_y.shape\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    y_hat_matrix = np.array(y_hat, ndmin=2).T\n",
    "    compare_result = (y_hat_matrix == test_y)\n",
    "    count_true = np.count_nonzero(compare_result)\n",
    "    \n",
    "    accuracy = count_true / num_row\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "### Bayes' rule\n",
    "- **assumes each word in sentence are independent from one another** \n",
    "\n",
    "Conditional probability\n",
    "- probability of B given that A happened OR\n",
    "- looking at elements of A, probability that they also being to B\n",
    "\n",
    "Bayes rule\n",
    "- $P(X|Y) = P(Y|X) \\times \\dfrac{P(X)}{P(Y)}$\n",
    "\n",
    "Table from before but add the total sum\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>PosFreq(1)</td>\n",
    "    <td>NegFreq(0)</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>N</td>\n",
    "    <td>13</td>\n",
    "    <td>12</td>\n",
    "<tr>\n",
    "</table>\n",
    "\n",
    "Compute conditional probabilities, for example\n",
    "- $P(I|Pos) = \\dfrac{3}{13}, P(I|Neg) = \\dfrac{3}{12}$\n",
    "\n",
    "Then, fill those conditional probabilties to a new table\n",
    "\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>Pos</td>\n",
    "    <td>Neg</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>0.24</td>\n",
    "    <td>0.25</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>0.24</td>\n",
    "    <td>0.25</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>0.15</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.17</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.17</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Sum</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Naive Bayes binary classification rule\n",
    "- $\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|POS)}{P({w_{i}}|NEG)}$\n",
    "\n",
    "For an example sentence \"I am happy today; I am learning\"\n",
    "- $\\dfrac{0.2}{0.2} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.14}{0.10} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.1}{0.1} = 1.4$\n",
    "\n",
    "### Laplacian smoothing\n",
    "- avoid problems of probabilities being $0$\n",
    "- $P(w_{i}|class) = \\dfrac{freq(w_{i},class)+1}{(N_{class}+V)}$\n",
    "    - $N_{class}$ : frequency of all words in class\n",
    "    - $V$ : number of unique words in vocabulary\n",
    "- for example, $P(I|POS) = \\dfrac{3+1}{13+8} = 0.19$\n",
    "- then, the table becomes\n",
    "\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>Pos</td>\n",
    "    <td>Neg</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>0.19</td>\n",
    "    <td>0.20</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>0.19</td>\n",
    "    <td>0.20</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>0.14</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.05</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.15</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.15</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Sum</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### Log likelihood\n",
    "\n",
    "To do inference, we can compute\n",
    "- $\\dfrac{P(pos)}{P(neg)}\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|pos)}{P({w_{i}}|neg)} \\gt 1$\n",
    "\n",
    "To avoid numerical overflow as $m$ gets larger, we introduce \"log\" such that\n",
    "- $\\log\\left(\\dfrac{P(pos)}{P(neg)}\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|pos)}{P({w_{i}}|neg)}\\right) = \\log\\left(\\dfrac{P(pos)}{P(neg)}\\right) + \\log\\left(\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|pos)}{P({w_{i}}|neg)}\\right)$\n",
    "\n",
    "We let lambda such that\n",
    "- $\\lambda(w) = \\log\\left(\\dfrac{P(w|pos)}{P(w|neg)}\\right)$\n",
    "\n",
    "Log likelyhood is give by\n",
    "- $\\displaystyle\\sum_{i=1}^{m}\\lambda(w) = \\displaystyle\\sum_{i=1}^{m}\\log\\left(\\dfrac{P(w|pos)}{P(w|neg)}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Library for linear algebra and math utils\n",
    "import pandas as pd # Dataframe library\n",
    "\n",
    "import matplotlib.pyplot as plt # Library for plots\n",
    "from utils612 import confidence_ellipse # Function to add confidence ellipses to charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bayes_features.csv'); # Load the data from the csv file\n",
    "\n",
    "data.head(5) # Print the first 5 tweets features. Each row represents a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the samples using columns 1 and 2 of the matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 8)) #Create a new figure with a custom size\n",
    "\n",
    "colors = ['red', 'green'] # Define a color palete\n",
    "\n",
    "# Color base on sentiment\n",
    "ax.scatter(data.positive, data.negative, \n",
    "    c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for each tweet\n",
    "\n",
    "# Custom limits for this chart\n",
    "plt.xlim(-250,0)\n",
    "plt.ylim(-250,0)\n",
    "\n",
    "plt.xlabel(\"Positive\") # x-axis label\n",
    "plt.ylabel(\"Negative\") # y-axis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the samples using columns 1 and 2 of the matrix\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "colors = ['red', 'green'] # Define a color palete\n",
    "\n",
    "# Color base on sentiment\n",
    "\n",
    "ax.scatter(data.positive, data.negative, c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n",
    "\n",
    "# Custom limits for this chart\n",
    "plt.xlim(-200,40)  \n",
    "plt.ylim(-200,40)\n",
    "\n",
    "plt.xlabel(\"Positive\") # x-axis label\n",
    "plt.ylabel(\"Negative\") # y-axis label\n",
    "\n",
    "data_pos = data[data.sentiment == 1] # Filter only the positive samples\n",
    "data_neg = data[data.sentiment == 0] # Filter only the negative samples\n",
    "\n",
    "# Print confidence ellipses of 2 std\n",
    "confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\n",
    "confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n",
    "\n",
    "# Print confidence ellipses of 3 std\n",
    "confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\n",
    "confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy() # Copy the whole data frame\n",
    "\n",
    "# The following 2 lines only modify the entries in the data frame where sentiment == 1\n",
    "data2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\n",
    "data2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the samples using columns 1 and 2 of the matrix\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "colors = ['red', 'green'] # Define a color palete\n",
    "\n",
    "# Color base on sentiment\n",
    "\n",
    "#data.negative[data.sentiment == 1] =  data.negative * 2\n",
    "\n",
    "ax.scatter(data2.positive, data2.negative, c=[colors[int(k)] for k in data2.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n",
    "# Custom limits for this chart\n",
    "plt.xlim(-200,40)  \n",
    "plt.ylim(-200,40)\n",
    "\n",
    "plt.xlabel(\"Positive\") # x-axis label\n",
    "plt.ylabel(\"Negative\") # y-axis label\n",
    "\n",
    "data_pos = data2[data2.sentiment == 1] # Filter only the positive samples\n",
    "data_neg = data[data2.sentiment == 0] # Filter only the negative samples\n",
    "\n",
    "# Print confidence ellipses of 2 std\n",
    "confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\n",
    "confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n",
    "\n",
    "# Print confidence ellipses of 3 std\n",
    "confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\n",
    "confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word, y)\n",
    "\n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos and N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "    \n",
    "    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n",
    "    D_pos = 0\n",
    "    for i in train_y:\n",
    "        if i == 1:\n",
    "            D_pos += 1\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "        \n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "    \n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "            \n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Models\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "$\\cos (\\theta)=\\dfrac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\dfrac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}$\n",
    "\n",
    "\n",
    "### PCA\n",
    "\n",
    "1. Mean normalized your data\n",
    "2. Compute the covariance matrix\n",
    "3. Compute SVD on the covariance matrix. This returns $[USV] = svd(\\Sigma)$ where $U$ is eigenvectors and $S$ is eigenvalues\n",
    "4. You can then use first $n$ columns of $U$, to get new data by $XU[:,0 : n]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # The swiss knife of the data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badmatrix = np.array([[1, 2], [3, 4], [5, 6, 7]]) # Define a matrix. Note the third row contains 3 elements\n",
    "print(badmatrix) # Print the malformed matrix\n",
    "print(badmatrix * 2) # It is supposed to scale the whole matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray1 = np.array([1, 2, 3, 4]) # Define an array\n",
    "norm1 = np.linalg.norm(nparray1)\n",
    "\n",
    "nparray2 = np.array([[1, 2], [3, 4]]) # Define a 2 x 2 matrix. Note the 2 level of square brackets\n",
    "norm2 = np.linalg.norm(nparray2) \n",
    "\n",
    "print(norm1)\n",
    "print(norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n",
    "\n",
    "normByCols = np.linalg.norm(nparray2, axis=0) # Get the norm for each column. Returns 2 elements\n",
    "normByRows = np.linalg.norm(nparray2, axis=1) # get the norm for each row. Returns 3 elements\n",
    "\n",
    "print(normByCols)\n",
    "print(normByRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray1 = np.array([0, 1, 2, 3]) # Define an array\n",
    "nparray2 = np.array([4, 5, 6, 7]) # Define an array\n",
    "\n",
    "flavor1 = np.dot(nparray1, nparray2) # Recommended way\n",
    "print(flavor1)\n",
    "\n",
    "flavor2 = np.sum(nparray1 * nparray2) # Ok way\n",
    "print(flavor2)\n",
    "\n",
    "flavor3 = nparray1 @ nparray2         # Geeks way\n",
    "print(flavor3)\n",
    "\n",
    "# As you never should do:             # Noobs way\n",
    "flavor4 = 0\n",
    "for a, b in zip(nparray1, nparray2):\n",
    "    flavor4 += a * b\n",
    "    \n",
    "print(flavor4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. Chosen to be a matrix with 0 mean\n",
    "\n",
    "mean = np.mean(nparray2) # Get the mean for the whole matrix\n",
    "meanByCols = np.mean(nparray2, axis=0) # Get the mean for each column. Returns 2 elements\n",
    "meanByRows = np.mean(nparray2, axis=1) # get the mean for each row. Returns 3 elements\n",
    "\n",
    "print('Matrix mean: ')\n",
    "print(mean)\n",
    "print('Mean by columns: ')\n",
    "print(meanByCols)\n",
    "print('Mean by rows:')\n",
    "print(meanByRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n",
    "\n",
    "nparrayCentered = nparray2 - np.mean(nparray2, axis=0) # Remove the mean for each column\n",
    "\n",
    "print('Original matrix')\n",
    "print(nparray2)\n",
    "print('Centered by columns matrix')\n",
    "print(nparrayCentered)\n",
    "\n",
    "print('New mean by column')\n",
    "print(nparrayCentered.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix. \n",
    "\n",
    "nparrayCentered = nparray2.T - np.mean(nparray2, axis=1) # Remove the mean for each row\n",
    "nparrayCentered = nparrayCentered.T # Transpose back the result\n",
    "\n",
    "print('Original matrix')\n",
    "print(nparray2)\n",
    "print('Centered by columns matrix')\n",
    "print(nparrayCentered)\n",
    "\n",
    "print('New mean by rows')\n",
    "print(nparrayCentered.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Library for Dataframes \n",
    "import numpy as np # Library for math functions\n",
    "import pickle # Python object serialization library. Not secure\n",
    "\n",
    "word_embeddings = pickle.load( open( \"word_embeddings_subset.p\", \"rb\" ) )\n",
    "len(word_embeddings) # there should be 243 words that will be used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryVector = word_embeddings['country'] # Get the vector representation for the word 'country'\n",
    "print(type(countryVector)) # Print the type of the vector. Note it is a numpy array\n",
    "print(countryVector) # Print the values of the vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the vector for a given word:\n",
    "def vec(w):\n",
    "    return word_embeddings[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # Import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "bag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n",
    "\n",
    "col1 = 3 # Select the column for the x axis\n",
    "col2 = 2 # Select the column for the y axis\n",
    "\n",
    "# Print an arrow for each word\n",
    "for word in bag2d:\n",
    "    ax.arrow(0, 0, word[col1], word[col2], head_width=0.005, head_length=0.005, fc='r', ec='r', width = 1e-5)\n",
    "\n",
    "    \n",
    "ax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n",
    "\n",
    "# Add the word label over each dot in the scatter plot\n",
    "for i in range(0, len(words)):\n",
    "    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['sad', 'happy', 'town', 'village']\n",
    "\n",
    "bag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n",
    "\n",
    "col1 = 3 # Select the column for the x axe\n",
    "col2 = 2 # Select the column for the y axe\n",
    "\n",
    "# Print an arrow for each word\n",
    "for word in bag2d:\n",
    "    ax.arrow(0, 0, word[col1], word[col2], head_width=0.0005, head_length=0.0005, fc='r', ec='r', width = 1e-5)\n",
    "    \n",
    "# print the vector difference between village and town\n",
    "village = vec('village')\n",
    "town = vec('town')\n",
    "diff = town - village\n",
    "ax.arrow(village[col1], village[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n",
    "\n",
    "# print the vector difference between village and town\n",
    "sad = vec('sad')\n",
    "happy = vec('happy')\n",
    "diff = happy - sad\n",
    "ax.arrow(sad[col1], sad[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n",
    "\n",
    "\n",
    "ax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n",
    "\n",
    "# Add the word label over each dot in the scatter plot\n",
    "for i in range(0, len(words)):\n",
    "    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(vec('town'))) # Print the norm of the word town\n",
    "print(np.linalg.norm(vec('sad'))) # Print the norm of the word sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital = vec('France') - vec('Paris')\n",
    "country = vec('Madrid') + capital\n",
    "\n",
    "print(country[0:5]) # Print the first 5 values of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = country - vec('Spain')\n",
    "print(diff[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe out of the dictionary embedding. This facilitate the algebraic operations\n",
    "keys = word_embeddings.keys()\n",
    "data = []\n",
    "for key in keys:\n",
    "    data.append(word_embeddings[key])\n",
    "\n",
    "embedding = pd.DataFrame(data=data, index=keys)\n",
    "# Define a function to find the closest word to a vector:\n",
    "def find_closest_word(v, k = 1):\n",
    "    # Calculate the vector difference from each word to the input vector\n",
    "    diff = embedding.values - v \n",
    "    # Get the norm of each difference vector. \n",
    "    # It means the squared euclidean distance from each word to the input vector\n",
    "    delta = np.sum(diff * diff, axis=1)\n",
    "    # Find the index of the minimun distance in the array\n",
    "    i = np.argmin(delta)\n",
    "    # Return the row name for this item\n",
    "    return embedding.iloc[i].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some rows of the embedding as a Dataframe\n",
    "embedding.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word(vec('Italy') - vec('Rome') + vec('Madrid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_word(vec('Berlin') + capital))\n",
    "print(find_closest_word(vec('Beijing') + capital))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_word(vec('Lisbon') + capital))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Spain petroleum city king\"\n",
    "vdoc = [vec(x) for x in doc.split(\" \")]\n",
    "doc2vec = np.sum(vdoc, axis = 0)\n",
    "doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word(doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                         # Linear algebra library\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "from sklearn.decomposition import PCA      # PCA library\n",
    "import pandas as pd                        # Data frame library\n",
    "import math                                # Library for math functions\n",
    "import random                              # Library for pseudo random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1  # The amount of the correlation\n",
    "x = np.random.uniform(1,2,1000) # Generate 1000 samples from a uniform random variable\n",
    "y = x.copy() * n # Make y = n * x\n",
    "\n",
    "# PCA works better if the data is centered\n",
    "x = x - np.mean(x) # Center x. Remove its mean\n",
    "y = y - np.mean(y) # Center y. Remove its mean\n",
    "\n",
    "data = pd.DataFrame({'x': x, 'y': y}) # Create a data frame with x and y\n",
    "plt.scatter(data.x, data.y) # Plot the original correlated data in blue\n",
    "\n",
    "pca = PCA(n_components=2) # Instantiate a PCA. Choose to get 2 output variables\n",
    "\n",
    "# Create the transformation model for this data. Internally, it gets the rotation \n",
    "# matrix and the explained variance\n",
    "pcaTr = pca.fit(data)\n",
    "\n",
    "rotatedData = pcaTr.transform(data) # Transform the data base on the rotation matrix of pcaTr\n",
    "# # Create a data frame with the new variables. We call these new variables PC1 and PC2\n",
    "dataPCA = pd.DataFrame(data = rotatedData, columns = ['PC1', 'PC2']) \n",
    "\n",
    "# Plot the transformed data in orange\n",
    "plt.scatter(dataPCA.PC1, dataPCA.PC2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Eigenvectors or principal component: First row must be in the direction of [1, n]')\n",
    "print(pcaTr.components_)\n",
    "\n",
    "print()\n",
    "print('Eigenvalues or explained variance')\n",
    "print(pcaTr.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "std1 = 1     # The desired standard deviation of our first random variable\n",
    "std2 = 0.333 # The desired standard deviation of our second random variable\n",
    "\n",
    "x = np.random.normal(0, std1, 1000) # Get 1000 samples from x ~ N(0, std1)\n",
    "y = np.random.normal(0, std2, 1000)  # Get 1000 samples from y ~ N(0, std2)\n",
    "#y = y + np.random.normal(0,1,1000)*noiseLevel * np.sin(0.78)\n",
    "\n",
    "# PCA works better if the data is centered\n",
    "x = x - np.mean(x) # Center x \n",
    "y = y - np.mean(y) # Center y\n",
    "\n",
    "#Define a pair of dependent variables with a desired amount of covariance\n",
    "n = 1 # Magnitude of covariance. \n",
    "angle = np.arctan(1 / n) # Convert the covariance to and angle\n",
    "print('angle: ',  angle * 180 / math.pi)\n",
    "\n",
    "# Create a rotation matrix using the given angle\n",
    "rotationMatrix = np.array([[np.cos(angle), np.sin(angle)],\n",
    "                 [-np.sin(angle), np.cos(angle)]])\n",
    "\n",
    "\n",
    "print('rotationMatrix')\n",
    "print(rotationMatrix)\n",
    "\n",
    "xy = np.concatenate(([x] , [y]), axis=0).T # Create a matrix with columns x and y\n",
    "\n",
    "# Transform the data using the rotation matrix. It correlates the two variables\n",
    "data = np.dot(xy, rotationMatrix) # Return a nD array\n",
    "\n",
    "# Print the rotated data\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1]) # Print the original data in blue\n",
    "\n",
    "# Apply PCA. In theory, the Eigenvector matrix must be the \n",
    "# inverse of the original rotationMatrix. \n",
    "pca = PCA(n_components=2)  # Instantiate a PCA. Choose to get 2 output variables\n",
    "\n",
    "# Create the transformation model for this data. Internally it gets the rotation \n",
    "# matrix and the explained variance\n",
    "pcaTr = pca.fit(data)\n",
    "\n",
    "# Create an array with the transformed data\n",
    "dataPCA = pcaTr.transform(data)\n",
    "\n",
    "print('Eigenvectors or principal component: First row must be in the direction of [1, n]')\n",
    "print(pcaTr.components_)\n",
    "\n",
    "print()\n",
    "print('Eigenvalues or explained variance')\n",
    "print(pcaTr.explained_variance_)\n",
    "\n",
    "# Print the rotated data\n",
    "plt.scatter(dataPCA[:,0], dataPCA[:,1])\n",
    "\n",
    "# Plot the first component axe. Use the explained variance to scale the vector\n",
    "plt.plot([0, rotationMatrix[0][0] * std1 * 3], [0, rotationMatrix[0][1] * std1 * 3], 'k-', color='red')\n",
    "# Plot the second component axe. Use the explained variance to scale the vector\n",
    "plt.plot([0, rotationMatrix[1][0] * std2 * 3], [0, rotationMatrix[1][1] * std2 * 3], 'k-', color='green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPoints = len(data)\n",
    "\n",
    "# Plot the original data in blue\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "\n",
    "#Plot the projection along the first component in orange\n",
    "plt.scatter(data[:,0], np.zeros(nPoints))\n",
    "\n",
    "#Plot the projection along the second component in green\n",
    "plt.scatter(np.zeros(nPoints), data[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    '''\n",
    "\n",
    "    dot = np.dot(A,B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(city1, country1, city2, embeddings):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        city1: a string (the capital city of country1)\n",
    "        country1: a string (the country of capital1)\n",
    "        city2: a string (the capital city of country2)\n",
    "        embeddings: a dictionary where the keys are words and values are their embeddings\n",
    "    Output:\n",
    "        countries: a dictionary with the most likely country and its similarity score\n",
    "    \"\"\"\n",
    "\n",
    "    # store the city1, country 1, and city 2 in a set called group\n",
    "    group = set((city1, country1, city2))\n",
    "\n",
    "    # get embeddings of city 1\n",
    "    city1_emb = embeddings[city1]\n",
    "\n",
    "    # get embedding of country 1\n",
    "    country1_emb = embeddings[country1]\n",
    "\n",
    "    # get embedding of city 2\n",
    "    city2_emb = embeddings[city2]\n",
    "\n",
    "    # get embedding of country 2 (it's a combination of the embeddings of country 1, city 1 and city 2)\n",
    "    # Remember: King - Man + Woman = Queen\n",
    "    vec = country1_emb - city1_emb + city2_emb\n",
    "    \n",
    "    # Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)\n",
    "    similarity = -1\n",
    "\n",
    "    # initialize country to an empty string\n",
    "    country = ''\n",
    "\n",
    "    # loop through all words in the embeddings dictionary\n",
    "    for word in embeddings.keys():\n",
    "\n",
    "        # first check that the word is not already in the 'group'\n",
    "        if word not in group:\n",
    "\n",
    "            # get the word embedding\n",
    "            word_emb = embeddings[word]\n",
    "\n",
    "            # calculate cosine similarity between embedding of country 2 and the word in the embeddings dictionary\n",
    "            cur_similarity = cosine_similarity(vec, word_emb)\n",
    "\n",
    "            # if the cosine similarity is more similar than the previously best similarity...\n",
    "            if cur_similarity > similarity:\n",
    "\n",
    "                # update the similarity to the new, better similarity\n",
    "                similarity = cur_similarity\n",
    "\n",
    "                # store the country as a tuple, which contains the word and the similarity\n",
    "                country = (word, cur_similarity)\n",
    "\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(word_embeddings, data):\n",
    "    '''\n",
    "    Input:\n",
    "        word_embeddings: a dictionary where the key is a word and the value is its embedding\n",
    "        data: a pandas dataframe containing all the country and capital city pairs\n",
    "    \n",
    "    Output:\n",
    "        accuracy: the accuracy of the model\n",
    "    '''\n",
    "\n",
    "    # initialize num correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through the rows of the dataframe\n",
    "    for i, row in data.iterrows():\n",
    "\n",
    "        # get city1\n",
    "        city1 = row[0]\n",
    "\n",
    "        # get country1\n",
    "        country1 = row[1]\n",
    "\n",
    "        # get city2\n",
    "        city2 =  row[2]\n",
    "\n",
    "        # get country2\n",
    "        country2 = row[3]\n",
    "\n",
    "        # use get_country to find the predicted country2\n",
    "        predicted_country2, _ = get_country(city1, country1, city2, word_embeddings)\n",
    "\n",
    "        # if the predicted country2 is the same as the actual country2...\n",
    "        if predicted_country2 == country2:\n",
    "            # increment the number of correct by 1\n",
    "            num_correct += 1\n",
    "\n",
    "    # get the number of rows in the data dataframe (length of dataframe)\n",
    "    m = len(data)\n",
    "\n",
    "    # calculate the accuracy by dividing the number correct by m\n",
    "    accuracy = num_correct / m\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation\n",
    "\n",
    "### Transform word vectors\n",
    "\n",
    "Suppose\n",
    "- $X$ is the english word vectors\n",
    "- $Y$ is the french word vectors\n",
    "- $R$ is the mapping matrix\n",
    "\n",
    "Step to learn $R$ will be\n",
    "- initialize $R$\n",
    "- For loop\n",
    "    - $Loss = \\|{XR-Y}\\|_{F}$ (frobenius norm - square all elements of matrix and add them up)\n",
    "    - $g = \\dfrac{d}{dR}Loss$\n",
    "    - $R = R - \\alpha g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                     # Import numpy for array manipulation\n",
    "import matplotlib.pyplot as plt        # Import matplotlib for charts\n",
    "from utils614 import plot_vectors      # Function to plot vectors (arrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 100 * (np.pi / 180) #convert degrees to radians\n",
    "\n",
    "Ro = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "              [np.sin(angle), np.cos(angle)]])\n",
    "\n",
    "x2 = np.array([2, 2]).reshape(1, -1) # make it a row vector\n",
    "y2 = np.dot(x2, Ro)\n",
    "\n",
    "print('Rotation matrix')\n",
    "print(Ro)\n",
    "print('\\nRotated vector')\n",
    "print(y2)\n",
    "\n",
    "print('\\n x2 norm', np.linalg.norm(x2))\n",
    "print('\\n y2 norm', np.linalg.norm(y2))\n",
    "print('\\n Rotation matrix norm', np.linalg.norm(Ro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vectors([x2, y2], fname='transform_02.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2, 2],\n",
    "              [2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_squared = np.square(A)\n",
    "A_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_Frobenius = np.sqrt(np.sum(A_squared))\n",
    "A_Frobenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Frobenius norm of the Rotation matrix')\n",
    "print(np.sqrt(np.sum(Ro * Ro)), '== ', np.linalg.norm(Ro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                # library for array and matrix manipulation\n",
    "import pprint                     # utilities for console printing \n",
    "from utils614 import plot_vectors # helper function to plot vectors\n",
    "import matplotlib.pyplot as plt   # visualization library\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4) # Instantiate a pretty printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_hash_table(value_l, n_buckets):\n",
    "    \n",
    "    def hash_function(value, n_buckets):\n",
    "        return int(value) % n_buckets\n",
    "    \n",
    "    hash_table = {i:[] for i in range(n_buckets)} # Initialize all the buckets in the hash table as empty lists\n",
    "\n",
    "    for value in value_l:\n",
    "        hash_value = hash_function(value,n_buckets) # Get the hash key for the given value\n",
    "        hash_table[hash_value].append(value) # Add the element to the corresponding bucket\n",
    "    \n",
    "    return hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_l = [100, 10, 14, 17, 97] # Set of values to hash\n",
    "hash_table_example = basic_hash_table(value_l, n_buckets=10)\n",
    "pp.pprint(hash_table_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[1, 1]]) # Define a single plane. \n",
    "fig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot\n",
    "\n",
    "plot_vectors([P], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n",
    "\n",
    "# Plot  random points. \n",
    "for i in range(0, 10):\n",
    "        v1 = np.array(np.random.uniform(-2, 2, 2)) # Get a pair of random numbers between -4 and 4 \n",
    "        side_of_plane = np.sign(np.dot(P, v1.T)) \n",
    "        \n",
    "        # Color the points depending on the sign of the result of np.dot(P, point.T)\n",
    "        if side_of_plane == 1:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot blue points\n",
    "        else:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot red points\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[1, 2]])  # Define a single plane. You may change the direction\n",
    "\n",
    "# Get a new plane perpendicular to P. We use a rotation matrix\n",
    "PT = np.dot([[0, 1], [-1, 0]], P.T).T  \n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot with custom size\n",
    "\n",
    "plot_vectors([P], colors=['b'], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n",
    "\n",
    "# Plot the plane P as a 2 vectors. \n",
    "# We scale by 2 just to get the arrows outside the current box\n",
    "plot_vectors([PT * 4, PT * -4], colors=['k', 'k'], axes=[4, 4], ax=ax1)\n",
    "\n",
    "# Plot 20 random points. \n",
    "for i in range(0, 20):\n",
    "        v1 = np.array(np.random.uniform(-4, 4, 2)) # Get a pair of random numbers between -4 and 4 \n",
    "        side_of_plane = np.sign(np.dot(P, v1.T)) # Get the sign of the dot product with P\n",
    "        # Color the points depending on the sign of the result of np.dot(P, point.T)\n",
    "        if side_of_plane == 1:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot a blue point\n",
    "        else:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot a red point\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[1, 1]])      # Single plane\n",
    "v1 = np.array([[1, 2]])     # Sample point 1\n",
    "v2 = np.array([[-1, 1]])    # Sample point 2\n",
    "v3 = np.array([[-2, -1]])   # Sample point 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(P, v1.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(P, v2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(P, v3.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_of_plane(P, v):\n",
    "    dotproduct = np.dot(P, v.T) # Get the dot product P * v'\n",
    "    sign_of_dot_product = np.sign(dotproduct) # The sign of the elements of the dotproduct matrix \n",
    "    sign_of_dot_product_scalar = sign_of_dot_product.item() # The value of the first item\n",
    "    return sign_of_dot_product_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_of_plane(P, v1) # In which side is [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_of_plane(P, v2) # In which side is [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_of_plane(P, v3) # In which side is [-2, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = np.array([[1, 1]])   # First plane 2D\n",
    "P2 = np.array([[-1, 1]])  # Second plane 2D\n",
    "P3 = np.array([[-1, -1]]) # Third plane 2D\n",
    "P_l = [P1, P2, P3]  # List of arrays. It is the multi plane\n",
    "\n",
    "# Vector to search\n",
    "v = np.array([[2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_multi_plane(P_l, v):\n",
    "    hash_value = 0\n",
    "    for i, P in enumerate(P_l):\n",
    "        sign = side_of_plane(P,v)\n",
    "        hash_i = 1 if sign >=0 else 0\n",
    "        hash_value += 2**i * hash_i\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_multi_plane(P_l, v) # Find the number of the plane that containes this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_dimensions = 2 # is 300 in assignment\n",
    "num_planes = 3 # is 10 in assignment\n",
    "random_planes_matrix = np.random.normal(\n",
    "                       size=(num_planes,\n",
    "                             num_dimensions))\n",
    "print(random_planes_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([[2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side of the plane function. The result is a matrix\n",
    "def side_of_plane_matrix(P, v):\n",
    "    dotproduct = np.dot(P, v.T)\n",
    "    sign_of_dot_product = np.sign(dotproduct) # Get a boolean value telling if the value in the cell is positive or negative\n",
    "    return sign_of_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sides_l = side_of_plane_matrix(\n",
    "            random_planes_matrix, v)\n",
    "sides_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_multi_plane_matrix(P, v, num_planes):\n",
    "    sides_matrix = side_of_plane_matrix(P, v) # Get the side of planes for P and v\n",
    "    hash_value = 0\n",
    "    for i in range(num_planes):\n",
    "        sign = sides_matrix[i].item() # Get the value inside the matrix cell\n",
    "        hash_i = 1 if sign >=0 else 0\n",
    "        hash_value += 2**i * hash_i # sum 2^i * hash_i\n",
    "        \n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_multi_plane_matrix(random_planes_matrix, v, num_planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = {\"I\": np.array([1,0,1]),\n",
    "                   \"love\": np.array([-1,0,1]),\n",
    "                   \"learning\": np.array([1,0,1])\n",
    "                  }\n",
    "words_in_document = ['I', 'love', 'learning', 'not_a_word']\n",
    "document_embedding = np.array([0,0,0])\n",
    "for word in words_in_document:\n",
    "    document_embedding += word_embedding.get(word,0)\n",
    "    \n",
    "print(document_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # get the english words (the keys in the dictionary) and store in a set()\n",
    "    english_set = set(english_vecs.keys())\n",
    "    \n",
    "    # get the french words (keys in the dictionary) and store in a set()\n",
    "    french_set = set(french_vecs.keys())\n",
    "    \n",
    "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    # loop through all english, french word pairs in the english french dictionary\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # check that the french word has an embedding and that the english word has an embedding\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            # get the english embedding\n",
    "            en_vec = english_vecs[en_word]\n",
    "\n",
    "            # get the french embedding\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "\n",
    "            # add the english embedding to the list\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # add the french embedding to the list\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # stack the vectors of X_l into a matrix X\n",
    "    X = np.array(X_l, ndmin=2)\n",
    "\n",
    "    # stack the vectors of Y_l into a matrix Y\n",
    "    Y = np.array(Y_l, ndmin=2)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "\n",
    "    # m is the number of rows in X\n",
    "    num_row, num_col = X.shape\n",
    "    m = num_row\n",
    "    \n",
    "    # diff is XR - Y\n",
    "    diff = np.dot(X,R) - Y\n",
    "\n",
    "    # diff_squared is the element-wise square of the difference\n",
    "    diff_squared = np.square(diff)\n",
    "\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
    "    loss = sum_diff_squared / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    \n",
    "    # m is the number of rows in X\n",
    "    num_row, num_col = X.shape\n",
    "    m = num_row\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m\n",
    "    gradient = np.dot(X.T, (np.dot(X, R) - Y)) * (2/m) \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        \n",
    "        # use the function that you defined to compute the gradient\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -= learning_rate * gradient\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    \n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(v, row)\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[len(sorted_ids)-k:]\n",
    "    \n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    # The prediction is X times R\n",
    "    pred = np.dot(X,R)\n",
    "    num_row, num_col = pred.shape\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i], Y)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct / num_row\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet, en_embeddings): \n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    '''\n",
    "    doc_embedding = np.zeros(300)\n",
    "\n",
    "    # process the document into a list of words (process the tweet)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    \n",
    "    for word in processed_doc:\n",
    "        # add the word embedding to the running total for the document embedding\n",
    "        if word in en_embeddings:\n",
    "            doc_embedding += en_embeddings[word]\n",
    "    \n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "    '''\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in our dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - document_vec_matrix: matrix of tweet embeddings.\n",
    "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    '''\n",
    "\n",
    "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
    "    # the value is the document embedding for that document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # this is list that will store the document vectors\n",
    "    document_vec_l = []\n",
    "\n",
    "    for i, doc in enumerate(all_docs):\n",
    "\n",
    "        # get the document embedding of the tweet\n",
    "        doc_embedding = get_document_embedding(doc, en_embeddings)\n",
    "\n",
    "        # save the document embedding into the ind2Tweet dictionary at index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "\n",
    "        # append the document embedding to the list of document vectors\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "    # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # for the set of planes,\n",
    "    # calculate the dot product between the vector and the matrix containing the planes\n",
    "    # remember that planes has shape (300, 10)\n",
    "    # The dot product will have the shape (1,10)\n",
    "    dot_product = np.dot(v, planes)\n",
    "\n",
    "    # get the sign of the dot product (1,10) shaped vector\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "    \n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    # if the sign is 0, i.e. the vector is in the plane, consider the sign to be positive\n",
    "    h = (sign_of_dot_product>=0)\n",
    "    \n",
    "    h = h.astype(int)\n",
    "    \n",
    "    h = np.squeeze(h)\n",
    "    \n",
    "    # initialize the hash value to 0\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        # increment the hash value by 2^i * h_i\n",
    "        hash_value += (2 ** i) * h[i]\n",
    "    \n",
    "    # cast hash_value as an integer\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_row, num_col = planes.shape\n",
    "    num_of_planes = num_col\n",
    "\n",
    "    # number of buckets is 2^(number of planes)\n",
    "    num_buckets = 2 ** num_of_planes\n",
    "\n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {new_list: [] for new_list in range(num_buckets)}\n",
    "\n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {new_list: [] for new_list in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):\n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
