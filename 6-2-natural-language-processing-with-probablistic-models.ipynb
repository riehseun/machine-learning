{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrect\n",
    "\n",
    "- Identify a mis-spelled word\n",
    "- Find string $n$ edit distances away\n",
    "- Filter candidates (by keeping only real words from the previous step)\n",
    "- Calculate word probabilities by taking the context into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    all_words = [] # return this variable correctly\n",
    "\n",
    "    content = open(file_name, \"r\").read()\n",
    "    content = content.lower()\n",
    "    all_words = re.findall(r'\\w+', content)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "\n",
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    \n",
    "    word_count_dict = {}  # fill this with word counts\n",
    "\n",
    "    for word in word_l:\n",
    "        if word not in word_count_dict:\n",
    "            word_count_dict[word] = 1\n",
    "        else:\n",
    "            word_count_dict[word] += 1\n",
    "    \n",
    "    return word_count_dict\n",
    "\n",
    "\n",
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {}  # return this variable correctly\n",
    "    \n",
    "    total = 0 \n",
    "    for word in word_count_dict:\n",
    "        total += word_count_dict[word]\n",
    "        \n",
    "    for word in word_count_dict:\n",
    "        probs[word] = word_count_dict[word] / total\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    # 'nice' is split into : [('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')]\n",
    "    # For our 'nice' example you get: ['ice', 'nce', 'nie', 'nic']\n",
    "    for i in range(len(word)):\n",
    "        split_l.append((word[:i], word[i:]))\n",
    "        \n",
    "    for j in range(len(split_l)):\n",
    "        first = split_l[j][0]\n",
    "        second = split_l[j][1]\n",
    "        new = first + second\n",
    "        delete_l.append(new[:j]+new[j+1:])\n",
    "    \n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return delete_l\n",
    "\n",
    "\n",
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    if len(word) == 1:\n",
    "        return word\n",
    "    \n",
    "    for i in range(len(word)):\n",
    "        split_l.append((word[:i], word[i:]))\n",
    "        \n",
    "    for (a,b) in split_l:\n",
    "        if len(a) > 1 and len(b) > 1:\n",
    "            temp1 = b[0]\n",
    "            temp2 = a[len(a)-1]\n",
    "            switch_l.append(a[len(a)-2]+temp1+temp2+b[1:])\n",
    "        elif len(a) == 1 and len(b) > 1:\n",
    "            temp1 = b[0]\n",
    "            temp2 = a[len(a)-1]\n",
    "            switch_l.append(temp1+temp2+b[1:])\n",
    "        elif len(a) > 1 and len(b) == 1:\n",
    "            temp1 = b[0]\n",
    "            temp2 = a[len(a)-1]\n",
    "            switch_l.append(a[len(a)-2]+temp1+temp2)\n",
    "        elif len(a) == 1 and len(b) == 1:\n",
    "            switch_l.append(b+a)\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "\n",
    "    return switch_l\n",
    "\n",
    "\n",
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    for i in range(len(word)):\n",
    "        split_l.append((word[:i], word[i:]))\n",
    "        \n",
    "    # TODO\n",
    "                \n",
    "    if word in replace_l:\n",
    "        replace_l.remove(word)\n",
    "        \n",
    "    replace_set = set(replace_l)\n",
    "    \n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l\n",
    "\n",
    "\n",
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    for i in range(len(word)+1):\n",
    "        split_l.append((word[:i], word[i:]))\n",
    "            \n",
    "    for j in range(len(word)+1):\n",
    "        for k in letters:\n",
    "            if j == 0:\n",
    "                insert_l.append(k+word)\n",
    "            elif j == len(word):\n",
    "                insert_l.append(word+k)\n",
    "            else:\n",
    "                insert_l.append(word[:j]+k+word[j:])\n",
    "\n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l\n",
    "\n",
    "\n",
    "def edit_one_letter(word, allow_switches = True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "    \n",
    "    inserts = insert_letter(word)\n",
    "    for i in inserts:\n",
    "        edit_one_set.add(i)\n",
    "        \n",
    "    deletes = delete_letter(word)\n",
    "    for j in deletes:\n",
    "        edit_one_set.add(j)\n",
    "        \n",
    "    replaces = replace_letter(word)\n",
    "    for k in replaces:\n",
    "        edit_one_set.add(k)\n",
    "            \n",
    "    if allow_switches:\n",
    "        switches = switch_letter(word)\n",
    "        for k in switches:\n",
    "            edit_one_set.add(k)\n",
    "            \n",
    "    if word in edit_one_set:\n",
    "        edit_one_set.remove(word)\n",
    "\n",
    "    return edit_one_set\n",
    "\n",
    "\n",
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "    \n",
    "    edit_one_set = edit_one_letter(word)\n",
    "    for word_one_edit_away in edit_one_set:\n",
    "        word_two_edits_away = edit_one_letter(word_one_edit_away)\n",
    "        for w in word_two_edits_away:\n",
    "            edit_two_set.add(w)\n",
    "            \n",
    "    return edit_two_set.union(edit_one_set)\n",
    "\n",
    "\n",
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    if word in vocab:\n",
    "        n_best.append((word, probs[word]))\n",
    "        \n",
    "    edit_one_set = edit_one_letter(word, True)    \n",
    "    for word in edit_one_set:\n",
    "        if word in vocab:\n",
    "            n_best.append((word, probs[word]))\n",
    "    \n",
    "    edit_two_set = edit_two_letters(word, True)\n",
    "    for word in edit_two_set:\n",
    "        if word in vocab:\n",
    "            n_best.append((word, probs[word]))\n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best\n",
    "\n",
    "\n",
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "        \n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): # Replace None with the proper range\n",
    "        D[row,0] = D[row-1,0]+del_cost\n",
    "        \n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1,n+1): # Replace None with the proper range\n",
    "        D[0,col] = D[0,col-1]+ins_cost\n",
    "        \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row,col] = min(D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost)\n",
    "          \n",
    "    # Set the minimum edit distance with the cost found at row m, column n\n",
    "    med = D[row,col]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chains\n",
    "\n",
    "![2-2-1](images/natural-language-processing/2-2-1.png)\n",
    "\n",
    "- $Q = \\{q_{1}, q_{2}, q_{3}\\}$: set of all states in the model\n",
    "\n",
    "![2-2-2](images/natural-language-processing/2-2-2.png)\n",
    "\n",
    "- Blue circles: part of speech tags\n",
    "- Arrows: transition probability from one part of speech to another\n",
    "- Table $A$ can be contructed from the diagram\n",
    "\n",
    "![2-2-3](images/natural-language-processing/2-2-3.png)\n",
    "\n",
    "### Transition matrix\n",
    "\n",
    "- $A$ can also be written as transition matrix\n",
    "\n",
    "![2-2-4](images/natural-language-processing/2-2-4.png)\n",
    "\n",
    "- Emission probability: probability to go from one state (POS tag) to a specific word\n",
    "\n",
    "![2-2-5](images/natural-language-processing/2-2-5.png)\n",
    "\n",
    "### Emission matrix\n",
    "\n",
    "- To populate emission matrix $B$, use labeled dataset\n",
    "\n",
    "![2-2-6](images/natural-language-processing/2-2-6.png)\n",
    "\n",
    "- $C(t_{(iâˆ’1)},t_{(i)})$ is the number of times that tag $t_{(i)}$ shows up after tag $t_{(i-1)}$\n",
    "\n",
    "![2-2-7](images/natural-language-processing/2-2-7.png)\n",
    "\n",
    "- $\\pi$ is the initial state\n",
    "- Numbers in the table indicates the number of times that a tag shows up right after another tag\n",
    "    - For example, Noun shows up after the initial state once, so 1\n",
    "    - For example, Other shows up after the inital state twice, so 2\n",
    "    \n",
    "![2-2-8](images/natural-language-processing/2-2-8.png)\n",
    "\n",
    "- To deal with \"no occurance\" problem, we introduce smoothing probability such that\n",
    "\n",
    "![2-2-9](images/natural-language-processing/2-2-9.png)\n",
    "\n",
    "- $P(t_{i}|t_{i-1}) = \\dfrac{C(t_{i-1},t_{i}) + \\alpha}{C(t_{i-1}) +\\alpha * N}$\n",
    "- $N$ is the **number of tags**\n",
    "\n",
    "![2-2-10](images/natural-language-processing/2-2-10.png)\n",
    "\n",
    "- We also use smoothing probability when populating Emission matrix\n",
    "- $P(W_{i}|t_{i}) = \\dfrac{C(t_{i},w_{i})+\\epsilon}{\\displaystyle\\sum_{j=1}^{V}C(t_{i},w_{i}) + N * \\epsilon} = = \\dfrac{C(t_{i},w_{i})+\\epsilon}{C(t_{i}) + N * \\epsilon}$\n",
    "    - where $C(t_{i},w_{i})$ means how many times tag $t_{i}$ is associated with word $w_{i}$ \n",
    "    - $N$ is the **number of words**\n",
    "    \n",
    "    \n",
    "### Viterbi algorithm\n",
    "\n",
    "![2-2-11](images/natural-language-processing/2-2-11.png)\n",
    "\n",
    "- To go from $\\pi$ to $O$, multiply transition probability of 0.3 to emission probability of 0.5, which results in 0.15\n",
    "\n",
    "![2-2-12](images/natural-language-processing/2-2-12.png)\n",
    "\n",
    "### Viterbi initialization\n",
    "\n",
    "- Populate two matrices $C$ and $D$ such that\n",
    "\n",
    "![2-2-13](images/natural-language-processing/2-2-13.png)\n",
    "\n",
    "![2-2-14](images/natural-language-processing/2-2-14.png)\n",
    "\n",
    "\n",
    "### Viterbi forward pass\n",
    "\n",
    "![2-2-15](images/natural-language-processing/2-2-15.png)\n",
    "\n",
    "![2-2-16](images/natural-language-processing/2-2-16.png)\n",
    "\n",
    "\n",
    "### Viterbi backward pass\n",
    "\n",
    "![2-2-17](images/natural-language-processing/2-2-17.png)\n",
    "\n",
    "- This equation gets the top right element of matrix $C$\n",
    "\n",
    "![2-2-18](images/natural-language-processing/2-2-18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the dictionaries using defaultdict\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    # use 'i' to track the line number in the corpus\n",
    "    i = 0 \n",
    "    \n",
    "    # Each item in the training corpus contains a word and its POS tag\n",
    "    # Go through each word and its tag in the training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        # Increment the word_tag count\n",
    "        i += 1\n",
    "        \n",
    "        # Every 50,000 words, print the word count\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"word count = {i}\")\n",
    "            \n",
    "        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        \n",
    "        # Increment the transition count for the previous word and tag\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        # Increment the emission count for the tag and word\n",
    "        emission_counts[(tag, word)] += 1\n",
    "\n",
    "        # Increment the tag count\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts\n",
    "\n",
    "\n",
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    '''\n",
    "    Input: \n",
    "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output: \n",
    "        accuracy: Number of times you classified a word correctly\n",
    "    '''\n",
    "    \n",
    "    # Initialize the number of correct predictions to zero\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Get the (tag, word) tuples, stored as a set\n",
    "    all_words = set(emission_counts.keys())\n",
    "    \n",
    "    # Get the number of (word, POS) tuples in the corpus 'y'\n",
    "    total = len(y)\n",
    "    for word, y_tup in zip(prep, y): \n",
    "\n",
    "        # Split the (word, POS) string into a list of two items\n",
    "        y_tup_l = y_tup.split()\n",
    "        \n",
    "        # Verify that y_tup contain both word and POS\n",
    "        if len(y_tup_l) == 2:\n",
    "            \n",
    "            # Set the true POS label for this word\n",
    "            true_label = y_tup_l[1]\n",
    "\n",
    "        else:\n",
    "            # If the y_tup didn't contain word and POS, go to next word\n",
    "            continue\n",
    "    \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        \n",
    "        # If the word is in the vocabulary...\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "\n",
    "            ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "                        \n",
    "                # define the key as the tuple containing the POS and word\n",
    "                key = (pos, word)\n",
    "\n",
    "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
    "                if key in emission_counts: # complete this line\n",
    "\n",
    "                # get the emission count of the (pos,word) tuple \n",
    "                    count = emission_counts[key]\n",
    "\n",
    "                    # keep track of the POS with the largest count\n",
    "                    if count > count_final: # complete this line\n",
    "\n",
    "                        # update the final count (largest count)\n",
    "                        count_final = count\n",
    "\n",
    "                        # update the final POS\n",
    "                        pos_final = pos\n",
    "\n",
    "            # If the final POS (with the largest count) matches the true POS:\n",
    "            if pos_final == true_label: # complete this line\n",
    "                \n",
    "                # Update the number of correct predictions\n",
    "                num_correct += 1\n",
    "            \n",
    "    accuracy = num_correct / total\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: transition count for the previous word and tag\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    \n",
    "    # Get a sorted list of unique POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Count the number of unique POS tags\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    # Initialize the transition matrix 'A'\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    # Get the unique transition tuples (previous POS, current POS)\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    # Go through each row of the transition matrix A\n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        # Go through each column of the transition matrix A\n",
    "        for j in range(num_tags):\n",
    "\n",
    "            # Initialize the count of the (prev POS, current POS) to zero\n",
    "            count = 0\n",
    "        \n",
    "            # Define the tuple (prev POS, current POS)\n",
    "            # Get the tag at position i and tag at position j (from the all_tags list)\n",
    "            key = (all_tags[i], all_tags[j])\n",
    "\n",
    "            # Check if the (prev POS, current POS) tuple \n",
    "            # exists in the transition counts dictionary\n",
    "            if key in transition_counts: #complete this line\n",
    "                \n",
    "                # Get count from the transition_counts dictionary \n",
    "                # for the (prev POS, current POS) tuple\n",
    "                count = transition_counts[key]\n",
    "                \n",
    "            # Get the count of the previous tag (index position i) from tag_counts\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "            \n",
    "            # Apply smoothing using count of the tuple, alpha, \n",
    "            # count of previous tag, alpha, and total number of tags\n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index.\n",
    "               within the function it'll be treated as a list\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    \n",
    "    # get the number of POS tag\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Get a list of all POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Get the total number of unique words in the vocabulary\n",
    "    num_words = len(vocab)\n",
    "    \n",
    "    # Initialize the emission matrix B with places for\n",
    "    # tags in the rows and words in the columns\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    \n",
    "    # Get a set of all (POS, word) tuples \n",
    "    # from the keys of the emission_counts dictionary\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "        \n",
    "    # Go through each row (POS tags)\n",
    "    for i in range(num_tags): # complete this line\n",
    "        \n",
    "        # Go through each column (words)\n",
    "        for j in range(num_words): # complete this line\n",
    "\n",
    "            # Initialize the emission count for the (POS tag, word) to zero\n",
    "            count = 0\n",
    "                    \n",
    "            # Define the (POS tag, word) tuple for this row and column\n",
    "            key = (all_tags[i], vocab[j])\n",
    "            \n",
    "            # check if the (POS tag, word) tuple exists as a key in emission counts\n",
    "            if key in emission_counts: # complete this line\n",
    "        \n",
    "                # Get the count of (POS tag, word) from the emission_counts d\n",
    "                count = emission_counts[key]\n",
    "                \n",
    "            # Get the count of the POS tag\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "                \n",
    "            # Apply smoothing and store the smoothed value \n",
    "            # into the emission matrix B for this row and column\n",
    "            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)\n",
    "\n",
    "    return B\n",
    "\n",
    "\n",
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    # Get the total number of unique POS tags\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Initialize best_probs matrix \n",
    "    # POS tags in the rows, number of words in the corpus as the columns\n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    \n",
    "    # Initialize best_paths matrix\n",
    "    # POS tags in the rows, number of words in the corpus as columns\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    # Define the start token\n",
    "    s_idx = states.index(\"--s--\")\n",
    "    \n",
    "    # Go through each of the POS tags\n",
    "    for i in range(num_tags): # complete this line\n",
    "        \n",
    "        # Handle the special case when the transition from start token to POS tag i is zero\n",
    "        if A[s_idx, i] == 0: # complete this line\n",
    "            \n",
    "            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n",
    "            best_probs[i,0] = float('-inf')\n",
    "        \n",
    "        # For all other cases when transition from start token to POS tag i is non-zero:\n",
    "        else:\n",
    "            \n",
    "            # Initialize best_probs at POS tag 'i', column 0\n",
    "            # Check the formula in the instructions above\n",
    "            best_probs[i,0] = math.log(A[s_idx, i]) + math.log(B[i,vocab[corpus[0]]])\n",
    "                        \n",
    "    return best_probs, best_paths\n",
    "\n",
    "\n",
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transition and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Go through every word in the corpus starting from word 1\n",
    "    # Recall that word 0 was initialized in `initialize()`\n",
    "    for i in range(1, len(test_corpus)): \n",
    "        \n",
    "        # Print number of words processed, every 5000 words\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "            \n",
    "        # For each unique POS tag that the current word can be\n",
    "        for j in range(num_tags): # complete this line\n",
    "            \n",
    "            # Initialize best_prob for word i to negative infinity\n",
    "            best_prob_i = float('-inf')\n",
    "            \n",
    "            # Initialize best_path for current word i to None\n",
    "            best_path_i = None\n",
    "\n",
    "            # For each POS tag that the previous word can be:\n",
    "            for k in range(num_tags): # complete this line\n",
    "            \n",
    "                # Calculate the probability = \n",
    "                # best probs of POS tag k, previous word i-1 + \n",
    "                # log(prob of transition from POS k to POS j) + \n",
    "                # log(prob that emission of POS j is word i)\n",
    "                prob = best_probs[k,i-1] + math.log(A[k,j]) + math.log(B[j,vocab[test_corpus[i]]])\n",
    "                \n",
    "                # check if this path's probability is greater than\n",
    "                # the best probability up to and before this point\n",
    "                if prob > best_prob_i: # complete this line\n",
    "                    \n",
    "                    # Keep track of the best probability\n",
    "                    best_prob_i = prob\n",
    "                    \n",
    "                    # keep track of the POS tag of the previous word\n",
    "                    # that is part of the best path.  \n",
    "                    # Save the index (integer) associated with \n",
    "                    # that previous word's POS tag\n",
    "                    best_path_i = k\n",
    "\n",
    "            # Save the best probability for the \n",
    "            # given current word's POS tag\n",
    "            # and the position of the current word inside the corpus\n",
    "            best_probs[j,i] = best_prob_i\n",
    "            \n",
    "            # Save the unique integer ID of the previous POS tag\n",
    "            # into best_paths matrix, for the POS tag of the current word\n",
    "            # and the position of the current word inside the corpus.\n",
    "            best_paths[j,i] = best_path_i\n",
    "\n",
    "    return best_probs, best_paths\n",
    "\n",
    "\n",
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    # Get the number of words in the corpus\n",
    "    # which is also the number of columns in best_probs, best_paths\n",
    "    m = best_paths.shape[1] \n",
    "    \n",
    "    # Initialize array z, same length as the corpus\n",
    "    z = [None] * m\n",
    "    \n",
    "    # Get the number of unique POS tags\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Initialize the best probability for the last word\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    \n",
    "    # Initialize pred array, same length as corpus\n",
    "    pred = [None] * m\n",
    "    \n",
    "    ## Step 1 ##\n",
    "    \n",
    "    # Go through each POS tag for the last word (last column of best_probs)\n",
    "    # in order to find the row (POS tag integer ID) \n",
    "    # with highest probability for the last word\n",
    "    for k in range(num_tags-1, 0, -1): # complete this line\n",
    "\n",
    "        # If the probability of POS tag at row k \n",
    "        # is better than the previously best probability for the last word:\n",
    "        if best_probs[k, m-1] > best_prob_for_last_word: # complete this line\n",
    "            \n",
    "            # Store the new best probability for the last word\n",
    "            best_prob_for_last_word = best_probs[k, m-1]\n",
    "    \n",
    "            # Store the unique integer ID of the POS tag\n",
    "            # which is also the row number in best_probs\n",
    "            z[m - 1] = k\n",
    "            \n",
    "    # Convert the last word's predicted POS tag\n",
    "    # from its unique integer ID into the string representation\n",
    "    # using the 'states' list\n",
    "    # store this in the 'pred' array for the last word\n",
    "    pred[m - 1] = states[z[m - 1]]\n",
    "    \n",
    "    ## Step 2 ##\n",
    "    # Find the best POS tags by walking backward through the best_paths\n",
    "    # From the last word in the corpus to the 0th word in the corpus\n",
    "    for i in range(m-1, 0, -1): # complete this line\n",
    "        \n",
    "        # Retrieve the unique integer ID of\n",
    "        # the POS tag for the word at position 'i' in the corpus\n",
    "        pos_tag_for_word_i = z[i]\n",
    "        \n",
    "        # In best_paths, go to the row representing the POS tag of word i\n",
    "        # and the column representing the word's position in the corpus\n",
    "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i, i]\n",
    "        \n",
    "        # Get the previous word's POS tag in string form\n",
    "        # Use the 'states' list, \n",
    "        # where the key is the unique integer ID of the POS tag,\n",
    "        # and the value is the string representation of that POS tag\n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "- Sequence of $N$ words\n",
    "\n",
    "![2-3-1](images/natural-language-processing/2-3-1.png)\n",
    "\n",
    "![2-3-2](images/natural-language-processing/2-3-2.png)\n",
    "\n",
    "- Introduce notation\n",
    "    - $w_{1}^{m} = w_{1}w_{2}w_{3} \\dots w_{m}$\n",
    "    - $w_{1}^{3} = w_{1}w_{2}w_{3}$\n",
    "    - $w_{m-2}^{m} = w_{m-2}w_{m-1}w_{m}$\n",
    "    \n",
    "### Unigram probability\n",
    "\n",
    "- $P(w) = \\dfrac{C(w)}{m}$\n",
    "\n",
    "### Bigram probability\n",
    "\n",
    "![2-3-3](images/natural-language-processing/2-3-3.png)\n",
    "\n",
    "### Trigram probability\n",
    "\n",
    "- $P(w_{3} | w_{1}^{2}) = \\dfrac{C(w_{1}^{2}w_{3})}{C(w_{1}^{2})}$\n",
    "- $C(w_{1}^{2}w_{3}) = C(w_{1}w_{2}w_{3}) = C(w_{1}^{3})$ \n",
    "\n",
    "### N-gram probability\n",
    "\n",
    "- $P(w_{N}|w_{1}^{N-1}) = \\dfrac{C(w_{1}^{N-1}w_{N})}{C(w_{1}^{N-1})}$\n",
    "- $C(w_{1}^{N-1}w_{N}) = C(w_{1}^{N})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
