{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with frequencies\n",
    "\n",
    "Example\n",
    "\n",
    "Postive\n",
    "- I am happy because I am learning NLP\n",
    "- I am happy\n",
    "\n",
    "Negative\n",
    "- I am sad, I am not learning NLP\n",
    "- I am sad\n",
    "\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>PosFreq(1)</td>\n",
    "    <td>NegFreq(0)</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "For the word, \"I am sad, I am not learning NLP\"\n",
    "- Freq(w,1) = 3+3+1+1 = 8 (\"happy\" and \"because\" do not appear on the sentence)\n",
    "- Freq(w,0) = 3+3+1+1+2+1 = 8 (\"happy\" and \"because\" do not appear on the sentence)\n",
    "\n",
    "The feature vector becomes [1,8,11] \n",
    "- 1 is the bias\n",
    "- 8 is positive feature\n",
    "- 11 is negative feature\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "- Eliminate \"Stop words\" like \"and, is, are, at, has, for, a\"\n",
    "- Eliminate punctuations\n",
    "- Eliminate handles (starting with @) and URLs\n",
    "- Stemming word \"tune\" has three forms \"tune, tuned, tuning\". Stemmed word becomes \"tun\"\n",
    "- Convert all words to lowercase\n",
    "\n",
    "For the word, \"I am happy Because i am learning NLP @DeepLearning\", we do the preprocessing to get\n",
    "- [happy, learn, nlp]\n",
    "\n",
    "Then, feature vector becomes [1,4,2]\n",
    "- 1 is the bias\n",
    "- happy appears twice, learn and nlp appear once each, thus 4 is the positive feature\n",
    "- learn and nlp appear once each, thus 2 is the negative feature\n",
    "\n",
    "If we have lots of $m$ sentences to construct the feature vectors,\n",
    "$\\begin{bmatrix}\n",
    "    1 & X_{1}^{(1)} & X_{2}^{(1)} \\\\\n",
    "    1 & X_{1}^{(2)} & X_{2}^{(2)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    1 & X_{1}^{(m)} & X_{2}^{(m)}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Conditional probability\n",
    "- probability of B given that A happened OR\n",
    "- looking at elements of A, probability that they also being to B\n",
    "\n",
    "Bayes rule\n",
    "- $P(X|Y) = P(Y|X) \\times \\dfrac{P(X)}{P(Y)}$\n",
    "\n",
    "Table from before but add the total sum\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>PosFreq(1)</td>\n",
    "    <td>NegFreq(0)</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>N</td>\n",
    "    <td>13</td>\n",
    "    <td>12</td>\n",
    "<tr>\n",
    "</table>\n",
    "\n",
    "Compute conditional probabilities, for example\n",
    "- $P(I|Pos) = \\dfrac{3}{13}, P(I|Neg) = \\dfrac{3}{12}$\n",
    "\n",
    "Then, fill those conditional probabilties to a new table\n",
    "\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>Pos</td>\n",
    "    <td>Neg</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>0.24</td>\n",
    "    <td>0.25</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>0.24</td>\n",
    "    <td>0.25</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>0.15</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.17</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.17</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Sum</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Naive Bayes binary classification rule\n",
    "- $\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|POS)}{P({w_{i}}|NEG)}$\n",
    "\n",
    "For an example sentence \"I am happy today; I am learning\"\n",
    "- $\\dfrac{0.2}{0.2} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.14}{0.10} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.1}{0.1} = 1.4$\n",
    "\n",
    "Laplacian smoothing\n",
    "- avoid problems of probabilities being $0$\n",
    "- $P(w_{i}|class) = \\dfrac{freq(w_{i},class)+1}{(N_{class}+V)}$\n",
    "    - $N_{class}$ : frequency of all words in class\n",
    "    - $V$ : number of unique words in vocabulary\n",
    "- for example, $P(I|POS) = \\dfrac{3+1}{13+8} = 0.19$\n",
    "- then, the table becomes\n",
    "\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>Pos</td>\n",
    "    <td>Neg</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>0.19</td>\n",
    "    <td>0.20</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>0.19</td>\n",
    "    <td>0.20</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>0.14</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.05</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.15</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.15</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Sum</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
