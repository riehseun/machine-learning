{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with frequencies\n",
    "\n",
    "Example\n",
    "\n",
    "Postive\n",
    "- I am happy because I am learning NLP\n",
    "- I am happy\n",
    "\n",
    "Negative\n",
    "- I am sad, I am not learning NLP\n",
    "- I am sad\n",
    "\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>PosFreq(1)</td>\n",
    "    <td>NegFreq(0)</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "For the word, \"I am sad, I am not learning NLP\"\n",
    "- Freq(w,1) = 3+3+1+1 = 8 (\"happy\" and \"because\" do not appear on the sentence)\n",
    "- Freq(w,0) = 3+3+1+1+2+1 = 8 (\"happy\" and \"because\" do not appear on the sentence)\n",
    "\n",
    "The feature vector becomes [1,8,11] \n",
    "- 1 is the bias\n",
    "- 8 is positive feature\n",
    "- 11 is negative feature\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "- Eliminate \"Stop words\" like \"and, is, are, at, has, for, a\"\n",
    "- Eliminate punctuations\n",
    "- Eliminate handles (starting with @) and URLs\n",
    "- Stemming word \"tune\" has three forms \"tune, tuned, tuning\". Stemmed word becomes \"tun\"\n",
    "- Convert all words to lowercase\n",
    "\n",
    "For the word, \"I am happy Because i am learning NLP @DeepLearning\", we do the preprocessing to get\n",
    "- [happy, learn, nlp]\n",
    "\n",
    "Then, feature vector becomes [1,4,2]\n",
    "- 1 is the bias\n",
    "- happy appears twice, learn and nlp appear once each, thus 4 is the positive feature\n",
    "- learn and nlp appear once each, thus 2 is the negative feature\n",
    "\n",
    "If we have lots of $m$ sentences to construct the feature vectors,\n",
    "$\\begin{bmatrix}\n",
    "    1 & X_{1}^{(1)} & X_{2}^{(1)} \\\\\n",
    "    1 & X_{1}^{(2)} & X_{2}^{(2)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    1 & X_{1}^{(m)} & X_{2}^{(m)}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    num_rows, num_cols = x.shape\n",
    "    m = num_rows\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = (-1/m) * ( np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)) )\n",
    "                      \n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha/m) * np.dot(x.T, h-y)\n",
    "        \n",
    "    J = float(J)\n",
    "    return J, theta\n",
    "\n",
    "\n",
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        key_pos = (word,1.0)\n",
    "        count_pos = freqs[key_pos] if key_pos in freqs else 0\n",
    "        x[0,1] += count_pos\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        key_neg = (word,0.0)\n",
    "        count_neg = freqs[key_neg] if key_neg in freqs else 0\n",
    "        x[0,2] += count_neg\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x\n",
    "\n",
    "\n",
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    num_row, num_col = test_y.shape\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    y_hat_matrix = np.array(y_hat, ndmin=2).T\n",
    "    compare_result = (y_hat_matrix == test_y)\n",
    "    count_true = np.count_nonzero(compare_result)\n",
    "    \n",
    "    accuracy = count_true / num_row\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "- **assumes each word in sentence are independent from one another** \n",
    "\n",
    "Conditional probability\n",
    "- probability of B given that A happened OR\n",
    "- looking at elements of A, probability that they also being to B\n",
    "\n",
    "Bayes rule\n",
    "- $P(X|Y) = P(Y|X) \\times \\dfrac{P(X)}{P(Y)}$\n",
    "\n",
    "Table from before but add the total sum\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>PosFreq(1)</td>\n",
    "    <td>NegFreq(0)</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>N</td>\n",
    "    <td>13</td>\n",
    "    <td>12</td>\n",
    "<tr>\n",
    "</table>\n",
    "\n",
    "Compute conditional probabilities, for example\n",
    "- $P(I|Pos) = \\dfrac{3}{13}, P(I|Neg) = \\dfrac{3}{12}$\n",
    "\n",
    "Then, fill those conditional probabilties to a new table\n",
    "\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>Pos</td>\n",
    "    <td>Neg</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>0.24</td>\n",
    "    <td>0.25</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>0.24</td>\n",
    "    <td>0.25</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>0.15</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.17</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.17</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Sum</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Naive Bayes binary classification rule\n",
    "- $\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|POS)}{P({w_{i}}|NEG)}$\n",
    "\n",
    "For an example sentence \"I am happy today; I am learning\"\n",
    "- $\\dfrac{0.2}{0.2} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.14}{0.10} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.1}{0.1} = 1.4$\n",
    "\n",
    "### Laplacian smoothing\n",
    "- avoid problems of probabilities being $0$\n",
    "- $P(w_{i}|class) = \\dfrac{freq(w_{i},class)+1}{(N_{class}+V)}$\n",
    "    - $N_{class}$ : frequency of all words in class\n",
    "    - $V$ : number of unique words in vocabulary\n",
    "- for example, $P(I|POS) = \\dfrac{3+1}{13+8} = 0.19$\n",
    "- then, the table becomes\n",
    "\n",
    "<table>\n",
    "<th>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>Pos</td>\n",
    "    <td>Neg</td>\n",
    "</th>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>0.19</td>\n",
    "    <td>0.20</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>0.19</td>\n",
    "    <td>0.20</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>0.14</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.05</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.15</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.15</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Sum</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### Log likelihood\n",
    "\n",
    "To do inference, we can compute\n",
    "- $\\dfrac{P(pos)}{P(neg)}\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|pos)}{P({w_{i}}|neg)} \\gt 1$\n",
    "\n",
    "To avoid numerical overflow as $m$ gets larger, we introduce \"log\" such that\n",
    "- $\\log\\left(\\dfrac{P(pos)}{P(neg)}\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|pos)}{P({w_{i}}|neg)}\\right) = \\log\\left(\\dfrac{P(pos)}{P(neg)}\\right) + \\log\\left(\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|pos)}{P({w_{i}}|neg)}\\right)$\n",
    "\n",
    "We let lambda such that\n",
    "- $\\lambda(w) = \\log\\left(\\dfrac{P(w|pos)}{P(w|neg)}\\right)$\n",
    "\n",
    "Log likelyhood is give by\n",
    "- $\\displaystyle\\sum_{i=1}^{m}\\lambda(w) = \\displaystyle\\sum_{i=1}^{m}\\log\\left(\\dfrac{P(w|pos)}{P(w|neg)}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word, y)\n",
    "\n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos and N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "    \n",
    "    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n",
    "    D_pos = 0\n",
    "    for i in train_y:\n",
    "        if i == 1:\n",
    "            D_pos += 1\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "        \n",
    "    return logprior, loglikelihood\n",
    "\n",
    "\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "    \n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "            \n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "$\\cos (\\theta)=\\dfrac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\dfrac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}$\n",
    "\n",
    "\n",
    "### PCA\n",
    "\n",
    "1. Mean normalized your data\n",
    "2. Compute the covariance matrix\n",
    "3. Compute SVD on the covariance matrix. This returns $[USV] = svd(\\Sigma)$ where $U$ is eigenvectors and $S$ is eigenvalues\n",
    "4. You can then use first $n$ columns of $U$, to get new data by $XU[:,0 : n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    '''\n",
    "\n",
    "    dot = np.dot(A,B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "\n",
    "    return cos\n",
    "\n",
    "\n",
    "def get_country(city1, country1, city2, embeddings):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        city1: a string (the capital city of country1)\n",
    "        country1: a string (the country of capital1)\n",
    "        city2: a string (the capital city of country2)\n",
    "        embeddings: a dictionary where the keys are words and values are their embeddings\n",
    "    Output:\n",
    "        countries: a dictionary with the most likely country and its similarity score\n",
    "    \"\"\"\n",
    "\n",
    "    # store the city1, country 1, and city 2 in a set called group\n",
    "    group = set((city1, country1, city2))\n",
    "\n",
    "    # get embeddings of city 1\n",
    "    city1_emb = embeddings[city1]\n",
    "\n",
    "    # get embedding of country 1\n",
    "    country1_emb = embeddings[country1]\n",
    "\n",
    "    # get embedding of city 2\n",
    "    city2_emb = embeddings[city2]\n",
    "\n",
    "    # get embedding of country 2 (it's a combination of the embeddings of country 1, city 1 and city 2)\n",
    "    # Remember: King - Man + Woman = Queen\n",
    "    vec = country1_emb - city1_emb + city2_emb\n",
    "    \n",
    "    # Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)\n",
    "    similarity = -1\n",
    "\n",
    "    # initialize country to an empty string\n",
    "    country = ''\n",
    "\n",
    "    # loop through all words in the embeddings dictionary\n",
    "    for word in embeddings.keys():\n",
    "\n",
    "        # first check that the word is not already in the 'group'\n",
    "        if word not in group:\n",
    "\n",
    "            # get the word embedding\n",
    "            word_emb = embeddings[word]\n",
    "\n",
    "            # calculate cosine similarity between embedding of country 2 and the word in the embeddings dictionary\n",
    "            cur_similarity = cosine_similarity(vec, word_emb)\n",
    "\n",
    "            # if the cosine similarity is more similar than the previously best similarity...\n",
    "            if cur_similarity > similarity:\n",
    "\n",
    "                # update the similarity to the new, better similarity\n",
    "                similarity = cur_similarity\n",
    "\n",
    "                # store the country as a tuple, which contains the word and the similarity\n",
    "                country = (word, cur_similarity)\n",
    "\n",
    "    return country\n",
    "\n",
    "\n",
    "def get_accuracy(word_embeddings, data):\n",
    "    '''\n",
    "    Input:\n",
    "        word_embeddings: a dictionary where the key is a word and the value is its embedding\n",
    "        data: a pandas dataframe containing all the country and capital city pairs\n",
    "    \n",
    "    Output:\n",
    "        accuracy: the accuracy of the model\n",
    "    '''\n",
    "\n",
    "    # initialize num correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through the rows of the dataframe\n",
    "    for i, row in data.iterrows():\n",
    "\n",
    "        # get city1\n",
    "        city1 = row[0]\n",
    "\n",
    "        # get country1\n",
    "        country1 = row[1]\n",
    "\n",
    "        # get city2\n",
    "        city2 =  row[2]\n",
    "\n",
    "        # get country2\n",
    "        country2 = row[3]\n",
    "\n",
    "        # use get_country to find the predicted country2\n",
    "        predicted_country2, _ = get_country(city1, country1, city2, word_embeddings)\n",
    "\n",
    "        # if the predicted country2 is the same as the actual country2...\n",
    "        if predicted_country2 == country2:\n",
    "            # increment the number of correct by 1\n",
    "            num_correct += 1\n",
    "\n",
    "    # get the number of rows in the data dataframe (length of dataframe)\n",
    "    m = len(data)\n",
    "\n",
    "    # calculate the accuracy by dividing the number correct by m\n",
    "    accuracy = num_correct / m\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform word vectors\n",
    "\n",
    "Suppose\n",
    "- $X$ is the english word vectors\n",
    "- $Y$ is the french word vectors\n",
    "- $R$ is the mapping matrix\n",
    "\n",
    "Step to learn $R$ will be\n",
    "- initialize $R$\n",
    "- For loop\n",
    "    - $Loss = \\|{XR-Y}\\|_{F}$ (frobenius norm - square all elements of matrix and add them up)\n",
    "    - $g = \\dfrac{d}{dR}Loss$\n",
    "    - $R = R - \\alpha g$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # get the english words (the keys in the dictionary) and store in a set()\n",
    "    english_set = set(english_vecs.keys())\n",
    "    \n",
    "    # get the french words (keys in the dictionary) and store in a set()\n",
    "    french_set = set(french_vecs.keys())\n",
    "    \n",
    "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    # loop through all english, french word pairs in the english french dictionary\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # check that the french word has an embedding and that the english word has an embedding\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            # get the english embedding\n",
    "            en_vec = english_vecs[en_word]\n",
    "\n",
    "            # get the french embedding\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "\n",
    "            # add the english embedding to the list\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # add the french embedding to the list\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # stack the vectors of X_l into a matrix X\n",
    "    X = np.array(X_l, ndmin=2)\n",
    "\n",
    "    # stack the vectors of Y_l into a matrix Y\n",
    "    Y = np.array(Y_l, ndmin=2)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "\n",
    "    # m is the number of rows in X\n",
    "    num_row, num_col = X.shape\n",
    "    m = num_row\n",
    "    \n",
    "    # diff is XR - Y\n",
    "    diff = np.dot(X,R) - Y\n",
    "\n",
    "    # diff_squared is the element-wise square of the difference\n",
    "    diff_squared = np.square(diff)\n",
    "\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
    "    loss = sum_diff_squared / m\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    \n",
    "    # m is the number of rows in X\n",
    "    num_row, num_col = X.shape\n",
    "    m = num_row\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m\n",
    "    gradient = np.dot(X.T, (np.dot(X, R) - Y)) * (2/m) \n",
    "    return gradient\n",
    "\n",
    "\n",
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        \n",
    "        # use the function that you defined to compute the gradient\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -= learning_rate * gradient\n",
    "    return R\n",
    "\n",
    "\n",
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    \n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(v, row)\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[len(sorted_ids)-k:]\n",
    "    \n",
    "    return k_idx\n",
    "\n",
    "\n",
    "def test_vocabulary(X, Y, R):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    # The prediction is X times R\n",
    "    pred = np.dot(X,R)\n",
    "    num_row, num_col = pred.shape\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i], Y)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct / num_row\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_document_embedding(tweet, en_embeddings): \n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    '''\n",
    "    doc_embedding = np.zeros(300)\n",
    "\n",
    "    # process the document into a list of words (process the tweet)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    \n",
    "    for word in processed_doc:\n",
    "        # add the word embedding to the running total for the document embedding\n",
    "        if word in en_embeddings:\n",
    "            doc_embedding += en_embeddings[word]\n",
    "    \n",
    "    return doc_embedding\n",
    "\n",
    "\n",
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "    '''\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in our dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - document_vec_matrix: matrix of tweet embeddings.\n",
    "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    '''\n",
    "\n",
    "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
    "    # the value is the document embedding for that document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # this is list that will store the document vectors\n",
    "    document_vec_l = []\n",
    "\n",
    "    for i, doc in enumerate(all_docs):\n",
    "\n",
    "        # get the document embedding of the tweet\n",
    "        doc_embedding = get_document_embedding(doc, en_embeddings)\n",
    "\n",
    "        # save the document embedding into the ind2Tweet dictionary at index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "\n",
    "        # append the document embedding to the list of document vectors\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "    # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict\n",
    "\n",
    "\n",
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # for the set of planes,\n",
    "    # calculate the dot product between the vector and the matrix containing the planes\n",
    "    # remember that planes has shape (300, 10)\n",
    "    # The dot product will have the shape (1,10)\n",
    "    dot_product = np.dot(v, planes)\n",
    "\n",
    "    # get the sign of the dot product (1,10) shaped vector\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "    \n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    # if the sign is 0, i.e. the vector is in the plane, consider the sign to be positive\n",
    "    h = (sign_of_dot_product>=0)\n",
    "    \n",
    "    h = h.astype(int)\n",
    "    \n",
    "    h = np.squeeze(h)\n",
    "    \n",
    "    # initialize the hash value to 0\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        # increment the hash value by 2^i * h_i\n",
    "        hash_value += (2 ** i) * h[i]\n",
    "    \n",
    "    # cast hash_value as an integer\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value\n",
    "\n",
    "\n",
    "def make_hash_table(vecs, planes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_row, num_col = planes.shape\n",
    "    num_of_planes = num_col\n",
    "\n",
    "    # number of buckets is 2^(number of planes)\n",
    "    num_buckets = 2 ** num_of_planes\n",
    "\n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {new_list: [] for new_list in range(num_buckets)}\n",
    "\n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {new_list: [] for new_list in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):\n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
