{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not standard network\n",
    "\n",
    "- inputs, outputs can be different lengths in different examples\n",
    "- doesn't share features learned across different positions of text\n",
    "\n",
    "## Recurrent neural network\n",
    "\n",
    "- $a^{<0>} = \\overrightarrow{0}$\n",
    "- $a^{<1>} = g_{1}(W_{aa}a^{<0>} + W_{ax}X^{<1>} + b_{a})$ (tanh/relu)\n",
    "- $\\hat{y}^{<1>} = g_{2}(W_{ya}a^{<1>} + b_{y})$ (sigmoid)\n",
    "- $a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}X^{<t>} + b_{a}) = g(W_{a}[a^{<t-1>}, X^{<t>}] + b_{a})$ where $[W_{aa} \\vdots W_{ax}] = W_{a}$ and $[a^{<t-1>}, X^{<t>}]$ = |\n",
    "$\\begin{bmatrix}\n",
    "   a^{<t-1>} \\\\\n",
    "   X^{<t>} \\\\\n",
    " \\end{bmatrix}$\n",
    "- $\\hat{y}^{<t>} = g(W_{ya}a^{<t>} + b_{y}) = g(W_{y}a^{<t>} + b_{y})$ \n",
    "\n",
    "## Backpropagation through time\n",
    "\n",
    "- $L^{<t>}(\\hat{y}^{<t>}, y^{<t>}) = -y^{<t>}log\\hat{y}^{<t>} - (1-y^{<t>})log(1-\\hat{y}^{<t>})$\n",
    "- $L(\\hat{y},y) = \\displaystyle\\sum_{t=1}^{T_{y}}L^{<t>}(\\hat{y}^{<t>}, y^{<t>})$\n",
    "\n",
    "## RNN types\n",
    "\n",
    "- one to many (ex. music generation)\n",
    "- many to one (ex. sentiment classification)\n",
    "- many to many (ex. name entity recognition)\n",
    "- many to many (ex. machine translation)\n",
    "\n",
    "## Language modelling\n",
    "\n",
    "- ex. P(The apple and pair salad) = $3.2x10^{-13}$, P(The apple and peer salad) = $5.7x10^{-13}$\n",
    "- ex. \"cats average 15 hours of sleep a day. (EOS)\"\n",
    "    - $L^{<t>}(\\hat{y}^{<t>}, y^{<t>}) = -\\displaystyle\\sum_{i}y_{i}^{<t>}log\\hat{y}_{i}^{<t>}$\n",
    "    - $L = \\displaystyle\\sum_{t}L^{<t>}(\\hat{y}^{<t>}, y^{<t>})$\n",
    "    - $P(y^{<1>}, y^{<2>}, y^{<3>}) = P(y^{<1>})P(y^{<2>}|y^{<1>})P(y^{<3>}|y^{<1>},y^{<2>})$\n",
    "    \n",
    "## Vanishing gradients with RNNs\n",
    "\n",
    "- ex. \"the cats, which, ..., were full\" vs \"the cat, which, ..., was full\"\n",
    "- capturing long-term dependencies is hard\n",
    "\n",
    "## Gated recurrent unit\n",
    "\n",
    "- RNN unit\n",
    "    - $a^{<t>} = g(W_{a}[a^{<t-1>}, x^{<t>}] + b_{a})$ (where $g$ is tanh)\n",
    "- GRU\n",
    "    - let $c$ = memory cell\n",
    "    - $c^{<t>} = a^{<t>}$\n",
    "    - $\\tilde{c}^{<t>} = tanh(W_{c}[c^{<t-1>},x^{<t>}] + b_{c})$\n",
    "    - $\\Gamma_{u} = \\sigma(W_{u}[c^{<t-1>},x^{<t>}] + b_{u})$\n",
    "    - $c^{<t>} = \\Gamma_{u}\\tilde{c}^{<t>} + (1-\\Gamma_{u}){c}^{<t-1>}$ (if vectors, multiplications are element-wise)\n",
    "- Full GRU\n",
    "    - $\\tilde{c}^{<t>} = tanh(W_{c}[\\Gamma_{r}c^{<t-1>},x^{<t>}] + b_{c})$\n",
    "    - $\\Gamma_{u} = \\sigma(W_{u}[c^{<t-1>},x^{<t>}] + b_{u})$\n",
    "    - $\\Gamma_{r} = \\sigma(W_{r}[c^{<t-1>},x^{<t>}] + b_{r})$\n",
    "    - $c^{<t>} = \\Gamma_{u}\\tilde{c}^{<t>} + (1-\\Gamma_{u}){c}^{<t-1>}$\n",
    "    - $a^{<t>} = c^{<t>}$\n",
    "    \n",
    "## Long short term memory (LSTM)\n",
    "\n",
    "- $\\tilde{c}^{<t>} = tanh(W_{c}[a^{<t-1>},x^{<t>}] + b_{c})$\n",
    "- $\\Gamma_{u} = \\sigma(W_{u}[c^{<t-1>},x^{<t>}] + b_{u})$ (update)\n",
    "- $\\Gamma_{f} = \\sigma(W_{f}[c^{<t-1>},x^{<t>}] + b_{f})$ (forget)\n",
    "- $\\Gamma_{o} = \\sigma(W_{o}[c^{<t-1>},x^{<t>}] + b_{o})$ (output)\n",
    "- $c^{<t>} = \\Gamma_{u}\\tilde{c}^{<t>} + \\Gamma_{u}{c}^{<t-1>}$\n",
    "- $a^{<t>} = \\Gamma_{o}tanhc^{<t>}$\n",
    "\n",
    "## Bidirectional RNN\n",
    "\n",
    "- getting information from the future\n",
    "    - ex. He said, \"Teddy bears are on sale!\"\n",
    "    - ex. He said, \"Teddy Roosevelt was a great President\"\n",
    "- $\\hat{y}^{<t>} = g(W_{y}[\\overrightarrow{a}^{<t>}, \\overleftarrow{a}^{<t>}] + b_{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word representation\n",
    "\n",
    "- ex.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <th></th>\n",
    "    <th>Man (5391)</th>\n",
    "    <th>Woman (9853)</th>\n",
    "    <th>King (4914)</th>\n",
    "    <th>Queen (7157)</th>\n",
    "    <th>Apple (456)</th>\n",
    "    <th>Orange (6257)</th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Gender</td>\n",
    "    <td>-1</td>\n",
    "    <td>-1</td>\n",
    "    <td>-0.95</td>\n",
    "    <td>0.97</td>\n",
    "    <td>0.00</td>\n",
    "    <td>0.01</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Royal</td>\n",
    "    <td>0.01</td>\n",
    "    <td>0.02</td>\n",
    "    <td>0.93</td>\n",
    "    <td>0.95</td>\n",
    "    <td>-0.01</td>\n",
    "    <td>0.00</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Age</td>\n",
    "    <td>0.03</td>\n",
    "    <td>0.02</td>\n",
    "    <td>0.7</td>\n",
    "    <td>0.69</td>\n",
    "    <td>0.03</td>\n",
    "    <td>-0.02</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Food</td>\n",
    "    <td>0.04</td>\n",
    "    <td>0.01</td>\n",
    "    <td>0.02</td>\n",
    "    <td>0.01</td>\n",
    "    <td>0.95</td>\n",
    "    <td>0.97</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "## Transfer learning and word embeddings\n",
    "\n",
    "- learn word embeddings from large text corpus (1-100B words) (or download pre-trained embedding online)\n",
    "- transfer embedding to new task with smaller training set (say, 100k words)\n",
    "- (optional) continue to fine tune the word embeddings with new data\n",
    "\n",
    "## Word2Vec (\"skip gram\")\n",
    "\n",
    "- vocab size = 10,000k\n",
    "- content c (\"orange\") $\\rightarrow$ target t (\"juice\")\n",
    "- $o_{c} \\rightarrow E \\rightarrow e_{c} \\rightarrow o$ (softmax) $\\rightarrow \\hat{y}$\n",
    "- softmax: $p(t|c) = \\dfrac{e^{\\theta_{t}^{T}}e_{c}}{\\displaystyle\\sum_{i=1}^{10000}e^{\\theta_{j}^{T}}e_{c}}$\n",
    "    - $\\theta_{t}$ = parameter associated with output $t$\n",
    "    - $L(\\hat{y}, t) = -\\displaystyle\\sum_{i=1}^{10000}y_{i}log\\hat{y}_{i}$ \n",
    "    \n",
    "## Negative sampling\n",
    "\n",
    "- ex. \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <th>context</th>\n",
    "    <th>word</th>\n",
    "    <th>target</th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>orange</td>\n",
    "    <td>juice</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>orange</td>\n",
    "    <td>king</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>orange</td>\n",
    "    <td>book</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>orange</td>\n",
    "    <td>the</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>orange</td>\n",
    "    <td>of</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "- $P(y=1|c,t) = \\sigma(\\theta_{t}^{T}e_{c})$\n",
    "- selecting negative examples\n",
    "    - $p(w_{i}) = \\dfrac{f(w_{i})^{3/4}}{\\displaystyle\\sum_{j=1}^{10000}f(w_{j})^{3/4}}$\n",
    "    \n",
    "## GloVe (global vectors for word representation)\n",
    "\n",
    "- $X_{ij}$ = number of times $j$ appears in context of $i$ ($i$ = context, $j$ = target)\n",
    "- $X_{ij} = X_{ji}$\n",
    "- minimize $\\displaystyle\\sum_{i=1}^{10000}\\displaystyle\\sum_{j=1}^{10000}f(X_{ij})(\\theta_{i}^{T}e_{j} + b_{i} + b_{j}^{'} -logX_{ij})^{2}$\n",
    "    - $f(X_{ij})$ = weighting term, which is equal to zero if $X_{ij} = 0$\n",
    "    - $\\theta_{i}, e_{j}$ are symmetric\n",
    "    - $e_{w}^{find} = \\dfrac{e_{w} + \\theta_{w}}{2}$\n",
    "    \n",
    "## Sentiment classification\n",
    "\n",
    "- use RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to sequence model\n",
    "\n",
    "- ex. French to English translation\n",
    "    - Jane visite l'Afrique en septembre $(x^{<1>}, x^{<2>}, x^{<3>}, x^{<4>}, x^{<5>})$\n",
    "    - Jane is visiting Africa in September $(y^{<1>}, y^{<2>}, y^{<3>}, y^{<4>}, y^{<5>}, y^{<6>})$\n",
    "- conditional language model\n",
    "    - $P(y^{<1>} \\dots y^{<T_{y}>}|x^{<1>} \\dots x^{<T_{x}>})$\n",
    "- objective\n",
    "    - $\\underset{y^{<1>} \\dots y^{<T_{y}>}}{\\arg\\max}P(y^{<1>} \\dots y^{<T_{y}>}|x)$\n",
    "    \n",
    "## Beam search\n",
    "\n",
    "- define beam width (say 3)\n",
    "- then\n",
    "    - $P(y^{<1>}|x)$; pick top 3\n",
    "    - $P(y^{<1>},y^{<2>}|x) = P(y^{<1>}|x)P(y^{<2>}|x,y^{<1>})$; pick top 3\n",
    "    - and so on\n",
    "- in general\n",
    "    - $\\underset{y}{\\arg\\max}\\displaystyle\\prod_{t=1}^{T_{y}}P(y^{<t>}|x, y^{<1>} \\dots y^{<t-1>})$\n",
    "- length normalization\n",
    "    - $\\underset{y}{\\arg\\max}\\dfrac{1}{T_{y}}\\displaystyle\\sum_{y=1}^{T_{y}}P(y^{<t>}|x, y^{<1>} \\dots y^{<t-1>})$\n",
    "- large B: better result but slower\n",
    "- small B: worse result but faster\n",
    "- error analysis\n",
    "    - ex. Jane visite l'Afrique en septembre\n",
    "    - human: Jane visits Africa in September $(y^{*})$\n",
    "    - algorithm: Jane visited Africa last September $(\\hat{y})$\n",
    "    - RNN computes $P(y^{*}|x), P(\\hat{y}|x)$\n",
    "- case 1: $P(y^{*}|x) \\gt P(\\hat{y}|x)$\n",
    "    - beam search chose $\\hat{y}$, but $y^{*}$ attains higher $P(y|x)$\n",
    "    - conclusion: beam search is at fault\n",
    "- case 2: $P(y^{*}|x) \\le P(\\hat{y}|x)$\n",
    "    - $y^{*}$ is a better translation than $\\hat{y}$, but RNN predicted $P(y^{*}|x) \\le P(\\hat{y}|x)$\n",
    "    - RNN model is at fault\n",
    "    \n",
    "## Bleu score\n",
    "\n",
    "- evaluate machine translation\n",
    "    - ex. Le chat est sur le tapis\n",
    "    - reference 1: The cat is on the mat\n",
    "    - reference 2: There is a cat on the mat\n",
    "    - MT output: the the the the the the the $(\\hat{y})$\n",
    "- unigram\n",
    "    - $P_{1} = \\dfrac{\\displaystyle\\sum_{unigram \\in \\hat{y}}Count_{clip}(unigram)}{\\displaystyle\\sum_{unigram \\in \\hat{y}}Count(unigram)}$\n",
    "- n-gram\n",
    "    - $P_{1} = \\dfrac{\\displaystyle\\sum_{n-gram \\in \\hat{y}}Count_{clip}(n-gram)}{\\displaystyle\\sum_{n-gram \\in \\hat{y}}Count(n-gram)}$\n",
    "- BP (brevity penalty)\n",
    "    - 1 if MT_output_length $\\gt$ reference_output_length\n",
    "    - exp(1 - reference_output_length / MT_output_length) otherwise\n",
    "    \n",
    "## Attention model\n",
    "\n",
    "- $\\alpha^{<t,t'>}$ = amount of \"attention\" $y^{<t>}$ should pay to $a^{<t'>} = \\dfrac{exp(e^{<t,t'>})}{\\displaystyle\\sum_{t=1}^{T_{x}}exp(e^{<t,t'>})}$ \n",
    "- $a^{<t'>} = (\\overrightarrow{a}^{<t'>}, \\overleftarrow{a}^{<t'>})$\n",
    "- $\\displaystyle\\sum_{t'}\\alpha^{<1,t'>} = 1$\n",
    "- $c^{<1>} = \\displaystyle\\sum_{t'}\\alpha^{<1,t'>}a^{<t'>}$\n",
    "- $c^{<2>} = \\displaystyle\\sum_{t'}\\alpha^{<2,t'>}a^{<t'>}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
