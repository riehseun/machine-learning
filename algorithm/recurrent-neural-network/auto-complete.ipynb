{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161280b6-5264-41fc-892b-66fcbf77c736",
   "metadata": {},
   "source": [
    "# Autocomplete\n",
    "\n",
    "## N-grams\n",
    "\n",
    "- Sequence of $N$ words\n",
    "\n",
    "![2-3-1](images/natural-language-processing/2-3-1.png)\n",
    "\n",
    "![2-3-2](images/natural-language-processing/2-3-2.png)\n",
    "\n",
    "- Introduce notation\n",
    "    - $w_{1}^{m} = w_{1}w_{2}w_{3} \\dots w_{m}$\n",
    "    - $w_{1}^{3} = w_{1}w_{2}w_{3}$\n",
    "    - $w_{m-2}^{m} = w_{m-2}w_{m-1}w_{m}$\n",
    "    \n",
    "## Unigram probability\n",
    "\n",
    "- $P(w) = \\dfrac{C(w)}{m}$\n",
    "\n",
    "## Bigram probability\n",
    "\n",
    "![2-3-3](images/natural-language-processing/2-3-3.png)\n",
    "\n",
    "## Trigram probability\n",
    "\n",
    "- $P(w_{3}|w_{1}^{2}) = \\dfrac{C(w_{1}^{2}w_{3})}{C(w_{1}^{2})}$\n",
    "- $C(w_{1}^{2}w_{3}) = C(w_{1}w_{2}w_{3}) = C(w_{1}^{3})$ \n",
    "\n",
    "## N-gram probability\n",
    "\n",
    "- $P(w_{N}|w_{1}^{N-1}) = \\dfrac{C(w_{1}^{N-1}w_{N})}{C(w_{1}^{N-1})}$\n",
    "- $C(w_{1}^{N-1}w_{N}) = C(w_{1}^{N})$\n",
    "\n",
    "## Sequence probability\n",
    "\n",
    "- Markov assupmtion indicates that only the last word matters\n",
    "- $P(w_{n}|w_{1}^{n-1}) \\approx P(w_{n}|w_{n-N+1}^{n-1})$\n",
    "- $P(w_{1}^{n}) \\approx \\displaystyle\\prod_{i=1}^{n} P(w_{n} | w_{i-1}) = P(w_{1})P(w_{2}|w_{1}) \\dots P(w_{n}|w_{n-1})$\n",
    "\n",
    "## Starting and ending sentences\n",
    "\n",
    "- Append start token `<s>` in the beginning of the sentence. For N-gram, add $N-1$ `<s>`. \n",
    "- For end token `</s>`, you just need one even for N-gram\n",
    "\n",
    "![2-3-4](images/natural-language-processing/2-3-4.png)\n",
    "\n",
    "## N-gram language model\n",
    "\n",
    "- Count matrix\n",
    "    - Rows correspond to unique corpus $N-1$ grams\n",
    "    - Columns correspond to unique corpus words\n",
    "    \n",
    "![2-3-5](images/natural-language-processing/2-3-5.png)\n",
    "\n",
    "- To convert this to the probability matrix\n",
    "    - $P(w_{n}|w_{n-N+1}^{n-1}) = \\dfrac{C(w_{n-N+1}^{n-1}, w_{n})}{C(w_{n-N+1}^{n-1})}$\n",
    "    - $sum(row) = \\displaystyle\\sum_{w \\in V} C(w_{n-N+1}^{n-1}, w) = C(w_{n-N+1}^{n-1}, w)$\n",
    "    \n",
    "- To compute the probability of a sequence\n",
    "    - $P(w_{1}^{n}) \\approx \\displaystyle\\prod_{i=1}^{n} P(w_{n} | w_{i-1})$\n",
    "    \n",
    "- To avoid underflow\n",
    "    - $log(P(w_{1}^{n})) \\approx \\displaystyle\\prod_{i=1}^{n} log(P(w_{n} | w_{i-1}))$\n",
    "    \n",
    "![2-3-6](images/natural-language-processing/2-3-6.png)\n",
    "\n",
    "- Create the generative model\n",
    "    - Choose sentence start\n",
    "    - Choose next bigram starting with previous word\n",
    "    - Continue until `</s>` is picked\n",
    "    \n",
    "## Language model evaluation\n",
    "\n",
    "- Perplexity: low if sentences look like written by human, high if sentences look like written by machine\n",
    "- $PP(W) = P(s_{1}, s_{1} \\dots s_{m})^{-\\frac{1}{m}} = \\left(\\displaystyle\\prod_{i=1}^{m} \\displaystyle\\prod_{j=1}^{|s_{i}|} \\dfrac{1}{P(w_{j}^{(i)}|w_{j-1}^{(i)})}\\right)^{-\\frac{1}{m}}$\n",
    "- $log PP(W) = -\\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m} log_{2}(P(w_{i} | w_{i-1})$\n",
    "\n",
    "# Out of vocabulary words\n",
    "\n",
    "- Open vocabulary: words from outside the vocabulary\n",
    "- Handle open vocabulary \n",
    "    - Create vocabulary $V$\n",
    "    - Replace any word in corpus and not in $V$ by `<UNK>`\n",
    "    - Count the probabilities with `<UNK>` as with any other word\n",
    "    \n",
    "![2-3-7](images/natural-language-processing/2-3-7.png)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60db99da-d51b-47fd-93c8-54803a507da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    \n",
    "    Args:\n",
    "        data: str\n",
    "    \n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = data.split(\"\\n\")\n",
    "    \n",
    "    # Additional clearning (This part is already implemented)\n",
    "    # - Remove leading and trailing spaces from each sentence\n",
    "    # - Drop sentences if they are empty strings.\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea06b875-4b3f-46d7-b99d-43b7dd681017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into tokens (words)\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list of lists of tokenized sentences\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    # Go through each sentence\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Convert to lowercase letters\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Convert into a list of words\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # append the list of words to the list of lists\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44971608-40c2-4fde-ab5d-60b72394ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    \"\"\"\n",
    "    Make a list of tokenized sentences\n",
    "    \n",
    "    Args:\n",
    "        data: String\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the sentences by splitting up the data\n",
    "    sentences = split_to_sentences(data)\n",
    "    \n",
    "    # Get the list of lists of tokens by tokenizing the sentences\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1138d524-0c27-4987-85a8-bc3c86d02164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    \"\"\"\n",
    "    Count the number of word appearence in the tokenized sentences\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "    \n",
    "    Returns:\n",
    "        dict that maps word (str) to the frequency (int)\n",
    "    \"\"\"\n",
    "        \n",
    "    word_counts = {}\n",
    "    \n",
    "    # Loop through each sentence\n",
    "    for sentence in tokenized_sentences: # complete this line\n",
    "        \n",
    "        # Go through each token in the sentence\n",
    "        for token in sentence: # complete this line\n",
    "\n",
    "            # If the token is not in the dictionary yet, set the count to 1\n",
    "            if token not in word_counts: # complete this line\n",
    "                word_counts[token] = 1\n",
    "            \n",
    "            # If the token is already in the dictionary, increment the count by 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c13255-ebe0-4d58-a79d-9350cd02e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    \"\"\"\n",
    "    Find the words that appear N times or more\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of sentences\n",
    "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        List of words that appear N times or more\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to contain the words that\n",
    "    # appear at least 'minimum_freq' times.\n",
    "    closed_vocab = []\n",
    "    \n",
    "    # Get the word couts of the tokenized sentences\n",
    "    # Use the function that you defined earlier to count the words\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    \n",
    "    # for each word and its count\n",
    "    for word, cnt in word_counts.items(): # complete this line\n",
    "        \n",
    "        # check that the word's count\n",
    "        # is at least as great as the minimum count\n",
    "        if cnt >= count_threshold:\n",
    "            \n",
    "            # append the word to the list\n",
    "            closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b94c66-97fc-49ef-8cc5-3333e8fdfe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with '<unk>' token.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "        vocabulary: List of strings that we will use\n",
    "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "    \n",
    "    # Place vocabulary into a set for faster search\n",
    "    vocabulary = set(vocabulary)\n",
    "    \n",
    "    # Initialize a list that will hold the sentences\n",
    "    # after less frequent words are replaced by the unknown token\n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    # Go through each sentence\n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        # Initialize the list that will contain\n",
    "        # a single sentence with \"unknown_token\" replacements\n",
    "        replaced_sentence = []\n",
    "        \n",
    "        # for each token in the sentence\n",
    "        for token in sentence: # complete this line\n",
    "            # Check if the token is in the closed vocabulary\n",
    "            if token in vocabulary: # complete this line\n",
    "                # If so, append the word to the replaced_sentence\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                # otherwise, append the unknown token instead\n",
    "                replaced_sentence.append('<unk>')\n",
    "        \n",
    "        # Append the list of tokens to the list of lists\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4007868a-0a8c-47a8-ac59-debc48d6c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "    \"\"\"\n",
    "    Preprocess data, i.e.,\n",
    "        - Find tokens that appear at least N times in the training data.\n",
    "        - Replace tokens that appear less than N times by \"<unk>\" both for training and test data.        \n",
    "    Args:\n",
    "        train_data, test_data: List of lists of strings.\n",
    "        count_threshold: Words whose count is less than this are \n",
    "                      treated as unknown.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of\n",
    "        - training data with low frequent words replaced by \"<unk>\"\n",
    "        - test data with low frequent words replaced by \"<unk>\"\n",
    "        - vocabulary of words that appear n times or more in the training data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the closed vocabulary using the train data\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    \n",
    "    # For the train data, replace less common words with \"<unk>\"\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token=\"<unk>\")\n",
    "    \n",
    "    # For the test data, replace less common words with \"<unk>\"\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token=\"<unk>\")\n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2755d3-7377-4510-85b8-3faf95a43321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \"\"\"\n",
    "    Count all n-grams in the data\n",
    "    \n",
    "    Args:\n",
    "        data: List of lists of words\n",
    "        n: number of words in a sequence\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "\n",
    "    # Go through each sentence in the data\n",
    "    for sentence in data: # complete this line\n",
    "        \n",
    "        # prepend start token n times, and  append <e> one time\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        \n",
    "        # convert list to tuple\n",
    "        # So that the sequence of words can be used as\n",
    "        # a key in the dictionary\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        # Use 'i' to indicate the start of the n-gram\n",
    "        # from index 0\n",
    "        # to the last index where the end of the n-gram\n",
    "        # is within the sentence.\n",
    "        \n",
    "        for i in range(len(sentence)): # complete this line\n",
    "\n",
    "            # Get the n-gram from i to i+n\n",
    "            n_gram = sentence[i:i+n] \n",
    "            \n",
    "            # If not unigram, skip ('<e>',) \n",
    "            if n_gram == ('<e>',) and n != 1:\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # check if the n-gram is in the dictionary\n",
    "                if n_gram in n_grams: # complete this line\n",
    "\n",
    "                    # Increment the count for this n-gram\n",
    "                    n_grams[n_gram] += 1\n",
    "                else:\n",
    "                    # Initialize this n-gram count to 1\n",
    "                    n_grams[n_gram] = 1\n",
    "                \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2d6459-e71b-4f91-870d-6e0962d1b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Args:\n",
    "        word: next word\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of words in the vocabulary\n",
    "        k: positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    # convert list to tuple to use it as a dictionary key\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "        \n",
    "    # Set the denominator\n",
    "    # If the previous n-gram exists in the dictionary of n-gram counts,\n",
    "    # Get its count.  Otherwise set the count to zero\n",
    "    # Use the dictionary that has counts for n-grams\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0 \n",
    "    \n",
    "    # Calculate the denominator using the count of the previous n gram\n",
    "    # and apply k-smoothing\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "    \n",
    "    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
    "    n_plus1_gram = (previous_n_gram[0], word)\n",
    "    \n",
    "    # Set the count to the count in the dictionary,\n",
    "    # otherwise 0 if not in the dictionary\n",
    "    # use the dictionary that has counts for the n-gram plus current word\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0 \n",
    "        \n",
    "    # Define the numerator use the count of the n-gram plus current word,\n",
    "    # and apply smoothing\n",
    "    numerator = n_plus1_gram_count + k\n",
    "\n",
    "    # Calculate the probability as the numerator divided by denominator\n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a71091-620b-4f06-a723-55d1eb649180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of strings\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of unique words in the vocabulary\n",
    "        k: Positive smoothing constant\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity score\n",
    "    \"\"\"\n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # prepend <s> and append <e>\n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    \n",
    "    # Cast the sentence from a list to a tuple\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    # length of sentence (after adding <s> and <e> tokens)\n",
    "    N = len(sentence)\n",
    "    \n",
    "    # The variable p will hold the product\n",
    "    # that is calculated inside the n-root\n",
    "    # Update this in the code below\n",
    "    product_pi = 1.0\n",
    "    \n",
    "    # Index t ranges from n to N - 1, inclusive on both ends\n",
    "    for t in range(n, N): # complete this line\n",
    "\n",
    "        # get the n-gram preceding the word at position t\n",
    "        n_gram = sentence[t-n:t]\n",
    "        \n",
    "        # get the word at position t\n",
    "        word = sentence[t]\n",
    "        \n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        # using the n-gram counts, n-plus1-gram counts,\n",
    "        # vocabulary size, and smoothing constant\n",
    "        probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size)\n",
    "        \n",
    "        # Update the product of the probabilities\n",
    "        # This 'product_pi' is a cumulative product \n",
    "        # of the (1/P) factors that are calculated in the loop\n",
    "        product_pi *= 1/probability\n",
    "\n",
    "    # Take the Nth root of the product\n",
    "    perplexity = product_pi ** (1/N)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3393c59f-6dfb-47e6-9af3-107677e85e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    \n",
    "    Args:\n",
    "        previous_tokens: The sentence you input where each token is a word. Must have length > n \n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "        start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of \n",
    "          - string of the most likely next word\n",
    "          - corresponding probability\n",
    "    \"\"\"\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # From the words that the user already typed\n",
    "    # get the most recent 'n' words as the previous n-gram\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    # Estimate the probabilities that each word in the vocabulary\n",
    "    # is the next word,\n",
    "    # given the previous n-gram, the dictionary of n-gram counts,\n",
    "    # the dictionary of n plus 1 gram counts, and the smoothing constant\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    \n",
    "    # Initialize suggested word to None\n",
    "    # This will be set to the word with highest probability\n",
    "    suggestion = None\n",
    "    \n",
    "    # Initialize the highest word probability to 0\n",
    "    # this will be set to the highest probability \n",
    "    # of all words to be suggested\n",
    "    max_prob = 0\n",
    "    \n",
    "    # For each word and its probability in the probabilities dictionary:\n",
    "    for word, prob in probabilities.items(): # complete this line\n",
    "        \n",
    "        # If the optional start_with string is set\n",
    "        if start_with : # complete this line\n",
    "            \n",
    "            # Check if the beginning of word does not match with the letters in 'start_with'\n",
    "            if start_with not in word: # complete this line\n",
    "\n",
    "                # if they don't match, skip this word (move onto the next word)\n",
    "                continue # complete this line\n",
    "        \n",
    "        # Check if this word's probability\n",
    "        # is greater than the current maximum probability\n",
    "        if prob > max_prob: # complete this line\n",
    "            \n",
    "            # If so, save this word as the best suggestion (so far)\n",
    "            suggestion = word\n",
    "            \n",
    "            # Save the new maximum probability\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87171b-5a82-441c-b7fb-3005714d00e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
