{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62720ab2-4fbc-4453-a71d-41fe65217f7c",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "- $Z^{[1]} = w^{[1]}X + b^{[1]}$\n",
    "- $A^{[1]} = \\sigma(Z^{[1]})$\n",
    "- $Z^{[2]} = w^{[2]}A^{[1]} + b^{[2]}$\n",
    "- $A^{[2]} = \\sigma(Z^{[2]})$\n",
    "    - Where \n",
    "        - $X$: $(n_{x}, m)$ matrix\n",
    "        - $Z$: $($number of hidden units, $m)$ matrix\n",
    "        - $A$: $($number of hidden units, $m)$ matrix\n",
    "        \n",
    "## Activation function\n",
    "\n",
    "- Binary classification? use sigmoid\n",
    "- All other cases? use RELU (rectified linear unit)\n",
    "\n",
    "Why use non-linear activation function?\n",
    "- If use linear activation funciotn, having layers becomes meaningless because combinations of linear function reduce down to a single linear function\n",
    "\n",
    "## Gradient descent for neural networks\n",
    "\n",
    "Parameters\n",
    "- $n_{x} = n^{[0]}$: number of features\n",
    "- $n^{[1]}$: number of hidden units\n",
    "- $n^{[2]} = 1$: number of output units\n",
    "- $w^{[1]}$: $(n^{[1]}, n^{[0]})$ matrix\n",
    "- $b^{[1]}$: $(n^{[1]}, 1)$ matrix\n",
    "- $w^{[2]}$: $(n^{[2]}, n^{[1]})$ matrix\n",
    "- $b^{[2]}$: $(n^{[2]}, 1)$ matrix\n",
    "\n",
    "Cost function\n",
    "- $J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{n}L(\\hat{y}, y)$ where $L(\\hat{y}, y) = a^{[2]}$\n",
    "\n",
    "Gradient descent\n",
    "- Repeat\n",
    "    - Compute prediction $\\hat{y}^{(i)}$ for $i = 1 \\dots m$\n",
    "    - $dw^{[1]} = \\dfrac{\\partial J}{\\partial w^{[1]}}$, $db^{[1]} = \\dfrac{\\partial J}{\\partial b^{[1]}}$, $dw^{[2]} = \\dfrac{\\partial J}{\\partial w^{[2]}}$, $db^{[2]} = \\dfrac{\\partial J}{\\partial b^{[2]}}$\n",
    "    - $w^{[1]} = w^{[1]} - \\alpha dw^{[1]}$, $b^{[1]} = b^{[1]} - \\alpha db^{[1]}$, $w^{[2]} = w^{[2]} - \\alpha dw^{[2]}$, $b^{[2]} = b^{[2]} - \\alpha db^{[2]}$\n",
    "    \n",
    "Backward propagation\n",
    "- $dZ^{[2]} = A^{[2]} - Y$\n",
    "- $dw^{[2]} = \\dfrac{1}{m}dZ^{[2]}A^{[1]^{T}}$\n",
    "- $db^{[2]} = \\dfrac{1}{m}$np.sum$(dZ^{[2]}$, axis=1, keepdims=True$)$\n",
    "- $dZ^{[1]} = w^{[2]^{T}}dZ^{[2]} * g^{[1]^{'}}(Z^{[1]})$\n",
    "- $dw^{[1]} = \\dfrac{1}{m}dZ^{[1]}X^{T}$\n",
    "- $db^{[1]} = \\dfrac{1}{m}$np.sum$(dZ^{[1]}$, axis=1, keepdims=True$)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d972a5e-54dd-4aeb-a67e-493bc7453f62",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "<img src=\"img/classification_kiank.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49433007-2e8b-4dd4-9ad1-f3f048d35e44",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3af91-d31b-4fa2-bbed-3aee091817d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "from dl_utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d6c13d-8d9d-416e-8b61-571e4403d10b",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3d6cb-9696-42ec-b1bd-4c825af05f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273413f-3192-4155-b051-339a6166466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- Input dataset of shape. (input size, number of examples)\n",
    "    Y -- Labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- The size of the input layer.\n",
    "    n_h -- The size of the hidden layer.\n",
    "    n_y -- The size of the output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2a513-731f-43a8-ae90-c663505cc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- Size of the input layer.\n",
    "    n_h -- Size of the hidden layer.\n",
    "    n_y -- Size of the output layer.\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape. (n_h, n_x)\n",
    "                    b1 -- bias vector of shape. (n_h, 1)\n",
    "                    W2 -- weight matrix of shape. (n_y, n_h)\n",
    "                    b2 -- bias vector of shape. (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)  # we set up a seed so that we get expected output.\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d7ba77-f7c1-4312-a12b-b981ec15e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- Input data of size. (n_x, m)\n",
    "    parameters -- Python dictionary containing your parameters. (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation.\n",
    "    cache -- A dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2. (probabilities)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a0052-ec69-405e-bea1-c62fb0539892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost.\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape. (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape. (1, number of examples)\n",
    "    parameters -- Python dictionary containing your parameters W1, b1, W2 and b2.\n",
    "    \n",
    "    Returns:\n",
    "    cost -- Cross-entropy cost given equation.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]  # Number of example.\n",
    "\n",
    "    # Compute the cross-entropy cost.\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), (1-Y))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    \n",
    "    # Makes sure cost is the dimension we expect. \n",
    "    # E.g. turns [[17]] into 17 .\n",
    "    cost = float(np.squeeze(cost))  \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709b3a3-d539-46b4-9a9a-e173d255f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- Python dictionary containing our parameters. \n",
    "    cache -- A dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- Input data of shape. (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape. (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- Python dictionary containing your gradients with respect to different parameters.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m \n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927abfb2-ef05-4e2e-aae2-466b229f64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- Python dictionary containing your parameters. \n",
    "    grads -- Python dictionary containing your gradients. \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Python dictionary containing your updated parameters. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3132c07-e867-4687-8dd4-3cfeb2333ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- Dataset of shape. (2, number of examples)\n",
    "    Y -- Labels of shape. (1, number of examples)\n",
    "    n_h -- Size of the hidden layer.\n",
    "    num_iterations -- Number of iterations in gradient descent loop.\n",
    "    print_cost -- If True, print the cost every 1000 iterations.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Parameters learnt by the model, which can be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters.\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Loop. (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate = 1.2)\n",
    "        \n",
    "        # Print the cost every 1000 iterations.\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab90356-fb1b-485e-966c-3341fa1d57df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- Python dictionary containing your parameters. \n",
    "    X -- Input data of size. (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- Vector of predictions of our model. (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056eeb7-892f-460c-a7b9-773626a3b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y.reshape(Y.shape[1]))\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))\n",
    "\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb83675-0e12-4d50-91e6-e7424d4bcc44",
   "metadata": {},
   "source": [
    "### Tuning hidden layer size\n",
    "\n",
    "- Larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. \n",
    "- The best hidden layer size seems to be around `n_h = 5`. Indeed, a value around here seems to  fits the data well without also incurring noticeable overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9add5ca-37ef-4c61-a4c4-8faecd52daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Hidden Layer of size %d' % n_h)\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
    "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y.reshape(Y.shape[1]))\n",
    "    predictions = predict(parameters, X)\n",
    "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
    "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b686a8-e320-4e15-98f2-453dd9f014e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
