{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713c0ea1-9358-43a9-9be3-6261b63b5fcf",
   "metadata": {},
   "source": [
    "# Face recognition\n",
    "\n",
    "## Face verification\n",
    "\n",
    "- input image\n",
    "- output whether the input image is that of the claimed person\n",
    "\n",
    "## Face recognition\n",
    "\n",
    "- has database of $K$ persons\n",
    "- get an input image\n",
    "- output ID if the image is any of the $K$ persons (or not recognized)\n",
    "\n",
    "## One-shot learning\n",
    "\n",
    "- learning from one example to recognize the person again\n",
    "- \"similarity\" function d(img1, img2) = degree of difference between images\n",
    "\n",
    "## Siamese network\n",
    "\n",
    "- run different images on the same network and compare encodings $f(x^{(i)})$\n",
    "- if $x^{(i)}, x^{(j)}$ are the same person, $||f(x^{(i)}) - f(x^{(j)})||^{2}$ is small\n",
    "- if $x^{(i)}, x^{(j)}$ are the different person, $||f(x^{(i)}) - f(x^{(j)})||^{2}$ is large\n",
    "\n",
    "## Triplet loss\n",
    "\n",
    "- anchor, positive, negative\n",
    "- want $||f(A)-f(P)||^{2} \\le ||f(A)-f(N)||^{2} - \\alpha$ (margin to avoid parameters being all zeros)\n",
    "- $L(A,P,N) = max(||f(A)-f(P)||^{2} - ||f(A)-f(N)||^{2} + \\alpha, 0)$\n",
    "- $J = \\displaystyle\\sum_{i=1}^{m}L(A^{(i)}, P^{(i)}, N^{(i)})$\n",
    "- during training, if A,P,N are chosen randomly, $d(A,P) + \\alpha \\le d(A,N)$ is easily satisfied\n",
    "    - choose triplets that are \"hard\" to train on $d(A,P) \\approx d(A,N)$\n",
    "    \n",
    "## Learning the similarity function\n",
    "\n",
    "- $\\hat{y} = \\sigma\\left(\\displaystyle\\sum_{k=1}^{128}w_{i}|f(x^{(i)})_{k} - f(x^{(j)})_{k}| + b\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a4ae4-f522-4e19-8c50-6fa2ec354e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "# from keras.engine.topology import Layer\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from cv_utils import * \n",
    "K.set_image_data_format('channels_first')\n",
    "# import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# import keras.backend.tensorflow_backend as tfback\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "print(\"tf.__version__ is\", tf.__version__)\n",
    "print(\"tf.keras.__version__ is:\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fc14f-4a38-4b17-90ec-257d0ab44b50",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "- Encodes each input face image into a 128-dimensional vector.\n",
    "- Input: tensor of shape $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$ \n",
    "- Output: matrix of shape $(m, 128)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddacb301-1ba0-405d-bae4-aaa2efe7b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_bn(x,\n",
    "              layer=None,\n",
    "              cv1_out=None,\n",
    "              cv1_filter=(1, 1),\n",
    "              cv1_strides=(1, 1),\n",
    "              cv2_out=None,\n",
    "              cv2_filter=(3, 3),\n",
    "              cv2_strides=(1, 1),\n",
    "              padding=None):\n",
    "    num = '' if cv2_out == None else '1'\n",
    "    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, data_format='channels_first', name=layer+'_conv'+num)(x)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    if padding == None:\n",
    "        return tensor\n",
    "    tensor = ZeroPadding2D(padding=padding, data_format='channels_first')(tensor)\n",
    "    if cv2_out == None:\n",
    "        return tensor\n",
    "    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, data_format='channels_first', name=layer+'_conv'+'2')(tensor)\n",
    "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n",
    "    tensor = Activation('relu')(tensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8905bb-1d25-4155-9681-5f60ff567f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceRecoModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception model used for FaceNet\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "        \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # First Block\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 1, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "    X = MaxPooling2D((3, 3), strides = 2)(X)\n",
    "    \n",
    "    # Second Block\n",
    "    X = Conv2D(64, (1, 1), strides = (1, 1), name = 'conv2')(X)\n",
    "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn2')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "\n",
    "    # Second Block\n",
    "    X = Conv2D(192, (3, 3), strides = (1, 1), name = 'conv3')(X)\n",
    "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn3')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Zero-Padding + MAXPOOL\n",
    "    X = ZeroPadding2D((1, 1))(X)\n",
    "    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n",
    "    \n",
    "    # Inception 1: a/b/c\n",
    "    X = inception_block_1a(X)\n",
    "    X = inception_block_1b(X)\n",
    "    X = inception_block_1c(X)\n",
    "    \n",
    "    # Inception 2: a/b\n",
    "    X = inception_block_2a(X)\n",
    "    X = inception_block_2b(X)\n",
    "    \n",
    "    # Inception 3: a/b\n",
    "    X = inception_block_3a(X)\n",
    "    X = inception_block_3b(X)\n",
    "    \n",
    "    # Top layer\n",
    "    X = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), data_format='channels_first')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128, name='dense_layer')(X)\n",
    "    \n",
    "    # L2 normalization\n",
    "    X = Lambda(lambda  x: K.l2_normalize(x,axis=1))(X)\n",
    "\n",
    "    # Create model instance\n",
    "    model = Model(inputs = X_input, outputs = X, name='FaceRecoModel')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27c3be-b454-4621-a6be-8ebacb98554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "print(\"Total Params:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a40ed-d1d6-446e-ae60-0bb334f88fed",
   "metadata": {},
   "source": [
    "Use encoding to compare two images\n",
    "\n",
    "<img src=\"img/distance_kiank.png\" style=\"width:680px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3129bf53-a3e9-4cb5-b2e1-37ff1a30b552",
   "metadata": {},
   "source": [
    "### Triplet loss\n",
    "\n",
    "- Pull the encodings of images of same person close together.\n",
    "- Push the encodings of images of different people further apart.\n",
    "\n",
    "<img src=\"img/triplet_comparison.png\" style=\"width:280px;height:150px;\">\n",
    "\n",
    "Training will use triplets of images $(A, P, N)$:  \n",
    "- A is an \"Anchor\" image. \n",
    "- P is a \"Positive\" image.\n",
    "- N is a \"Negative\" image.\n",
    "\n",
    "We want image $A^{(i)}$ to be closer to Positive $P^{(i)}$ than Negative $N^{(i)}$) by at least margin $\\alpha$\n",
    "\n",
    "$$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$$\n",
    "\n",
    "Thus, minimize the triplet cost\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c21adb-c515-4940-908d-e2379c86001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0), axis=-1)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f22422-23f6-49e3-9a6f-e1d00930a7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
