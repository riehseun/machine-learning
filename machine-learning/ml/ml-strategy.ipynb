{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdb78a8-7199-4210-b06e-92c9247cf39f",
   "metadata": {},
   "source": [
    "## Batch nomralization\n",
    "\n",
    "- Normalize activations.\n",
    "    - Given some intermediate values in neural network $z^{(1)} \\dots z^{(m)}$\n",
    "    - $\\mu = \\dfrac{1}{m}\\displaystyle\\sum_{i}z^{(i)}$\n",
    "    - $\\sigma = \\dfrac{1}{m}\\displaystyle\\sum_{i}(z_{i}-\\mu)^{2}$\n",
    "    - $z_{norm}^{(i)} = \\dfrac{z^{(i)}-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}$\n",
    "    - $\\tilde{z}^{(i)} = \\gamma z_{norm}^{(i)} + \\beta$\n",
    "- For example, if $\\gamma = \\sqrt{\\sigma^{2}+\\epsilon}, \\beta = \\mu$, then $z_{norm}^{(i)} = \\tilde{z}^{(i)}$\n",
    "- Use $\\tilde{z}^{(i)}$ instead of ${z}^{(i)}$ \n",
    "- But unlike inputs, you don't want to force activation to be ~ $N(0,1)$\n",
    "\n",
    "$X \\xrightarrow{w^{[1]}, b^{[1]}} z^{[1]} \\xrightarrow{\\beta^{[1]}, \\gamma^{[1]}} \\tilde{z}^{[1]} \\rightarrow a^{[1]} = g^{[1]}(\\tilde{z}^{[1]}) \\xrightarrow{w^{[2]}, b^{[2]}} z^{[2]} \\xrightarrow{\\beta^{[2]}, \\gamma^{[2]}} \\tilde{z}^{[2]} \\rightarrow a^{[2]} \\rightarrow \\dots$ \n",
    "- parameters: $w, b, \\beta, \\gamma$\n",
    "\n",
    "Working with mini-batches\n",
    "- Parameters: $w, \\beta, \\gamma$ (no need for $b$)\n",
    "- $z^{[l]} = w^{[l]}a^{[l-1]}$\n",
    "- $\\tilde{z}^{[l]} = \\gamma^{[l]}z_{norm}^{[l]} + \\beta^{[l]}$\n",
    "- For $t = 1 \\dots$ num_mini_batches\n",
    "    - Compute forward prop on $X^{\\{t\\}}$ \n",
    "        - In each layer, use BN to replace $z^{[l]}$ with $\\tilde{z}^{[l]}$\n",
    "    - Use backprop to compute $dw^{[l]}, d\\beta^{[l]}, d\\gamma^{[l]}$ (no need for $db^{[l]}$)\n",
    "    - Update $w^{[l]} = w^{[l]} - \\alpha dw^{[l]}, \\beta^{[l]} = \\beta^{[l]} - \\alpha d\\beta^{[l]}, \\gamma^{[l]} = \\gamma^{[l]} - \\alpha d\\gamma^{[l]}$\n",
    "    \n",
    "Batch normalization as regularization\n",
    "- Each mini-batch is scaled by mean/variance computed on just that mini-batch.\n",
    "- This adds some noise to $z^{[l]}$\n",
    "- This has slight regularization effect.\n",
    "\n",
    "Batch normalization as test time\n",
    "- $\\mu, \\sigma^{2}$: estimate using exponentially weighted average (across mini-batches)\n",
    "- $X^{\\{1\\}} \\rightarrow \\mu^{\\{1\\}[l]}, \\sigma^{\\{1\\}[l]}, X^{\\{2\\}} \\rightarrow \\mu^{\\{2\\}[l]}, \\sigma^{\\{1\\}[2]}, X^{\\{3\\}} \\rightarrow \\mu^{\\{3\\}[l]}, \\sigma^{\\{3\\}[l]}, \\dots$\n",
    "\n",
    "## Softmax regression\n",
    "\n",
    "- Let $C$ be number of classes.\n",
    "- Last layer (softwax layer) has $n^{[L]}= C$ units.\n",
    "    - $z^{[L]} = w^{[L]}a^{[L-1]} + b^{[L]}$\n",
    "    - $t = e^{(z^{[L]})}$\n",
    "    - $a^{[L]} = \\dfrac{e^{(z^{[L]})}}{\\displaystyle\\sum_{j}t_{i}}, a_{i}^{[L]} = \\dfrac{t_{i}}{\\displaystyle\\sum_{j}t_{i}}$\n",
    "- Softmax regression generalizes logistic regression to $C$ classes.\n",
    "- Loss function\n",
    "    - $L(\\hat{y}, y) = -\\displaystyle\\sum_{j}y_{j}log\\hat{y}_{j}$\n",
    "- Cost function\n",
    "    - $J(w^{[1]}, b^{[1]}, \\dots) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)})$\n",
    "    \n",
    "$z^{[L]} \\rightarrow a^{[L]} = \\hat{y} \\rightarrow L(\\hat{y}, y)$\n",
    "- Backprod: $dz^{[L]} = \\hat{y} - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169ef65-0531-444c-a924-074970ab8afa",
   "metadata": {},
   "source": [
    "# Structuring Machine Learning Projects\n",
    "\n",
    "## Introduction to ML strategy\n",
    "\n",
    "## Setting up your goal\n",
    "\n",
    "### Sinle number evaluation metric\n",
    "\n",
    "### Satisfying and optimizaing metric\n",
    "\n",
    "- Ex. maximize accuracy subject to running_time $\\le 100ms$\n",
    "- $N$ metrics: $1$ optimizing, $N-1$ satisfying.\n",
    "\n",
    "## Comparing to human-level performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48ef51-97c9-4c5a-9dc9-bb5c324ef4d3",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335beb26-1ab4-4ff9-899f-e35ee3177932",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "- Remove unneeded, irrelevant, redundant attribute from data.\n",
    "- Redundant features can mislead the model. \n",
    "    - Especially, k-nearest neighbors.\n",
    "- Irrelevant features can overfit the model. \n",
    "- Ex. PCA\n",
    "\n",
    "## Filter method\n",
    "\n",
    "- Assign score to each feature.\n",
    "- Often considers features independent.\n",
    "- Ex. chi squared test, information gain, correlation coefficient scores\n",
    "\n",
    "## Embedded method\n",
    "\n",
    "- Learn which features are contributing to the accuracy of model.\n",
    "- Ex. regularization (LASSO, elastic net, ridge regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93519480-e633-4b07-8633-615714840dd7",
   "metadata": {},
   "source": [
    "# Precision and recall\n",
    "\n",
    "- True Negative: ground truth was negative and prediction was negative.\n",
    "- True Positive: ground truth was positive and prediction was positive.\n",
    "- False Negative: ground truth was positive but prediction was negative.\n",
    "- False Positive: ground truth was negative but prediction was positive.\n",
    "- Confusion table shows TP, FP, TN, FN.\n",
    "- In perfectly separable data, both precision and recall can be $1$.\n",
    "- But in real world, shift decision boundary increase one but decrease the other.\n",
    "\n",
    "Precision\n",
    "- Correctness on predicted positive.\n",
    "- What percentage of positive predictions were correct?\n",
    "    - Ex. Of examples recognized as cat, what % actually are cats?\n",
    "- True Positive / (True Positive + False Positive)\n",
    "\n",
    "Recall\n",
    "- Correctness of actual positive.\n",
    "- What percentage of positive cases did you catch?\n",
    "    - Ex. What % of actual cats are correctly recognized.\n",
    "- True Positive / (True Positive + False Negative)\n",
    "\n",
    "F1 score\n",
    "- Average of precision and recall.\n",
    "- $\\dfrac{2}{\\dfrac{1}{P}+\\dfrac{1}{R}}$\n",
    "\n",
    "Accuracy\n",
    "- What percentage of predictions were correct?\n",
    "- (True Positive + True Negative) / (True Negative + True Positive + False Negative + False Positive)\n",
    "\n",
    "False Positive Vs. False Negative\n",
    "- In medical exam, False Negative is threatening to patients. Thus, False Positive is preferred.\n",
    "- In spam filtering, False Positive is annoying to users. Thus, False Negative is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3b586-01be-40d1-8b91-f0444eda5bdc",
   "metadata": {},
   "source": [
    "# Imbalanced data in classification?\n",
    "\n",
    "1. Collect more data.\n",
    "2. Undersample from over-represented class.\n",
    "3. Change performance metric\n",
    "    - Accuracy is not the right metric to use when data is imbalanced.\n",
    "    - Look at precision / recall / F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103da33-ef3c-4b25-b7a7-63ae02b3af75",
   "metadata": {},
   "source": [
    "# Bias and variance\n",
    "\n",
    "Bias\n",
    "- How far off model prediction is from correct value.\n",
    "- Error from approximately true underlying function.\n",
    "- Difference between predicted and actual value.\n",
    "\n",
    "Variance\n",
    "- Variability of model prediction for given data point.\n",
    "- Sensitivity to changes in training data.\n",
    "- Overfitting: model works well on training data, but doesn't generalize well on unseen data.\n",
    "\n",
    "Ex. election survey\n",
    "- Surveying from a phonebook is source of bias.\n",
    "- Small sample size is source of variance.  \n",
    "\n",
    "## Why human-level performance\n",
    "\n",
    "- While ML is worse than human, you can\n",
    "    - Get labeled data from human.\n",
    "    - Gain insight from manual error analysis. (why did a person get this right?)\n",
    "    - Better analysis of bias/variance.\n",
    "    \n",
    "## Avoidable bias\n",
    "\n",
    "- Human error as a proxy for bays error.\n",
    "- Gap between human and training error: avoidable bias.\n",
    "- Gap between training and dev error: variance.\n",
    "\n",
    "## Two fundamental assumptions of supervised learning\n",
    "\n",
    "- You can fit the training set pretty well ~ avoidable bias.\n",
    "- Training set performance generalizes pretty well to dev/test set ~ variance.\n",
    "- Avoidable bias\n",
    "    - Traing bigger model.\n",
    "    - Train longer / use better optimization algorithms.\n",
    "    - NN architecture / hyperparameters search.\n",
    "- Dev error\n",
    "    - More data.\n",
    "    - Regularization.\n",
    "    - NN architecture / hyperparameters search.\n",
    "- Increasing $\\lambda$ decrease variance, decreasing $\\lambda$ decrease bias.\n",
    "- More features decrease bias but increases variance. Less features decreases variance but increases bias.\n",
    "\n",
    "## Approaches\n",
    "\n",
    "Linear model\n",
    "- Regularization is used to decrease variance at the cost of increasing bias.\n",
    "\n",
    "Neural network\n",
    "- Variance increases and bias decreases with number of hidden units. Regularization is used.\n",
    "\n",
    "K-nearest neighbor\n",
    "- High $k$ leads to high bias and low variance.\n",
    "\n",
    "Decision tree\n",
    "- Depth of trees increases variance. Trees are pruned to control variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dc128-5503-4f06-bc38-7b3037ef5ce4",
   "metadata": {},
   "source": [
    "# Sparse data\n",
    "\n",
    "- L1 regularization.\n",
    "- Linear regression if linear relationship.\n",
    "- One-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84549094-9430-4340-a7cf-892167787ee0",
   "metadata": {},
   "source": [
    "# Statistical power\n",
    "\n",
    "- Likelyhood that study will find effect when in fact there is effect.\n",
    "- Higher statistical power, less likely to make false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd298a-79e1-4ec0-8a66-0b584e62f6b9",
   "metadata": {},
   "source": [
    "# Outlier\n",
    "\n",
    "- Can be removed during data preparation using standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce15da70-90c5-478d-85e9-c47014f1df18",
   "metadata": {},
   "source": [
    "# Anomaly\n",
    "\n",
    "- 68% of data is one std away.\n",
    "- 95% of data is two std away.\n",
    "- 99% of data is three std away.\n",
    "\n",
    "Statistical method\n",
    "- Consider data point with z-score $\\ge 3$ outlier and likely anomaly.\n",
    "\n",
    "Metric method\n",
    "- A point is considered anomaly if removing it significantly improves the model.\n",
    "- Outlier score is a degree that a point doesn't belong to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb4d88-b65b-4743-ae1a-09093c6ef68f",
   "metadata": {},
   "source": [
    "# Gaussian mixture model Vs K-means\n",
    "\n",
    "K-mean\n",
    "- Data point must belong to one cluster.\n",
    "- Computes distance.\n",
    "\n",
    "GM\n",
    "- Probability of point belonging to each cluster.\n",
    "- Computes weighted distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c94431-426e-431f-bce1-392146b1becf",
   "metadata": {},
   "source": [
    "# Gradient boosting\n",
    "\n",
    "- Relies on regression trees, which minimizes MSE.\n",
    "- Greedy algorithm: tree is built starting from root. For each leaf, split selected to minimize MSE for this step.\n",
    "- Build collection of trees one by one. Then, predictions of individual trees are summed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c005288f-c903-433b-b5bd-1a90d2c00d81",
   "metadata": {},
   "source": [
    "# Decision tree\n",
    "\n",
    "- Used for classification.\n",
    "- Internal node: test on attribute.\n",
    "- Branch: test outcome.\n",
    "- Leaf: class label.\n",
    "- Main parameters: maximum tree depth, minimum samples per tree node, impurity criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc9ff4-245a-4079-a991-f6e43810b7b4",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "- Used for regression and classification.\n",
    "- Consist of many decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e461aa-bfaf-472d-bb24-ed3e693b3af1",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "\n",
    "- Training with labeled data.\n",
    "- Ex. linear regression, logistic regression, KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb27af2-e06d-46d3-bb75-63cc68e7df07",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "- Detect patterns in data without labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38461a5-41a2-4417-a49a-48e799216c05",
   "metadata": {},
   "source": [
    "# K-means\n",
    "\n",
    "1. partition points into $k$ subsets.\n",
    "2. compute centroid of current partitioning.\n",
    "3. assign each point to cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c03995-436f-4837-8131-169f7b3f1320",
   "metadata": {},
   "source": [
    "# Normal distribution\n",
    "\n",
    "- Central limit theorem: draw large enough samples. Their mean will be normally distributed. (Regardless of sample's initial distribution) In other words, distribution of mean of samples is normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d4dd1-eec2-4a29-b6c9-a1d4c6bb79a4",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "\n",
    "- Models are trained on train set.\n",
    "- Hyperpameters are selected on validation set.\n",
    "- Final evaluation is done on test set.\n",
    "\n",
    "## Orthogonalization\n",
    "\n",
    "- Fit training set well on cost function. (bigger network, better optimization algorithm)\n",
    "- Then, fit dev set well on cost function. (regularization, bigger training set)\n",
    "- Then, fit test set well on cost function. (bigger dev set)\n",
    "- Then, perform well in real world. (change dev set or cost function)\n",
    "\n",
    "## Train/dev/test distributions\n",
    "\n",
    "- Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on.\n",
    "- Dev and test set must come from the same distribution.\n",
    "\n",
    "## Size of dev/test sets\n",
    "\n",
    "- Set your test set to be big enough to give high confidence in the overall performance of your system.\n",
    "\n",
    "## When to change dev/test sets and metrics\n",
    "\n",
    "- If doing well on your metric and dev/test set does not correcpond to doing well on your application, change your metric and/or dev/test set.\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "- Treat train & validation as one set. Select different train and validation sets each time.\n",
    "\n",
    "## K-fold cross validation\n",
    "\n",
    "- Separate data into $k$ parts. Select one set as validation and $k-1$ sets as traiing. Repeat this process for all sets.\n",
    "- Must not be used in time-series data.\n",
    "- $k$ in practice should be $4-5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fe4f3-595e-4425-a8a2-29e881726497",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "- Ex. logistic regression, decision tree, random forests.\n",
    "- Evaluation\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 score (between $0$ and $1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c665ec-a8ef-4888-b853-d9e485276e2e",
   "metadata": {},
   "source": [
    "# Activation function\n",
    "\n",
    "- Learn complex non-linear functions.\n",
    "- If not, we are just stacking linear layers, which lead to learning a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cf523-b92b-4675-8a30-bb4d33051444",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "\n",
    "- An activation function.\n",
    "- Limit output range to $0$ and $1$.\n",
    "- Derivative for large positive or negative is near $0$. \n",
    "    - Lead to vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4a25d-880a-4308-9b3e-73029c196c0a",
   "metadata": {},
   "source": [
    "# ReLU (Rectificed linear unit)\n",
    "\n",
    "- Addresses problem of vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178bc64-ea9d-4049-97e7-110f4b95146b",
   "metadata": {},
   "source": [
    "# One hot encoding\n",
    "\n",
    "- Represent categorial variable in numerical vector space.\n",
    "- Vectors of each category has equal distance to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701094f-2ab0-454c-b962-aaa227e33e1c",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "- Should use random sampling to choose the number of layers, number of features, etc.\n",
    "- Scale parameters accordingly.\n",
    "    - For example, $\\alpha = 0.0001 \\dots 1$\n",
    "        - Use log scale such that $0.0001, 0.001, 0.01, 0.1, 1$\n",
    "    - For example, $\\beta = 0.9 \\dots 0.999$\n",
    "        - Use $1-\\beta$ such that $0.1, 0.01, 0.001$\n",
    "- Panda: babysit one model.\n",
    "- Caviar: train many models in parallel.\n",
    "\n",
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8069ad9-8999-49a4-9138-eb198f20035d",
   "metadata": {},
   "source": [
    "# Time series\n",
    "\n",
    "- Observations ordered in time.\n",
    "- Prediction depending on input vs prediction depending on certain pattern over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58d592-de64-4b4a-b415-45e75f222c23",
   "metadata": {},
   "source": [
    "# Recommender system\n",
    "\n",
    "Baseline\n",
    "- Relevant and personalized information.\n",
    "- Should not be something users know well.\n",
    "- Diverse suggestions.\n",
    "- Users should explore new items.\n",
    "\n",
    "## Collaborative filtering\n",
    "\n",
    "- Recommendation is calculated as average of other experiences.\n",
    "- Does not work well on sparse data, also has cold start problem.\n",
    "\n",
    "## Cold start problem\n",
    "- Cannot make recommendation for new item.\n",
    "- Cannot find similarity with other users for new user.\n",
    "\n",
    "## Content-based filtering\n",
    "- An approach to solve cold start problem.\n",
    "- Recommend items that are similar to items that user liked already."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d87e6d-a674-42f2-9201-a229a86eb51c",
   "metadata": {},
   "source": [
    "# Dimensionality\n",
    "\n",
    "Curse of dimensionalty\n",
    "- High dimensional data is extremely sparse.\n",
    "- It's hard to do machine learning on sparse data.\n",
    "\n",
    "Sigular value decomposition\n",
    "- Refactor a matrix into three pieces: left matrix, diagonal matrix, right matrix.\n",
    "\n",
    "Priciple component analysis\n",
    "- Special type of SVD.\n",
    "- Left matrix and right matrix are eigenvectors.\n",
    "- Diagonal matrix is eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae956d62-d365-4fdd-9a51-d7587d7825b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
