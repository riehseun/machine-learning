{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdb78a8-7199-4210-b06e-92c9247cf39f",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "\n",
    "- Training with labeled data.\n",
    "- Ex. linear regression, logistic regression, naive Bayes, KNN, SVM, decision tree, random forest, boosting tree, MLP, CNN, RNN, LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4f405-442a-4d2f-8982-3dc8f4e03f5f",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "- Finite number of outputs\n",
    "- Ex. logistic regression, decision tree, random forests.\n",
    "- Evaluation\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 score (between $0$ and $1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea9bbe-1fe3-4e19-ad0d-84c7c7fa9590",
   "metadata": {},
   "source": [
    "## Imbalanced data in classification?\n",
    "\n",
    "1. Collect more data.\n",
    "2. Undersample from over-represented class.\n",
    "3. Change performance metric\n",
    "    - Accuracy is not the right metric to use when data is imbalanced.\n",
    "    - Look at precision / recall / F1 score.\n",
    "4. Data augmentation \n",
    "    - For example, crop/rotate images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97bca6-68f2-43a3-977f-b6eacb47f2c8",
   "metadata": {},
   "source": [
    "## Evaluate classification model\n",
    "\n",
    "- True Negative: ground truth was negative and prediction was negative.\n",
    "- True Positive: ground truth was positive and prediction was positive.\n",
    "- False Negative: ground truth was positive but prediction was negative.\n",
    "- False Positive: ground truth was negative but prediction was positive.\n",
    "- Confusion table shows TP, FP, TN, FN.\n",
    "- In perfectly separable data, both precision and recall can be $1$.\n",
    "- But in real world, shift decision boundary increase one but decrease the other.\n",
    "\n",
    "Precision\n",
    "- Correctness on predicted positive.\n",
    "- What percentage of positive predictions were correct?\n",
    "    - Ex. Of examples recognized as cat, what % actually are cats?\n",
    "- True Positive / (True Positive + False Positive)\n",
    "\n",
    "Recall\n",
    "- Correctness of actual positive.\n",
    "- What percentage of positive cases did you catch?\n",
    "    - Ex. What % of actual cats are correctly recognized.\n",
    "- True Positive / (True Positive + False Negative)\n",
    "\n",
    "F1 score\n",
    "- Average of precision and recall.\n",
    "- $\\dfrac{2}{\\dfrac{1}{P}+\\dfrac{1}{R}}$\n",
    "\n",
    "Accuracy\n",
    "- What percentage of predictions were correct?\n",
    "- (True Positive + True Negative) / (True Negative + True Positive + False Negative + False Positive)\n",
    "\n",
    "False Positive Vs. False Negative\n",
    "- In medical exam, False Negative is threatening to patients. Thus, False Positive is preferred.\n",
    "- In spam filtering, False Positive is annoying to users. Thus, False Negative is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd378f-4878-4af4-a6bf-c6d4b0ba6632",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "- Represent categorial variable in numerical vector space.\n",
    "- Vectors of each category has equal distance to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a1606-5e1b-4f73-b4d5-f269c4e0ce8e",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "- Detect patterns in data without labels.\n",
    "- Ex. clustering (k-means), PCA, autoencoder, GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551aeba6-c004-43b6-86f7-9cc6e7b0959c",
   "metadata": {},
   "source": [
    "## K-means\n",
    "\n",
    "1. partition points into $k$ subsets.\n",
    "2. compute centroid of current partitioning.\n",
    "3. assign each point to cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad65ab-2156-4812-af9c-72e97aa8a7ff",
   "metadata": {},
   "source": [
    "## Gaussian mixture model Vs K-means\n",
    "\n",
    "K-mean\n",
    "- Data point must belong to one cluster.\n",
    "- Computes distance.\n",
    "\n",
    "GM\n",
    "- Probability of point belonging to each cluster.\n",
    "- Computes weighted distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808cd23-7c54-46ab-9a15-d91280af23e3",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "- Remove unneeded, irrelevant, redundant attribute from data.\n",
    "- Redundant features can mislead the model. \n",
    "    - Especially, k-nearest neighbors.\n",
    "- Irrelevant features can overfit the model. \n",
    "- Ex. PCA\n",
    "\n",
    "## Filter method\n",
    "\n",
    "- Assign score to each feature.\n",
    "- Often considers features independent.\n",
    "- Ex. chi squared test, information gain, correlation coefficient scores\n",
    "\n",
    "## Embedded method\n",
    "\n",
    "- Learn which features are contributing to the accuracy of model.\n",
    "- Ex. regularization (LASSO, elastic net, ridge regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83574c58-beda-425c-8a4b-18a491eaf091",
   "metadata": {},
   "source": [
    "# Decision tree\n",
    "\n",
    "- Used for classification.\n",
    "- Internal node: test on attribute.\n",
    "- Branch: test outcome.\n",
    "- Leaf: class label.\n",
    "- Main parameters: maximum tree depth, minimum samples per tree node, impurity criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25b072-5049-4ada-9c05-41230153852e",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "- Used for regression and classification.\n",
    "- Consist of many decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172807eb-898a-41c3-a144-fda89c823a93",
   "metadata": {},
   "source": [
    "# Dimensionality\n",
    "\n",
    "Curse of dimensionalty\n",
    "- High dimensional data is extremely sparse.\n",
    "- It's hard to do machine learning on sparse data.\n",
    "\n",
    "Sigular value decomposition\n",
    "- Refactor a matrix into three pieces: left matrix, diagonal matrix, right matrix.\n",
    "\n",
    "Priciple component analysis\n",
    "- Special type of SVD.\n",
    "- Left matrix and right matrix are eigenvectors.\n",
    "- Diagonal matrix is eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e158d-1d56-4937-b452-8d364a0f627e",
   "metadata": {},
   "source": [
    "# Recommender system\n",
    "\n",
    "Baseline\n",
    "- Relevant and personalized information.\n",
    "- Should not be something users know well.\n",
    "- Diverse suggestions.\n",
    "- Users should explore new items.\n",
    "\n",
    "## Collaborative filtering\n",
    "\n",
    "- Recommendation is calculated as average of other experiences.\n",
    "- Does not work well on sparse data, also has cold start problem.\n",
    "\n",
    "## Cold start problem\n",
    "- Cannot make recommendation for new item.\n",
    "- Cannot find similarity with other users for new user.\n",
    "\n",
    "## Content-based filtering\n",
    "- An approach to solve cold start problem.\n",
    "- Recommend items that are similar to items that user liked already."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92213d46-5b53-4ab6-bf30-c4c42be3e58b",
   "metadata": {},
   "source": [
    "# Time series\n",
    "\n",
    "- Observations ordered in time.\n",
    "- Prediction depending on input vs prediction depending on certain pattern over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6648f-b626-4fc7-9003-9556153b6e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d19a4-8a46-432c-8ffd-c6ec2de39d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c00879-37e1-4f49-98ea-2f4208428e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222cb5ec-1c04-4554-9d35-cd94390219af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d58ab73b-aa84-4212-a27a-d1ceea7d44b8",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "- Assures better convergence during backpropagation.\n",
    "\n",
    "## Batch nomralization\n",
    "\n",
    "- Normalize activations.\n",
    "    - Given some intermediate values in neural network $z^{(1)} \\dots z^{(m)}$\n",
    "    - $\\mu = \\dfrac{1}{m}\\displaystyle\\sum_{i}z^{(i)}$\n",
    "    - $\\sigma = \\dfrac{1}{m}\\displaystyle\\sum_{i}(z_{i}-\\mu)^{2}$\n",
    "    - $z_{norm}^{(i)} = \\dfrac{z^{(i)}-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}$\n",
    "    - $\\tilde{z}^{(i)} = \\gamma z_{norm}^{(i)} + \\beta$\n",
    "- For example, if $\\gamma = \\sqrt{\\sigma^{2}+\\epsilon}, \\beta = \\mu$, then $z_{norm}^{(i)} = \\tilde{z}^{(i)}$\n",
    "- Use $\\tilde{z}^{(i)}$ instead of ${z}^{(i)}$ \n",
    "- But unlike inputs, you don't want to force activation to be ~ $N(0,1)$\n",
    "\n",
    "$X \\xrightarrow{w^{[1]}, b^{[1]}} z^{[1]} \\xrightarrow{\\beta^{[1]}, \\gamma^{[1]}} \\tilde{z}^{[1]} \\rightarrow a^{[1]} = g^{[1]}(\\tilde{z}^{[1]}) \\xrightarrow{w^{[2]}, b^{[2]}} z^{[2]} \\xrightarrow{\\beta^{[2]}, \\gamma^{[2]}} \\tilde{z}^{[2]} \\rightarrow a^{[2]} \\rightarrow \\dots$ \n",
    "- parameters: $w, b, \\beta, \\gamma$\n",
    "\n",
    "Working with mini-batches\n",
    "- Parameters: $w, \\beta, \\gamma$ (no need for $b$)\n",
    "- $z^{[l]} = w^{[l]}a^{[l-1]}$\n",
    "- $\\tilde{z}^{[l]} = \\gamma^{[l]}z_{norm}^{[l]} + \\beta^{[l]}$\n",
    "- For $t = 1 \\dots$ num_mini_batches\n",
    "    - Compute forward prop on $X^{\\{t\\}}$ \n",
    "        - In each layer, use BN to replace $z^{[l]}$ with $\\tilde{z}^{[l]}$\n",
    "    - Use backprop to compute $dw^{[l]}, d\\beta^{[l]}, d\\gamma^{[l]}$ (no need for $db^{[l]}$)\n",
    "    - Update $w^{[l]} = w^{[l]} - \\alpha dw^{[l]}, \\beta^{[l]} = \\beta^{[l]} - \\alpha d\\beta^{[l]}, \\gamma^{[l]} = \\gamma^{[l]} - \\alpha d\\gamma^{[l]}$\n",
    "    \n",
    "Batch normalization as regularization\n",
    "- Each mini-batch is scaled by mean/variance computed on just that mini-batch.\n",
    "- This adds some noise to $z^{[l]}$\n",
    "- This has slight regularization effect.\n",
    "\n",
    "Batch normalization as test time\n",
    "- $\\mu, \\sigma^{2}$: estimate using exponentially weighted average (across mini-batches)\n",
    "- $X^{\\{1\\}} \\rightarrow \\mu^{\\{1\\}[l]}, \\sigma^{\\{1\\}[l]}, X^{\\{2\\}} \\rightarrow \\mu^{\\{2\\}[l]}, \\sigma^{\\{1\\}[2]}, X^{\\{3\\}} \\rightarrow \\mu^{\\{3\\}[l]}, \\sigma^{\\{3\\}[l]}, \\dots$\n",
    "\n",
    "## Softmax regression\n",
    "\n",
    "- Let $C$ be number of classes.\n",
    "- Last layer (softwax layer) has $n^{[L]}= C$ units.\n",
    "    - $z^{[L]} = w^{[L]}a^{[L-1]} + b^{[L]}$\n",
    "    - $t = e^{(z^{[L]})}$\n",
    "    - $a^{[L]} = \\dfrac{e^{(z^{[L]})}}{\\displaystyle\\sum_{j}t_{i}}, a_{i}^{[L]} = \\dfrac{t_{i}}{\\displaystyle\\sum_{j}t_{i}}$\n",
    "- Softmax regression generalizes logistic regression to $C$ classes.\n",
    "- Loss function\n",
    "    - $L(\\hat{y}, y) = -\\displaystyle\\sum_{j}y_{j}log\\hat{y}_{j}$\n",
    "- Cost function\n",
    "    - $J(w^{[1]}, b^{[1]}, \\dots) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)})$\n",
    "    \n",
    "$z^{[L]} \\rightarrow a^{[L]} = \\hat{y} \\rightarrow L(\\hat{y}, y)$\n",
    "- Backprod: $dz^{[L]} = \\hat{y} - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3b586-01be-40d1-8b91-f0444eda5bdc",
   "metadata": {},
   "source": [
    "# Activation function\n",
    "\n",
    "- Learn complex non-linear functions.\n",
    "- If not, we are just stacking linear layers, which lead to learning a linear function.\n",
    "\n",
    "## Sigmoid\n",
    "- Use in binary classification.\n",
    "- An activation function.\n",
    "- Limit output range to $0$ and $1$.\n",
    "- Derivative for large positive or negative is near $0$. \n",
    "    - Lead to vanishing gradient.\n",
    "    \n",
    "## Softmax\n",
    "- Used in multiclass classification.    \n",
    "    \n",
    "## ReLU (Rectificed linear unit)\n",
    "- Used for non-linearity of model.\n",
    "- Addresses problem of vanishing gradient.\n",
    "- About 50% of network yields 0 activation, thus fewer neurons are passing inputs to next layers making network light. \n",
    "\n",
    "## Linear\n",
    "- Used in regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103da33-ef3c-4b25-b7a7-63ae02b3af75",
   "metadata": {},
   "source": [
    "# Bias and variance\n",
    "\n",
    "Bias\n",
    "- How far off model prediction is from correct value.\n",
    "- Error from approximately true underlying function.\n",
    "- Difference between predicted and actual value.\n",
    "\n",
    "Variance\n",
    "- Variability of model prediction for given data point.\n",
    "- Sensitivity to changes in training data.\n",
    "- Overfitting: model works well on training data, but doesn't generalize well on unseen data.\n",
    "\n",
    "Ex. election survey\n",
    "- Surveying from a phonebook is source of bias.\n",
    "- Small sample size is source of variance.  \n",
    "\n",
    "Need to find right balance without overfitting or underfitting the data.\n",
    "\n",
    "## Why human-level performance\n",
    "\n",
    "- While ML is worse than human, you can\n",
    "    - Get labeled data from human.\n",
    "    - Gain insight from manual error analysis. (why did a person get this right?)\n",
    "    - Better analysis of bias/variance.\n",
    "    \n",
    "## Avoidable bias\n",
    "\n",
    "- Human error as a proxy for bays error.\n",
    "- Gap between human and training error: avoidable bias.\n",
    "- Gap between training and dev error: variance.\n",
    "\n",
    "## Two fundamental assumptions of supervised learning\n",
    "\n",
    "- You can fit the training set pretty well ~ avoidable bias.\n",
    "- Training set performance generalizes pretty well to dev/test set ~ variance.\n",
    "- Avoidable bias\n",
    "    - Traing bigger model.\n",
    "    - Train longer / use better optimization algorithms.\n",
    "    - NN architecture / hyperparameters search.\n",
    "- Dev error\n",
    "    - More data.\n",
    "    - Regularization.\n",
    "    - NN architecture / hyperparameters search.\n",
    "- Increasing $\\lambda$ decrease variance, decreasing $\\lambda$ decrease bias.\n",
    "- More features decrease bias but increases variance. Less features decreases variance but increases bias.\n",
    "\n",
    "## Approaches\n",
    "\n",
    "Linear model\n",
    "- Regularization is used to decrease variance at the cost of increasing bias.\n",
    "\n",
    "Neural network\n",
    "- Variance increases and bias decreases with number of hidden units. Regularization is used.\n",
    "\n",
    "K-nearest neighbor\n",
    "- High $k$ leads to high bias and low variance.\n",
    "\n",
    "Decision tree\n",
    "- Depth of trees increases variance. Trees are pruned to control variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dc128-5503-4f06-bc38-7b3037ef5ce4",
   "metadata": {},
   "source": [
    "# Sparse data\n",
    "\n",
    "- L1 regularization.\n",
    "- Linear regression if linear relationship.\n",
    "- One-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84549094-9430-4340-a7cf-892167787ee0",
   "metadata": {},
   "source": [
    "# Statistical power\n",
    "\n",
    "- Likelyhood that study will find effect when in fact there is effect.\n",
    "- Higher statistical power, less likely to make false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd298a-79e1-4ec0-8a66-0b584e62f6b9",
   "metadata": {},
   "source": [
    "# Outlier\n",
    "\n",
    "- Can be removed during data preparation using standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce15da70-90c5-478d-85e9-c47014f1df18",
   "metadata": {},
   "source": [
    "# Anomaly\n",
    "\n",
    "- 68% of data is one std away.\n",
    "- 95% of data is two std away.\n",
    "- 99% of data is three std away.\n",
    "\n",
    "Statistical method\n",
    "- Consider data point with z-score $\\ge 3$ outlier and likely anomaly.\n",
    "\n",
    "Metric method\n",
    "- A point is considered anomaly if removing it significantly improves the model.\n",
    "- Outlier score is a degree that a point doesn't belong to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c94431-426e-431f-bce1-392146b1becf",
   "metadata": {},
   "source": [
    "# Gradient boosting\n",
    "\n",
    "- Relies on regression trees, which minimizes MSE.\n",
    "- Greedy algorithm: tree is built starting from root. For each leaf, split selected to minimize MSE for this step.\n",
    "- Build collection of trees one by one. Then, predictions of individual trees are summed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701094f-2ab0-454c-b962-aaa227e33e1c",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "- Should use random sampling to choose the number of layers, number of features, etc.\n",
    "- Scale parameters accordingly.\n",
    "    - For example, $\\alpha = 0.0001 \\dots 1$\n",
    "        - Use log scale such that $0.0001, 0.001, 0.01, 0.1, 1$\n",
    "    - For example, $\\beta = 0.9 \\dots 0.999$\n",
    "        - Use $1-\\beta$ such that $0.1, 0.01, 0.001$\n",
    "- Panda: babysit one model.\n",
    "- Caviar: train many models in parallel.\n",
    "\n",
    "## Hyperparameter tuning\n",
    "- Grid search\n",
    "- Random search\n",
    "- Bayesian Optimization (heaviliy outperforms above two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c241e2e-ff53-464c-bc09-0aa7d9500ba2",
   "metadata": {},
   "source": [
    "# Project workflow\n",
    "\n",
    "1. What is business objective?\n",
    "    - Increase revenue, win more customers?\n",
    "2. Define problem\n",
    "    - Outline the gap we are trying to solve. \n",
    "3. Can the problem be solved without data science?\n",
    "    - For example, just recommend top N items based on very simple logic.\n",
    "4. Review existing ML\n",
    "    - No need to re-invent the wheel.\n",
    "5. Setup metrics.\n",
    "    - What does it mean to be sucessful and not successful?\n",
    "6. Exploratory data analysis\n",
    "    - See what data is like via lots of plotting.\n",
    "7. Partition data into 3 sets.\n",
    "    - Train/dev/test.\n",
    "8. Preprocess\n",
    "    - data cleaning, transformation, etc.\n",
    "9. Feature engineering\n",
    "    - Requires domain knowledge. Can be minimum if using deep learning.\n",
    "10. Develop model\n",
    "    - Choose algorithms, hypterparameters, etc.\n",
    "11. Ensemble\n",
    "    - Beware. Some ensembles are too complex to put into prodiction.\n",
    "12. Deploy/Monitor model\n",
    "    - Continue iterating afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4dbfb-6c39-4dda-9c74-d3d8e8225f37",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
