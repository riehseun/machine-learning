{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97d1a60-2673-44e8-a2b7-49686aaa11aa",
   "metadata": {},
   "source": [
    "# Word embedding\n",
    "\n",
    "## One-hot vetor\n",
    "\n",
    "- Simple and requires no implied ordering.\n",
    "- Huge and encodes no meaning.\n",
    "\n",
    "## Word embedding\n",
    "\n",
    "- Low dimension.\n",
    "- Can encode meanings.\n",
    "\n",
    "## Create word embedding\n",
    "\n",
    "- Need\n",
    "    - Crpus.\n",
    "    - Embedding method.\n",
    "- Self-supervised\n",
    "    - Unsupervised in a sense that input date (corpus) is unlabelled.\n",
    "    - Supervised in a sense that data provides context that would make up the labels. \n",
    "    \n",
    "## Continuous bag of words model\n",
    "\n",
    "- Predict a missing word based on the surrounding words\n",
    "\n",
    "![2-4-1](images/natural-language-processing/2-4-1.png)\n",
    "\n",
    "\n",
    "## Architecture of CBOW model\n",
    "\n",
    "![2-4-2](images/natural-language-processing/2-4-2.png)\n",
    "\n",
    "![2-4-3](images/natural-language-processing/2-4-3.png)\n",
    "\n",
    "![2-4-4](images/natural-language-processing/2-4-4.png)\n",
    "\n",
    "![2-4-5](images/natural-language-processing/2-4-5.png)\n",
    "\n",
    "## CBOW cost function\n",
    "\n",
    "- $J = -\\displaystyle\\sum_{k=1}^{V}y_{k}log\\hat{y}_{k}$\n",
    "\n",
    "## CBOW forward prop\n",
    "\n",
    "![2-4-5](images/natural-language-processing/2-4-5.png)\n",
    "\n",
    "- $Z_{1} = W_{1}X + B_{1}$\n",
    "- $H = ReLU(Z_{1})$\n",
    "- $Z_{2} = W_{2}H + B_{2}$\n",
    "- $\\hat{Y} = softmax(Z_{2})$\n",
    "- $J_{batch} = -\\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\displaystyle\\sum_{j=1}^{V}y_{j}^{(i)}log\\hat{y}_{j}^{(i)}$\n",
    "\n",
    "## CBOW backward prop\n",
    "\n",
    "- $W_{1} = W_{1} - \\alpha\\dfrac{\\partial J_{batch}}{\\partial W_{1}} = W_{1} - \\dfrac{1}{m}ReLu(W_{2}^{T}(\\hat{Y}-Y))X^{T}$\n",
    "- $W_{2} = W_{2} - \\alpha\\dfrac{\\partial J_{batch}}{\\partial W_{2}} = W_{2} - \\dfrac{1}{m}(\\hat{Y}-Y))H^{T}$\n",
    "- $b_{1} = b_{1} - \\alpha\\dfrac{\\partial J_{batch}}{\\partial b_{1}} = b_{1} - \\dfrac{1}{m}ReLu(W_{2}^{T}(\\hat{Y}-Y))1_{m}^{T}$\n",
    "- $b_{2} = b_{2} - \\alpha\\dfrac{\\partial J_{batch}}{\\partial b_{2}} = b_{2} - \\dfrac{1}{m}(\\hat{Y}-Y))1_{m}^{T}$\n",
    "\n",
    "## Extract word embedding vectors\n",
    "\n",
    "![2-4-7](images/natural-language-processing/2-4-7.png)\n",
    "\n",
    "![2-4-8](images/natural-language-processing/2-4-8.png)\n",
    "\n",
    "![2-4-9](images/natural-language-processing/2-4-9.png)\n",
    "\n",
    "![2-4-10](images/natural-language-processing/2-4-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea9f895-8ef7-400d-9291-c937b93a285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N,V)\n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V,N)\n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N,1)\n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cca2ce2-c203-46eb-9292-b6bbd9b03fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    '''\n",
    "    Inputs: \n",
    "        z: output scores from the hidden layer\n",
    "    Outputs: \n",
    "        yhat: prediction (estimate of y)\n",
    "    '''\n",
    "    \n",
    "    # Calculate yhat (softmax)\n",
    "    yhat = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3522532f-0cd0-436e-b062-785860319f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "    \n",
    "    # Calculate h\n",
    "    h = np.dot(W1, x) + b1\n",
    "    \n",
    "    # Apply the relu on h (store result in h)\n",
    "    h = np.maximum(0, â„Ž)\n",
    "    \n",
    "    # Calculate z\n",
    "    z = np.dot(W2, h) + b2\n",
    "\n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d623b9-5191-47cc-996e-f1175c76a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, yhat, batch_size):\n",
    "    # cost function \n",
    "    logprobs = np.multiply(np.log(yhat),y)\n",
    "    cost = - 1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a50a65-d681-4057-97d5-0a2bc3af38d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    \n",
    "    # Compute the gradient of W1\n",
    "    grad_W1 = (1 / batch_size) * np.dot(np.maximum(0, np.dot(W2.T, yhat - y)), x.T)\n",
    "    # Compute the gradient of W2\n",
    "    grad_W2 = (1 / batch_size) * np.dot(yhat - y, h.T)\n",
    "    # Compute the gradient of b1\n",
    "    grad_b1 = (1 / batch_size) * np.sum(np.maximum(0, np.dot(W2.T, yhat - y)), axis=1, keepdims=True)\n",
    "    # Compute the gradient of b2\n",
    "    grad_b2 = (1 / batch_size) * np.sum((yhat - y), axis=1, keepdims=True)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e57aa0-6f26-4351-aa88-2c8d490abe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases   \n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=282)\n",
    "    batch_size = 128\n",
    "    iters = 0\n",
    "    C = 2\n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "        # Get z and h\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "        # Get yhat\n",
    "        yhat = softmax(z)\n",
    "        # Get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ( (iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "        # Get gradients\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W1 = W1 - alpha*grad_W1 \n",
    "        W2 = W2 - alpha*grad_W2\n",
    "        b1 = b1 - alpha*grad_b1\n",
    "        b2 = b2 - alpha*grad_b2\n",
    "        \n",
    "        iters += 1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d6ac2-a8b3-419a-89d2-f94205af0979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
