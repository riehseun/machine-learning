{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7484a638-451e-4dfc-bb95-254578b59243",
   "metadata": {},
   "source": [
    "# Word vector\n",
    "\n",
    "- One hot vectors are poor when capturing similarity between words.\n",
    "    - They all have the same Euclidean distance from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e59ec-0329-47df-ad08-5cf86dc2869b",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03dc30-70c3-451a-b283-00573da372c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678f2ee-497e-4dff-9264-a23ab0f8586f",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)$$\n",
    "\n",
    "<img src=\"img/cosine_sim.png\" style=\"width:800px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3126494-b91b-43e6-98fd-e9b0a04027ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    # Compute the dot product between u and v (≈1 line)\n",
    "    dot = np.dot(u, v)\n",
    "    # Compute the L2 norm of u (≈1 line)\n",
    "    norm_u = np.sqrt(np.sum(np.square(u)))\n",
    "    \n",
    "    # Compute the L2 norm of v (≈1 line)\n",
    "    norm_v = np.sqrt(np.sum(np.square(v)))\n",
    "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd45b86-39b1-4aae-b7f8-b02f3e92d15b",
   "metadata": {},
   "source": [
    "### Word analogy\n",
    "\n",
    "<font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>.\n",
    "- Ex. <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce8874-c876-4dd6-8aca-93bd17cb146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lower case\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings e_a, e_b and e_c (≈1-3 lines)\n",
    "    print(word_to_vec_map)\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c] \n",
    "\n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, pass on them.\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
    "        cosine_sim = cosine_similarity(np.subtract(e_b, e_a), np.subtract(word_to_vec_map[w], e_c))\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585eb711-0cc2-497a-892d-e80eacd82e11",
   "metadata": {},
   "source": [
    "### Neutralize bias\n",
    "\n",
    "<img src=\"img/neutral.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "- If using 50 dimensional word embedding, we have two parts\n",
    "    - The bias-direction $g$.\n",
    "    - Remaining 49 dimensions $g_{\\perp}$.\n",
    "    \n",
    "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g$$\n",
    "\n",
    "$$e^{debiased} = e - e^{bias\\_component}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2214900-85ba-44ed-bc6e-057abea1def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    \n",
    "    Arguments:\n",
    "        word -- string indicating the word to debias\n",
    "        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
    "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
    "    \n",
    "    Returns:\n",
    "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select word vector representation of \"word\". Use word_to_vec_map. (≈ 1 line)\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # Compute e_biascomponent using the formula give above. (≈ 1 line)\n",
    "    e_biascomponent = (np.dot(e,g)/np.linalg.norm(g)**2)*g\n",
    " \n",
    "    # Neutralize e by substracting e_biascomponent from it \n",
    "    # e_debiased should be equal to its orthogonal projection. (≈ 1 line)\n",
    "    e_debiased = e-e_biascomponent\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf43a5c-b14a-4e2b-87de-35004666d92f",
   "metadata": {},
   "source": [
    "### Equalization algorithm\n",
    "\n",
    "<img src=\"img/equalize10.png\" style=\"width:800px;height:400px;\">\n",
    "\n",
    "- Make sure a pair of words are equi-distant from the 49-dimensional $g_\\perp$.\n",
    "- Ensures that two equalized steps are the same distance from $e_{receptionist}^{debiased}$\n",
    "    - Ex. \"actor\" and \"actress\" are equidistant from \"babysit.\"\n",
    "    \n",
    "$$\\mu = \\frac{e_{w1} + e_{w2}}{2}$$ \n",
    "    \n",
    "$$\\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}$$ \n",
    "\n",
    "$$\\mu_{\\perp} = \\mu - \\mu_{B}$$\n",
    "\n",
    "$$e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}$$\n",
    "\n",
    "$$e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}$$\n",
    "\n",
    "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B||}$$\n",
    "\n",
    "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {||(e_{w2} - \\mu_{\\perp}) - \\mu_B||}$$\n",
    "\n",
    "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp}$$\n",
    "\n",
    "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81d161-8dae-4d34-b0d0-b7a0eaeb3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Debias gender specific words by following the equalize method described in the figure above.\n",
    "    \n",
    "    Arguments:\n",
    "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
    "    \n",
    "    Returns\n",
    "    e_1 -- word vector corresponding to the first word\n",
    "    e_2 -- word vector corresponding to the second word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Select word vector representation of \"word\". Use word_to_vec_map. (≈ 2 lines)\n",
    "    w1, w2 = pair[0],pair[1]\n",
    "    e_w1, e_w2 = word_to_vec_map[w1],word_to_vec_map[w2]\n",
    "    \n",
    "    # Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)\n",
    "    mu = (e_w1 + e_w2)/2\n",
    "\n",
    "    # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)\n",
    "    mu_B = (np.dot(mu,bias_axis)/np.linalg.norm(bias_axis)**2)*bias_axis\n",
    "    mu_orth = mu-mu_B\n",
    "\n",
    "    # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)\n",
    "    e_w1B = (np.dot(e_w1,bias_axis)/np.linalg.norm(bias_axis)**2)*bias_axis\n",
    "    e_w2B = (np.dot(e_w2,bias_axis)/np.linalg.norm(bias_axis)**2)*bias_axis\n",
    "        \n",
    "    # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)\n",
    "    corrected_e_w1B = np.sqrt(np.abs(1-np.linalg.norm(mu_orth)**2))*((e_w1B - mu_B)/np.abs((e_w1-mu_orth)-mu_B))\n",
    "    corrected_e_w2B = np.sqrt(np.abs(1-np.linalg.norm(mu_orth)**2))*((e_w2B - mu_B)/np.abs((e_w2-mu_orth)-mu_B))\n",
    "\n",
    "    # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "    \n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b3fb0-d45a-4000-810b-b7c434c6008f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
