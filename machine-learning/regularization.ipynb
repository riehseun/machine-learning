{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9ccad9-b96f-45d3-a0d0-185128abdeaf",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "- Penalizes weights being large.\n",
    "- $J(w,b) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i))}) + \\dfrac{\\lambda}{2m}||w||^{2}$\n",
    "- Where $||w||^{2} = \\displaystyle\\sum_{j=1}^{n_{x}}w_{j}^{2} = w^{T}w$\n",
    "\n",
    "In general\n",
    "- $J(w^{[1]},b^{[1]} \\dots w^{[2]},b^{[2]}) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i))}) + \\dfrac{\\lambda}{2m}\\displaystyle\\sum_{l=1}^{L}||w^{[l]}||_{F}^{2}$\n",
    "- Where $||w^{[l]}||_{F}^{2} = \\displaystyle\\sum_{i=1}^{n^{[l-1]}}\\displaystyle\\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^{2}$\n",
    "- Add $\\dfrac{\\lambda}{m}w^{[l]}$ to $dw^{[l]}$\n",
    "- $w^{[l]} = w^{[l]} - \\alpha dw^{[l]}$ remains the same.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Example: $l =3$\n",
    "- keep_prop = 0.8 (20% chance that units will be shutdown)\n",
    "- d3 = np.random.rand(a3.shape[0], a3.shape[1]) $\\gt$ keep_prop\n",
    "- a3 = np.multiply(a3, d3)\n",
    "- a3 = a3 / keep_prop\n",
    "\n",
    "Other regularization\n",
    "- Data augmentation.\n",
    "- Early stopping.\n",
    "\n",
    "## Normalizing inputs\n",
    "\n",
    "- To make gradient descent faster. \n",
    "- Applies to both training and test sets.\n",
    "- $\\mu = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}x^{(i)}$, $\\sigma^{2} = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}x^{(i)}**2$\n",
    "- $x = \\dfrac{x-\\mu}{\\sigma}$\n",
    "\n",
    "## Vanishing/exploding gradient\n",
    "\n",
    "- Happens in deep neural network.\n",
    "- To partially overcome, weight initialization.\n",
    "    - $Var(w_{i})$ = $\\dfrac{1}{n}$ or $\\dfrac{2}{n}$ (good for RELU)\n",
    "    - $w^{[l]}$ = np.random.randn() * init_factor\n",
    "    - init_factor = np.sqrt$\\left(\\dfrac{2}{n^{[l-1]}}\\right)$ (good for RELU) or np.sqrt$\\left(\\dfrac{1}{n^{[l-1]}}\\right)$ (Xavier initialization)\n",
    "    \n",
    "## Gradient checking\n",
    "\n",
    "- Take $w^{[1]},b^{[1]} \\dots w^{[l]},b^{[l]}$ and reshape into a big factor $\\theta$\n",
    "    - $J(w^{[1]},b^{[1]} \\dots w^{[l]},b^{[l]}) J(\\theta)$\n",
    "- Take $dw^{[1]},db^{[1]} \\dots dw^{[l]},db^{[l]}$ and reshape into a big factor $d\\theta$\n",
    "- For each $i$\n",
    "    - $d\\theta_{approx}[i] = \\dfrac{J(\\theta_{1}, \\theta_{2} \\dots \\theta_{i+\\epsilon} \\dots) - J(\\theta_{1}, \\theta_{2} \\dots \\theta_{i-\\epsilon} \\dots)}{2\\epsilon} \\approx \\partial \\theta[i] = \\dfrac{\\partial J}{\\partial \\theta_{i}}$\n",
    "- Check\n",
    "    - $\\dfrac{||d\\theta_{approx}-d\\theta||_{2}}{||d\\theta_{approx}||_{2} + ||d\\theta||_{2}} \\approx 10^{-7}$ good \n",
    "    - bigger than $10^{-3}$ means something wrong!\n",
    "- Don't use in training, only to debug.\n",
    "- Include regularization term in $d\\theta$ calculation.\n",
    "- Doesn't work with dropout. (you can grad check with keep_prop=1.0, then later turn on dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a3730-6bbd-4916-8a12-94bcbb9ef4b4",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Recommend positions where the goal keeper should kick the ball so that the players can then hit it with their head. \n",
    "\n",
    "<img src=\"img/field_kiank.png\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec859701-fd75-4652-ac88-61ab3fad8094",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d381b05-056d-4219-aca8-2b210c3e16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy.io\n",
    "\n",
    "from dl_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3522a9-c7fa-415f-9bb8-a8a10b1d452c",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "- If the dot is blue, it means the French player managed to hit the ball with his/her head\n",
    "- If the dot is red, it means the other team's player hit the ball with their head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7309ce-5bcb-47e9-abb5-265deb1d7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_2D_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94e2da-83f3-4e84-a6c0-5c4d9c0a384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- If True, print the cost every 10000 iterations\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = []                            # to keep track of the cost\n",
    "    m = X.shape[1]                        # number of examples\n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        if keep_prob == 1:\n",
    "            a3, cache = forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "        # Cost function\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(a3, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, \n",
    "                                            # but this assignment will only explore one at a time\n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 10000 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (x1,000)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f1a4b-a9d5-4893-8127-3dc8e275c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y)\n",
    "print (\"On the training set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf2f2a-dcdd-45ef-9ffe-827fb03d97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dec(parameters, X):\n",
    "    \"\"\"\n",
    "    Used for plotting decision boundary.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    X -- input data of size (m, K)\n",
    "\n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    a3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (a3>0.5)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75585f1-99db-4716-a86b-7a380f78ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model without regularization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c97a6-9021-489b-b97e-22d1b2cb4fc3",
   "metadata": {},
   "source": [
    "### L2 Regularization\n",
    "\n",
    "Modifying cost function from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebfe9a9a-1c3c-43dd-af3c-48754f94b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n",
    "    \n",
    "    L2_regularization_cost = lambd * ( np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)) ) / (2 * m)\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38565cfe-0e85-45c8-8551-e12a2a76db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    \n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd * W3 / m) \n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd * W2 / m) \n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + (lambd * W1 / m) \n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb9116f-4484-4283-b17b-1a938225888e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parameters \u001b[38;5;241m=\u001b[39m model(\u001b[43mtrain_X\u001b[49m, train_Y, lambd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOn the train set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m predictions_train \u001b[38;5;241m=\u001b[39m predict(train_X, train_Y, parameters)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "parameters = model(train_X, train_Y, lambd = 0.7)\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed446131-ee7b-4105-8f62-c90f31455746",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m axes\u001b[38;5;241m.\u001b[39mset_xlim([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.75\u001b[39m,\u001b[38;5;241m0.40\u001b[39m])\n\u001b[1;32m      4\u001b[0m axes\u001b[38;5;241m.\u001b[39mset_ylim([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.75\u001b[39m,\u001b[38;5;241m0.65\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m plot_decision_boundary(\u001b[38;5;28;01mlambda\u001b[39;00m x: predict_dec(parameters, x\u001b[38;5;241m.\u001b[39mT), \u001b[43mtrain_X\u001b[49m, train_Y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Model with L2-regularization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9834d-a188-4a94-bc10-14ef92e393e7",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "- It randomly shuts down some neurons in each iteration.\n",
    "- Should only use dropout (randomly eliminate nodes) in training. (Not in testing)\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"img/dropout1_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"img/dropout2_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d7fb6e8-5928-4c16-a5ee-a86cc4f63e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (20, 2)\n",
    "                    b1 -- bias vector of shape (20, 1)\n",
    "                    W2 -- weight matrix of shape (3, 20)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n",
    "    cache -- tuple, information stored for computing the backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    # Steps 1-4 below correspond to the Steps 1-4 described above. \n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1])     # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "    D1 = (D1 < keep_prob).astype(int)                 # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A1 = A1 * D1                                      # Step 3: shut down some neurons of A1\n",
    "    A1 = A1 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down\n",
    "    \n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    D2 = np.random.rand(A2.shape[0], A2.shape[1])     # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n",
    "    D2 = (D2 < keep_prob).astype(int)                 # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A2 = A2 * D2                                      # Step 3: shut down some neurons of A2\n",
    "    A2 = A2 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a89de54-643a-4e18-82bc-28f3a54e8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added dropout.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation_with_dropout()\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    \n",
    "    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "    dA2 = dA2 / keep_prob              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    \n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    \n",
    "    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n",
    "    dA1 = dA1 / keep_prob              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    \n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34c2f8b-f6c5-4d95-800e-c217b02febd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parameters \u001b[38;5;241m=\u001b[39m model(\u001b[43mtrain_X\u001b[49m, train_Y, keep_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.86\u001b[39m, learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOn the train set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m predictions_train \u001b[38;5;241m=\u001b[39m predict(train_X, train_Y, parameters)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac0ec395-e88d-4aff-8411-e6b10369864b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m axes\u001b[38;5;241m.\u001b[39mset_xlim([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.75\u001b[39m,\u001b[38;5;241m0.40\u001b[39m])\n\u001b[1;32m      4\u001b[0m axes\u001b[38;5;241m.\u001b[39mset_ylim([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.75\u001b[39m,\u001b[38;5;241m0.65\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m plot_decision_boundary(\u001b[38;5;28;01mlambda\u001b[39;00m x: predict_dec(parameters, x\u001b[38;5;241m.\u001b[39mT), \u001b[43mtrain_X\u001b[49m, train_Y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Model with dropout\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b6987-0c35-4506-b166-4573c866e6b9",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "**Here are the results of our three models**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **model**\n",
    "        </td>\n",
    "        <td>\n",
    "        **train accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "        **test accuracy**\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        3-layer NN without regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "        <td>\n",
    "        91.5%\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with L2-regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        94%\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with dropout\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329721a9-00fb-4d62-84be-e8fc9488c010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
