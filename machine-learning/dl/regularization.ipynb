{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9ccad9-b96f-45d3-a0d0-185128abdeaf",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Penalizes weights being large.\n",
    "- $J(w,b) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i))}) + \\dfrac{\\lambda}{2m}||w||^{2}$\n",
    "- Where $||w||^{2} = \\displaystyle\\sum_{j=1}^{n_{x}}w_{j}^{2} = w^{T}w$\n",
    "\n",
    "In general\n",
    "- $J(w^{[1]},b^{[1]} \\dots w^{[2]},b^{[2]}) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i))}) + \\dfrac{\\lambda}{2m}\\displaystyle\\sum_{l=1}^{L}||w^{[l]}||_{F}^{2}$\n",
    "- Where $||w^{[l]}||_{F}^{2} = \\displaystyle\\sum_{i=1}^{n^{[l-1]}}\\displaystyle\\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^{2}$\n",
    "- Add $\\dfrac{\\lambda}{m}w^{[l]}$ to $dw^{[l]}$\n",
    "- $w^{[l]} = w^{[l]} - \\alpha dw^{[l]}$ remains the same.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Example: $l =3$\n",
    "- keep_prop = 0.8 (20% chance that units will be shutdown)\n",
    "- d3 = np.random.rand(a3.shape[0], a3.shape[1]) $\\gt$ keep_prop\n",
    "- a3 = np.multiply(a3, d3)\n",
    "- a3 = a3 / keep_prop\n",
    "\n",
    "Other regularization\n",
    "- Data augmentation.\n",
    "- Early stopping.\n",
    "\n",
    "## Normalizing inputs\n",
    "\n",
    "- To make gradient descent faster. \n",
    "- Applies to both training and test sets.\n",
    "- $\\mu = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}x^{(i)}$, $\\sigma^{2} = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}x^{(i)}**2$\n",
    "- $x = \\dfrac{x-\\mu}{\\sigma}$\n",
    "\n",
    "## Vanishing/exploding gradient\n",
    "\n",
    "- Happens in deep neural network.\n",
    "- To partially overcome, weight initialization.\n",
    "    - $Var(w_{i})$ = $\\dfrac{1}{n}$ or $\\dfrac{2}{n}$ (good for RELU)\n",
    "    - $w^{[l]}$ = np.random.randn() * init_factor\n",
    "    - init_factor = np.sqrt$\\left(\\dfrac{2}{n^{[l-1]}}\\right)$ (good for RELU) or np.sqrt$\\left(\\dfrac{1}{n^{[l-1]}}\\right)$ (Xavier initialization)\n",
    "    \n",
    "## Gradient checking\n",
    "\n",
    "- Take $w^{[1]},b^{[1]} \\dots w^{[l]},b^{[l]}$ and reshape into a big factor $\\theta$\n",
    "    - $J(w^{[1]},b^{[1]} \\dots w^{[l]},b^{[l]}) J(\\theta)$\n",
    "- Take $dw^{[1]},db^{[1]} \\dots dw^{[l]},db^{[l]}$ and reshape into a big factor $d\\theta$\n",
    "- For each $i$\n",
    "    - $d\\theta_{approx}[i] = \\dfrac{J(\\theta_{1}, \\theta_{2} \\dots \\theta_{i+\\epsilon} \\dots) - J(\\theta_{1}, \\theta_{2} \\dots \\theta_{i-\\epsilon} \\dots)}{2\\epsilon} \\approx \\partial \\theta[i] = \\dfrac{\\partial J}{\\partial \\theta_{i}}$\n",
    "- Check\n",
    "    - $\\dfrac{||d\\theta_{approx}-d\\theta||_{2}}{||d\\theta_{approx}||_{2} + ||d\\theta||_{2}} \\approx 10^{-7}$ good \n",
    "    - bigger than $10^{-3}$ means something wrong!\n",
    "- Don't use in training, only to debug.\n",
    "- Include regularization term in $d\\theta$ calculation.\n",
    "- Doesn't work with dropout. (you can grad check with keep_prop=1.0, then later turn on dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a3730-6bbd-4916-8a12-94bcbb9ef4b4",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Recommend positions where the goal keeper should kick the ball so that the players can then hit it with their head. \n",
    "\n",
    "<img src=\"img/field_kiank.png\" style=\"width:600px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec859701-fd75-4652-ac88-61ab3fad8094",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d381b05-056d-4219-aca8-2b210c3e16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy.io\n",
    "\n",
    "from dl_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3522a9-c7fa-415f-9bb8-a8a10b1d452c",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "- If the dot is blue, it means the French player managed to hit the ball with his/her head\n",
    "- If the dot is red, it means the other team's player hit the ball with their head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7309ce-5bcb-47e9-abb5-265deb1d7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_2D_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa02f38-a8c1-4453-9e17-3d9dc6c91ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "\n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1].\n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94e2da-83f3-4e84-a6c0-5c4d9c0a384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- If True, print the cost every 10000 iterations\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = []                            # to keep track of the cost\n",
    "    m = X.shape[1]                        # number of examples\n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        if keep_prob == 1:\n",
    "            a3, cache = forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "        # Cost function\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(a3, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, \n",
    "                                            # but this assignment will only explore one at a time\n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 10000 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (x1,000)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f1a4b-a9d5-4893-8127-3dc8e275c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y)\n",
    "print (\"On the training set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf2f2a-dcdd-45ef-9ffe-827fb03d97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dec(parameters, X):\n",
    "    \"\"\"\n",
    "    Used for plotting decision boundary.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    X -- input data of size (m, K)\n",
    "\n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    a3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (a3>0.5)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75585f1-99db-4716-a86b-7a380f78ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model without regularization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c97a6-9021-489b-b97e-22d1b2cb4fc3",
   "metadata": {},
   "source": [
    "### L2 Regularization\n",
    "\n",
    "Modifying cost function from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe9a9a-1c3c-43dd-af3c-48754f94b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n",
    "    \n",
    "    L2_regularization_cost = lambd * ( np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)) ) / (2 * m)\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38565cfe-0e85-45c8-8551-e12a2a76db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    \n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd * W3 / m) \n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd * W2 / m) \n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + (lambd * W1 / m) \n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb9116f-4484-4283-b17b-1a938225888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, lambd = 0.7)\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed446131-ee7b-4105-8f62-c90f31455746",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with L2-regularization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9834d-a188-4a94-bc10-14ef92e393e7",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "- It randomly shuts down some neurons in each iteration.\n",
    "- Should only use dropout (randomly eliminate nodes) in training. (Not in testing)\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"img/dropout1_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"img/dropout2_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7fb6e8-5928-4c16-a5ee-a86cc4f63e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (20, 2)\n",
    "                    b1 -- bias vector of shape (20, 1)\n",
    "                    W2 -- weight matrix of shape (3, 20)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n",
    "    cache -- tuple, information stored for computing the backward propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    # Steps 1-4 below correspond to the Steps 1-4 described above. \n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1])     # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "    D1 = (D1 < keep_prob).astype(int)                 # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A1 = A1 * D1                                      # Step 3: shut down some neurons of A1\n",
    "    A1 = A1 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down\n",
    "    \n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    D2 = np.random.rand(A2.shape[0], A2.shape[1])     # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n",
    "    D2 = (D2 < keep_prob).astype(int)                 # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n",
    "    A2 = A2 * D2                                      # Step 3: shut down some neurons of A2\n",
    "    A2 = A2 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89de54-643a-4e18-82bc-28f3a54e8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added dropout.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation_with_dropout()\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    \n",
    "    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
    "    dA2 = dA2 / keep_prob              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    \n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    \n",
    "    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n",
    "    dA1 = dA1 / keep_prob              # Step 2: Scale the value of neurons that haven't been shut down\n",
    "    \n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c2f8b-f6c5-4d95-800e-c217b02febd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ec395-e88d-4aff-8411-e6b10369864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with dropout\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b6987-0c35-4506-b166-4573c866e6b9",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "**Here are the results of our three models**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **model**\n",
    "        </td>\n",
    "        <td>\n",
    "        **train accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "        **test accuracy**\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        3-layer NN without regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "        <td>\n",
    "        91.5%\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with L2-regularization\n",
    "        </td>\n",
    "        <td>\n",
    "        94%\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with dropout\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329721a9-00fb-4d62-84be-e8fc9488c010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
