{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cc676d-e415-4c6e-8da7-78199583aa0e",
   "metadata": {},
   "source": [
    "# Mathematics\n",
    "\n",
    "## Vectors\n",
    "\n",
    "- $f(x) = \\dfrac{1}{\\sigma\\sqrt{2\\pi}}exp\\left[\\dfrac{-(x-\\mu)^{2}}{2\\sigma^{2}}\\right]$\n",
    "- Consider $\\begin{bmatrix} \\mu \\\\ \\sigma \\\\ \\end{bmatrix}$ as vectors\n",
    "\n",
    "For two vectors $r = \\begin{bmatrix} r_{i} \\\\ r_{j} \\\\ \\end{bmatrix}$ and $s = \\begin{bmatrix} s_{i} \\\\ s_{j} \\\\ \\end{bmatrix}$ \n",
    "- $|r| = \\sqrt{a^{2}+b^{2}}$\n",
    "- $r \\cdot s = r_{i}s_{i} + r_{j}s_{j}$ \n",
    "- $r \\cdot s = s \\cdot r$ (communitive)\n",
    "- $r \\cdot (s+t) = r \\cdot s + r \\cdot t$ (distributive)\n",
    "- $r \\cdot (as) = a(r \\cdot \\ s)$ (associative)\n",
    "\n",
    "## Cosine rule\n",
    "\n",
    "- $c^{2} = a^{2} + b^{2} - 2abcos\\theta$\n",
    "- $|r-s|^{2} = |r|^{2} + |s|^{2} - 2|r||s|cos\\theta$\n",
    "- $(r-s) \\cdot (r-s) = |r^{2}| -2s \\cdot r + |s^{2}|$\n",
    "- Thus, $s \\cdot r = |r||s|cos\\theta, cos\\theta = \\dfrac{s \\cdot r}{|r||s|}$\n",
    "\n",
    "## Projection\n",
    "\n",
    "- $\\dfrac{r \\cdot s}{|r|} = |s|cos\\theta$ (scalar projection)\n",
    "- $\\hat{r}|s|cos\\theta$ (vector projection = scalar projection multiplied by a unit vector)\n",
    "\n",
    "## Changing basis\n",
    "\n",
    "- $\\hat{e}_{1} = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\end{bmatrix}, \\hat{e}_{2} = \\begin{bmatrix} 0 \\\\ 1 \\\\ \\end{bmatrix}, b_{1}= \\begin{bmatrix} 2 \\\\ 1 \\\\ \\end{bmatrix}, b_{2}= \\begin{bmatrix} -2 \\\\ 4 \\\\ \\end{bmatrix}$\n",
    "- $r_{e} = 3\\hat{e}_{1} + 4\\hat{e}_{2} = \\begin{bmatrix} 3 \\\\ 4 \\\\ \\end{bmatrix}$\n",
    "- $r_{b} = \\begin{bmatrix} 2 \\\\ 1/2 \\\\ \\end{bmatrix} \\left(\\dfrac{r_{e}b_{1}}{|b_{1}|^{2}} = 2, \\dfrac{r_{e}b_{2}}{|b_{2}|^{2}} = \\dfrac{1}{2}\\right)$\n",
    "\n",
    "## Basis\n",
    "\n",
    "- Not linear combos\n",
    "- Spans the space\n",
    "\n",
    "## Matrices\n",
    "\n",
    "Rotate 90 degree anti-clockwise?\n",
    "- $\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ \\end{bmatrix} \\rightarrow \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\\\ \\end{bmatrix}$\n",
    "\n",
    "Matrix multiplication is not communitive\n",
    "- $A_{2}A_{1} \\ne A_{1}A_{2}$\n",
    "\n",
    "Matrix multiplication is associative\n",
    "- $A_{3}(A_{2}A_{1}) = (A_{3}A_{2})A_{1}$\n",
    "\n",
    "## Determinant\n",
    "\n",
    "- $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$\n",
    "- $\\dfrac{1}{ad-bc}$ (if 0, then linearly dependent)\n",
    "\n",
    "## Chaning basus\n",
    "\n",
    "- $B^{-1}RB = R_{B}$\n",
    "\n",
    "## Orthogonal matrices\n",
    "\n",
    "Orthonormal\n",
    "- $a_{i} \\cdot a_{j} = 0$ if $i \\ne j$ \n",
    "- $a_{i} \\cdot a_{j} = 1$ if $i = j$ \n",
    "\n",
    "Also\n",
    "- $A^{T}A = I$ $(A^{T} = A^{-1})$\n",
    "\n",
    "## Gram-Schmidt process \n",
    "\n",
    "How to construct orthonormal basis\n",
    "\n",
    "- let $v = \\{v_{1}, v_{2} \\dots v_{n}\\}$ vectors spanning a space\n",
    "- let $e = \\{e_{1}, e_{2} \\dots e_{n}\\}$ orthonormalized verions of $v$'s\n",
    "- leave $v_{1}$ as it is\n",
    "- $v_{2} = (v_{2} \\cdot e_{1})e_{1} + u_{2}$, rearranging $u_{2} = v_{2} - (v_{2} \\cdot e_{1})e_{1}$\n",
    "- then $e_{2} = \\dfrac{u_{2}}{|u_{2}|}$\n",
    "- $u_{3} = v_{3} - (v_{3} \\cdot e_{1})e_{1} - (v_{3} \\cdot e_{2})e_{2}$\n",
    "- then $e_{3} = \\dfrac{u_{3}}{|u_{3}|}$\n",
    "\n",
    "## Reflecting in a plane\n",
    "\n",
    "Example\n",
    "- $v_{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, v_{2} = \\begin{bmatrix} 2 \\\\ 0 \\\\ 1 \\end{bmatrix}, v_{3} = \\begin{bmatrix} 3 \\\\ 1 \\\\ -1 \\end{bmatrix}$\n",
    "- $e_{1} = \\dfrac{v_{1}}{|v_{1}|} = \\dfrac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$\n",
    "- $u_{2} = v_{2} - (v_{2} \\cdot e_{1})e_{1} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$\n",
    "- $e_{2} = \\dfrac{u_{2}}{|u_{2}|} = \\dfrac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$\n",
    "- $u_{3} = v_{3} - (v_{3} \\cdot e_{1})e_{1} - (v_{3} \\cdot e_{2})e_{2} = \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}$\n",
    "- $e_{3} = \\dfrac{u_{3}}{|u_{3}|} = \\dfrac{1}{\\sqrt{6}} \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}$\n",
    "\n",
    "Transformation matrix\n",
    "- $E = \\begin{bmatrix} e_{1} & e_{2} & e_{3} \\end{bmatrix}$ (note $E^{T} = E^{-1}$)\n",
    "\n",
    "Reflection matrix in $e_{3}$\n",
    "- $T_{E} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{bmatrix}$\n",
    "- Going from $r$ to $r^{'}$ is hard\n",
    "- So we do $r \\xrightarrow{E^{-1}} r_{E} \\xrightarrow{T_{E}} r_{E}^{'} \\xrightarrow{E} r^{'}$ \n",
    "- $ET_{E}E^{'}r = r^{'}$\n",
    "\n",
    "## Eigen\n",
    "\n",
    "- eigenvectors stay in the same span after applying transformations\n",
    "- for eigenvalues $\\lambda$, eigenvectors $x$, and transformation matrix $A$\n",
    "    - $Ax = \\lambda x$\n",
    "    - $(A-\\lambda I)x = 0$\n",
    "    - det$(A-\\lambda I) = 0$\n",
    "    \n",
    "## Changing to eigenbasis\n",
    "\n",
    "- $v \\xrightarrow{c^{-1}} [v]_{E} \\xrightarrow{D^{n}} [T^{n}v]_{E} \\xrightarrow{c} T^{n}v$\n",
    "- $T^{n} = CD^{n}C^{-1}$\n",
    "\n",
    "## Multivariate calculus\n",
    "\n",
    "- $\\dfrac{\\partial f}{\\partial t} = \\dfrac{\\partial f}{\\partial x}\\dfrac{dx}{dt} + \\dfrac{\\partial f}{\\partial y}\\dfrac{dy}{dt}$ \n",
    "\n",
    "## Jacobian and Hessian\n",
    "\n",
    "Let $u(x,y)$ and $v(x,y)$\n",
    "- then $J = \\begin{pmatrix} \\dfrac{\\partial u}{\\partial x} & \\dfrac{\\partial u}{\\partial y} \\\\ \\dfrac{\\partial v}{\\partial x} & \\dfrac{\\partial v}{\\partial y} \\end{pmatrix}$\n",
    "\n",
    "Let $f(x,y,z)$\n",
    "- then $J = \\begin{pmatrix} \\dfrac{\\partial f}{\\partial x} & \\dfrac{\\partial f}{\\partial y} & \\dfrac{\\partial f}{\\partial z} \\end{pmatrix}$\n",
    "- then $H = \\begin{pmatrix} \\dfrac{\\partial^{2}f}{\\partial x^{2}} & \\dfrac{\\partial f}{\\partial x}\\dfrac{\\partial f}{\\partial y} & \\dfrac{\\partial f}{\\partial x}\\dfrac{\\partial f}{\\partial z} \\\\ \\dfrac{\\partial f}{\\partial y}\\dfrac{\\partial f}{\\partial x} & \\dfrac{\\partial^{2}f}{\\partial y^{2}} & \\dfrac{\\partial f}{\\partial y}\\dfrac{\\partial f}{\\partial z} \\\\ \\dfrac{\\partial f}{\\partial z}\\dfrac{\\partial f}{\\partial x} & \\dfrac{\\partial f}{\\partial z}\\dfrac{\\partial f}{\\partial y} & \\dfrac{\\partial^{2}f}{\\partial z^{2}} \\end{pmatrix}$\n",
    "\n",
    "## Power series\n",
    "\n",
    "- $g(x) = a + bx + cx^{2} + dx^{3} + \\dots = \\displaystyle\\sum_{n=0}^{\\infty}\\dfrac{f^{n}(0)}{n!}x^{n}$ (Maclaurin series)\n",
    "    - example: $e^{x} = \\displaystyle\\sum_{n=0}^{\\infty}\\dfrac{x^{n}}{n!}$\n",
    "\n",
    "- $g(x) = \\displaystyle\\sum_{n=0}^{\\infty}\\dfrac{f^{n}(p)}{n!}(x-p)^{n}$\n",
    "    - example: $cos(x) = 1 - \\dfrac{x^{2}}{2} + \\dfrac{x^{4}}{24} - \\dfrac{x^{6}}{720} + \\dots = - \\displaystyle\\sum_{n=0}^{\\infty}\\dfrac{(-1)^{n}}{(2n)!}x^{2n}$\n",
    "    - example: $\\dfrac{1}{x} = 1 - (x-1) + (x-1)^2 - (x-1)^3 + \\dots = \\displaystyle\\sum_{n=0}^{\\infty}(-1)^{n}(x-1)^{n}$ \n",
    "\n",
    "## Multivariate Taylor\n",
    "\n",
    "First rewrite the expression such that\n",
    "- $f(x+\\Delta{x}) = \\displaystyle\\sum_{n=0}^{\\infty}\\dfrac{f^{n}(x)}{n!}\\Delta{x}^{n}$\n",
    "\n",
    "Then\n",
    "- $f(x+\\Delta{x}, y+\\Delta{y}) = f(x,y) + \\dfrac{\\partial f(x,y)}{\\partial x}\\Delta{x} + \\dfrac{\\partial f(x,y)}{\\partial y}\\Delta{y} + \\dfrac{1}{2}\\left(\\dfrac{\\partial^{2}f(x,y)}{\\partial x^{2}}\\Delta{x}^{2} + 2\\dfrac{\\partial f(x,y)}{\\partial x \\partial y}\\Delta{x}\\Delta{y} + \\dfrac{\\partial^{2}f(x,y)}{\\partial y^{2}}\\Delta{y}^{2}\\right) + \\dots = f(x,y) + J_{f}\\Delta{x} + \\dfrac{1}{2}\\Delta{x}^{t}H_{f}\\Delta{x} + \\dots$\n",
    "\n",
    "## Newton-Raphson method to solve functions\n",
    "\n",
    "- $x_{x+1} = x_{i} - \\dfrac{f(x_{i})}{f'(x_{i})}$\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "- $S_{n+1} = S_{n} - \\gamma\\nabla{f(s_{n})}$\n",
    "\n",
    "## Lagrange multiplier\n",
    "\n",
    "- In general for 2D, given following problem \n",
    "    - $min/max f(x)$ \n",
    "    - such that $g(x) = 0$\n",
    "    \n",
    "- Setup\n",
    "    - $\\nabla{f(x)} = \\lambda\\nabla{g(x)}$\n",
    "    - $\\nabla{L(x,y,z)} = \\begin{pmatrix} \\dfrac{\\partial f}{\\partial x} - \\lambda\\dfrac{\\partial g}{\\partial x} \\\\ \\dfrac{\\partial f}{\\partial y} - \\lambda\\dfrac{\\partial g}{\\partial y} \\\\ -g(x) \\end{pmatrix} = 0$\n",
    "    \n",
    "## Linear regression\n",
    "\n",
    "- $\\chi^2 = \\displaystyle\\sum(y_{i}-mx_{i}-c)^{2}$\n",
    "- $m = \\dfrac{\\displaystyle\\sum(x_{i}-\\bar{x})y_{i}}{\\displaystyle\\sum(x_{i}-\\bar{x})^{2}}$\n",
    "- $c = \\bar{y}-m\\bar{x}$\n",
    "\n",
    "## Cosine similarity\n",
    "\n",
    "$\\cos (\\theta)=\\dfrac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\dfrac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}$\n",
    "\n",
    "## Dot product\n",
    "\n",
    "- $cos(\\theta) = \\dfrac{x^{T}y}{\\|x\\|\\|y\\|} = \\dfrac{<x,y>}{\\|x\\|\\|y\\|}$\n",
    "\n",
    "## Inner product\n",
    "\n",
    "- $x,y \\in V$\n",
    "- $<,>: V x V -> R$\n",
    "\n",
    "### Symmetric\n",
    "\n",
    "- $<x,y> = <y,x>$\n",
    "\n",
    "### Positive definite\n",
    "\n",
    "- $<x,x> \\ge 0$ and $<x,x> = 0$ iff $x = 0$\n",
    "\n",
    "### Bilinear\n",
    "\n",
    "- $x,y,z \\in V, \\lambda \\in R$\n",
    "- $<\\lambda x + z, y> = \\lambda<x,y> + <z,y>$\n",
    "- $<x, \\lambda y + z> = \\lambda<x,y> + <x,z>$\n",
    "\n",
    "## Length / Norm of a vector\n",
    "\n",
    "- $\\|x\\| = \\sqrt{<x,x>}$\n",
    "\n",
    "## Triangle inequality\n",
    "\n",
    "- $\\|x+y\\| \\le \\|x\\| + \\|y\\|$\n",
    "\n",
    "## Distance\n",
    "\n",
    "- $d(x,y) = \\|x-y\\| = \\sqrt{<x-y,x-y>}$\n",
    "\n",
    "## Projection onto 1D subspace\n",
    "\n",
    "Using two conditions\n",
    "- $\\Pi_{u}(x) \\in u => \\exists \\lambda \\in R: \\Pi_{u}(x) = \\lambda b$\n",
    "- $<b, \\Pi_{u}(x)-x>$ = 0\n",
    "\n",
    "Then\n",
    "- $\\Pi_{u}(x) = \\dfrac{bb^{T}}{\\|b\\|^{2}}x$\n",
    "\n",
    "## Projection onto higher dimensional subspace\n",
    "\n",
    "- $\\lambda = \\begin{pmatrix} \\lambda_{1} \\\\ \\vdots \\\\ \\lambda_{M}  \\end{pmatrix} (M \\times 1)$ \n",
    "- $B = \\begin{pmatrix} B_{1} | \\dots | B_{M} \\end{pmatrix} (D \\times M)$\n",
    "\n",
    "Then\n",
    "- $\\Pi_{u}(x) = B(B^{T}B)^{-1}B^{T}X$\n",
    "\n",
    "## PCA\n",
    "\n",
    "1. Mean normalized your data\n",
    "2. Compute the covariance matrix\n",
    "3. Compute SVD on the covariance matrix. This returns $[USV] = svd(\\Sigma)$ where $U$ is eigenvectors and $S$ is eigenvalues\n",
    "4. You can then use first $n$ columns of $U$, to get new data by $XU[:,0 : n]$\n",
    "\n",
    "- Given $X = \\{x_{1} \\dots x_{n}\\}$ where $x_{i} \\in {\\rm I\\!R}^{D}$\n",
    "\n",
    "We have\n",
    "- $x_{n} = \\displaystyle\\sum_{i=1}^{D}B_{in}b_{i}, B_{in} = x_{n}^{T}b_{i}, B = (b_{1} \\dots b_{n})$\n",
    "- $\\tilde{x} = BB^{T}X$\n",
    "\n",
    "From above\n",
    "- $x_{n} = \\displaystyle\\sum_{i=1}^{M}B_{in}b_{i} + \\displaystyle\\sum_{i=M+1}^{D}B_{in}b_{i}$ (PCA ingores the second term. Also $b_{1} \\dots b_{M}$ span the principal subspace)\n",
    "\n",
    "Loss function is given by\n",
    "- $J = \\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\|x_{n}-\\tilde{x_{n}}\\|^{2}$\n",
    "\n",
    "Then, derivates are\n",
    "- $\\dfrac{\\partial J}{\\partial \\{B_{in}b_{i}\\}} = \\dfrac{\\partial J}{\\partial \\tilde{x_{n}}}\\dfrac{\\partial \\tilde{x_{n}}}{\\partial \\{B_{in}b_{i}\\}} = -\\dfrac{2}{N}(x_{n}-\\tilde{x_{n}})^{T}\\dfrac{\\partial \\tilde{x_{n}}}{\\partial \\{B_{in}b_{i}\\}}$\n",
    "- $\\dfrac{\\partial \\tilde{x_{n}}}{\\partial B_{in}} = b_{i}$\n",
    "- $\\dfrac{\\partial J}{\\partial B_{in}} = \\dfrac{\\partial J}{\\partial \\tilde{x_{n}}}\\dfrac{\\partial \\tilde{x_{n}}}{\\partial B_{in}} = -\\dfrac{2}{N}(x_{n}-\\tilde{x_{n}})^{T}b_{i} = -\\dfrac{2}{N}(x_{n}-\\displaystyle\\sum_{j=1}^{M}B_{jn}b_{j})^{T}b_{i} = -\\dfrac{2}{N}(x_{n}^{T}b_{i}-B_{in}b_{i}^{T}b_{i}) = -\\dfrac{2}{N}(x_{n}^{T}b_{i}-B_{in}) = 0$\n",
    "\n",
    "Thus\n",
    "- $B_{in} = x_{n}^{T}b_{i}$\n",
    "\n",
    "Now\n",
    "- $\\tilde{x_{n}} = \\displaystyle\\sum_{j=1}^{M}B_{jn}b_{j} = \\displaystyle\\sum_{j=1}^{M}(x_{n}^{T}b_{j})b_{j} = \\displaystyle\\sum_{j=1}^{M}b_{j}(b_{j}^{T}x_{n}) = \\displaystyle\\sum_{j=1}^{M}(b_{j}b_{j}^{T})x_{n}$\n",
    "\n",
    "Note\n",
    "- $x_{n} = \\left(\\displaystyle\\sum_{j=1}^{M}b_{j}b_{j}^{T}\\right)x_{n} + \\left(\\displaystyle\\sum_{j=M+1}^{D}b_{j}b_{j}^{T}\\right)x_{n}$\n",
    "\n",
    "Then\n",
    "- $x_{n} - \\tilde{x_{n}} = \\left(\\displaystyle\\sum_{j=M+1}^{D}b_{j}b_{j}^{T}\\right)x_{n} = \\displaystyle\\sum_{j=M+1}^{D}(b_{j}^{T}x_{n})b_{j}$\n",
    "\n",
    "Loss function becomes\n",
    "- $J = \\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\|x_{n}-\\tilde{x_{n}}\\|^{2} = \\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\|\\displaystyle\\sum_{j=M+1}^{D}(b_{j}^{T}x_{n})b_{j}\\|^{2} = \\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\displaystyle\\sum_{j=M+1}^{D}(b_{j}^{T}x_{n})^{2} = \\dfrac{1}{N}\\displaystyle\\sum_{n}\\displaystyle\\sum_{J}b_{j}^{T}x_{n}x_{n}^{T}b_{j} = \\displaystyle\\sum_{j=M+1}^{D}b_{j}^{T}\\left(\\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}x_{n}x_{n}^{T}\\right)b_{j} = \\displaystyle\\sum_{j=M+1}^{D}b_{j}^{T}Sb_{j} = trace\\left[\\left(\\displaystyle\\sum_{j=M+1}^{D}b_{j}^{T}b_{j}\\right)S\\right]$\n",
    "\n",
    "Example\n",
    "- $J = b_{2}^{T}b_{2}, b_{2}^{T}b_{2} = 1$\n",
    "- Lagrange $L = b_{2}^{T}Sb_{2} + \\lambda(1 - b_{2}^{T}b_{2})$\n",
    "- $\\dfrac{\\partial L}{\\partial \\lambda} = 1 - b_{2}^{T}b_{2} = 0$\n",
    "- $\\dfrac{\\partial L}{\\partial b_{2}} = 2b_{2}^{T}S - 2\\lambda b_{2}^{T} = 0$\n",
    "- $Sb_{2} = \\lambda b_{2}$\n",
    "- Now, $J = b_{2}^{T}Sb_{2} = b_{2}^{T}b_{2}\\lambda = \\lambda$\n",
    "- Thus, $J = \\displaystyle\\sum_{j=M+1}^{D}\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928c048-3f54-4bc8-a628-ce189f023c14",
   "metadata": {},
   "source": [
    "Variance\n",
    "- Expectation of squared deviation of random variable from its mean.\n",
    "- $Var(X) = E[(X-m)^2], m = E[X]$\n",
    "\n",
    "Expected value\n",
    "- Value that random variable takes with greatest likelihood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
