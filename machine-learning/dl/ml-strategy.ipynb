{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdb78a8-7199-4210-b06e-92c9247cf39f",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- Should use random sampling to choose the number of layers, number of features, etc.\n",
    "- Scale parameters accordingly.\n",
    "    - For example, $\\alpha = 0.0001 \\dots 1$\n",
    "        - Use log scale such that $0.0001, 0.001, 0.01, 0.1, 1$\n",
    "    - For example, $\\beta = 0.9 \\dots 0.999$\n",
    "        - Use $1-\\beta$ such that $0.1, 0.01, 0.001$\n",
    "- Panda: babysit one model.\n",
    "- Caviar: train many models in parallel.\n",
    "\n",
    "## Batch nomralization\n",
    "\n",
    "- Normalize activations.\n",
    "    - Given some intermediate values in neural network $z^{(1)} \\dots z^{(m)}$\n",
    "    - $\\mu = \\dfrac{1}{m}\\displaystyle\\sum_{i}z^{(i)}$\n",
    "    - $\\sigma = \\dfrac{1}{m}\\displaystyle\\sum_{i}(z_{i}-\\mu)^{2}$\n",
    "    - $z_{norm}^{(i)} = \\dfrac{z^{(i)}-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}$\n",
    "    - $\\tilde{z}^{(i)} = \\gamma z_{norm}^{(i)} + \\beta$\n",
    "- For example, if $\\gamma = \\sqrt{\\sigma^{2}+\\epsilon}, \\beta = \\mu$, then $z_{norm}^{(i)} = \\tilde{z}^{(i)}$\n",
    "- Use $\\tilde{z}^{(i)}$ instead of ${z}^{(i)}$ \n",
    "- But unlike inputs, you don't want to force activation to be ~ $N(0,1)$\n",
    "\n",
    "$X \\xrightarrow{w^{[1]}, b^{[1]}} z^{[1]} \\xrightarrow{\\beta^{[1]}, \\gamma^{[1]}} \\tilde{z}^{[1]} \\rightarrow a^{[1]} = g^{[1]}(\\tilde{z}^{[1]}) \\xrightarrow{w^{[2]}, b^{[2]}} z^{[2]} \\xrightarrow{\\beta^{[2]}, \\gamma^{[2]}} \\tilde{z}^{[2]} \\rightarrow a^{[2]} \\rightarrow \\dots$ \n",
    "- parameters: $w, b, \\beta, \\gamma$\n",
    "\n",
    "Working with mini-batches\n",
    "- Parameters: $w, \\beta, \\gamma$ (no need for $b$)\n",
    "- $z^{[l]} = w^{[l]}a^{[l-1]}$\n",
    "- $\\tilde{z}^{[l]} = \\gamma^{[l]}z_{norm}^{[l]} + \\beta^{[l]}$\n",
    "- For $t = 1 \\dots$ num_mini_batches\n",
    "    - Compute forward prop on $X^{\\{t\\}}$ \n",
    "        - In each layer, use BN to replace $z^{[l]}$ with $\\tilde{z}^{[l]}$\n",
    "    - Use backprop to compute $dw^{[l]}, d\\beta^{[l]}, d\\gamma^{[l]}$ (no need for $db^{[l]}$)\n",
    "    - Update $w^{[l]} = w^{[l]} - \\alpha dw^{[l]}, \\beta^{[l]} = \\beta^{[l]} - \\alpha d\\beta^{[l]}, \\gamma^{[l]} = \\gamma^{[l]} - \\alpha d\\gamma^{[l]}$\n",
    "    \n",
    "Batch normalization as regularization\n",
    "- Each mini-batch is scaled by mean/variance computed on just that mini-batch.\n",
    "- This adds some noise to $z^{[l]}$\n",
    "- This has slight regularization effect.\n",
    "\n",
    "Batch normalization as test time\n",
    "- $\\mu, \\sigma^{2}$: estimate using exponentially weighted average (across mini-batches)\n",
    "- $X^{\\{1\\}} \\rightarrow \\mu^{\\{1\\}[l]}, \\sigma^{\\{1\\}[l]}, X^{\\{2\\}} \\rightarrow \\mu^{\\{2\\}[l]}, \\sigma^{\\{1\\}[2]}, X^{\\{3\\}} \\rightarrow \\mu^{\\{3\\}[l]}, \\sigma^{\\{3\\}[l]}, \\dots$\n",
    "\n",
    "## Softmax regression\n",
    "\n",
    "- Let $C$ be number of classes.\n",
    "- Last layer (softwax layer) has $n^{[L]}= C$ units.\n",
    "    - $z^{[L]} = w^{[L]}a^{[L-1]} + b^{[L]}$\n",
    "    - $t = e^{(z^{[L]})}$\n",
    "    - $a^{[L]} = \\dfrac{e^{(z^{[L]})}}{\\displaystyle\\sum_{j}t_{i}}, a_{i}^{[L]} = \\dfrac{t_{i}}{\\displaystyle\\sum_{j}t_{i}}$\n",
    "- Softmax regression generalizes logistic regression to $C$ classes.\n",
    "- Loss function\n",
    "    - $L(\\hat{y}, y) = -\\displaystyle\\sum_{j}y_{j}log\\hat{y}_{j}$\n",
    "- Cost function\n",
    "    - $J(w^{[1]}, b^{[1]}, \\dots) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)})$\n",
    "    \n",
    "$z^{[L]} \\rightarrow a^{[L]} = \\hat{y} \\rightarrow L(\\hat{y}, y)$\n",
    "- Backprod: $dz^{[L]} = \\hat{y} - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169ef65-0531-444c-a924-074970ab8afa",
   "metadata": {},
   "source": [
    "# Structuring Machine Learning Projects\n",
    "\n",
    "## Introduction to ML strategy\n",
    "\n",
    "### Orthogonalization\n",
    "\n",
    "- Fit training set well on cost function. (bigger network, better optimization algorithm)\n",
    "- Then, fit dev set well on cost function. (regularization, bigger training set)\n",
    "- Then, fit test set well on cost function. (bigger dev set)\n",
    "- Then, perform well in real world. (change dev set or cost function)\n",
    "\n",
    "## Setting up your goal\n",
    "\n",
    "### Sinle number evaluation metric\n",
    "\n",
    "### Satisfying and optimizaing metric\n",
    "\n",
    "- Ex. maximize accuracy subject to running_time $\\le 100ms$\n",
    "- $N$ metrics: $1$ optimizing, $N-1$ satisfying.\n",
    "\n",
    "### Train/dev/test distributions\n",
    "\n",
    "- Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on.\n",
    "- Dev and test set must come from the same distribution.\n",
    "\n",
    "### Size of dev/test sets\n",
    "\n",
    "- Set your test set to be big enough to give high confidence in the overall performance of your system.\n",
    "\n",
    "### When to change dev/test sets and metrics\n",
    "\n",
    "- If doing well on your metric and dev/test set does not correcpond to doing well on your application, change your metric and/or dev/test set.\n",
    "\n",
    "## Comparing to human-level performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335beb26-1ab4-4ff9-899f-e35ee3177932",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "- Remove unneeded, irrelevant, redundant attribute from data.\n",
    "- Redundant features can mislead the model. (Especially, k-nearest neighbors)\n",
    "- Irrelevant features can overfit the model. \n",
    "\n",
    "## Filter method\n",
    "\n",
    "- Assign score to each feature.\n",
    "- Ex. Chi squared test, information gain, correlation coefficient scores\n",
    "\n",
    "## Embedded method\n",
    "\n",
    "- Learn which features are contributing to the accuracy of model.\n",
    "- Ex. regularization (LASSO, elastic net, ridge regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93519480-e633-4b07-8633-615714840dd7",
   "metadata": {},
   "source": [
    "# Precision and recall\n",
    "\n",
    "- True Negative: ground truth was negative and prediction was negative.\n",
    "- True Positive: ground truth was positive and prediction was positive.\n",
    "- False Negative: ground truth was positive but prediction was negative.\n",
    "- False Positive: ground truth was negative but prediction was positive.\n",
    "\n",
    "Precision: \n",
    "- What percentage of positive predictions were correct?\n",
    "    - Ex. Of examples recognized as cat, what % actually are cats?\n",
    "- True Positive / (True Positive + False Positive)\n",
    "\n",
    "Recall\n",
    "- What percentage of positive cases did you catch?\n",
    "    - Ex. What % of actual cats are correctly recognized.\n",
    "- True Positive / (True Positive + False Negative)\n",
    "\n",
    "F1 score\n",
    "- Average of precision and recall.\n",
    "- $\\dfrac{2}{\\dfrac{1}{P}+\\dfrac{1}{R}}$\n",
    "\n",
    "Accuracy\n",
    "- What percentage of predictions were correct?\n",
    "- (True Positive + True Negative) / (True Negative + True Positive + False Negative + False Positive)\n",
    "\n",
    "False Positive Vs. False Negative\n",
    "- In medical exam, False Negative is threatening to patients. Thus, False Positive is preferred.\n",
    "- In spam filtering, False Positive is annoying to users. Thus, False Negative is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3b586-01be-40d1-8b91-f0444eda5bdc",
   "metadata": {},
   "source": [
    "# Imbalanced data in classification?\n",
    "\n",
    "1. Collect more data.\n",
    "2. Undersample from over-represented class.\n",
    "3. Change performance metric\n",
    "    - Accuracy is not the right metric to use when data is imbalanced.\n",
    "    - Look at precision / recall / F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103da33-ef3c-4b25-b7a7-63ae02b3af75",
   "metadata": {},
   "source": [
    "# Bias and variance\n",
    "\n",
    "## Why human-level performance\n",
    "\n",
    "- While ML is worse than human, you can\n",
    "    - Get labeled data from human.\n",
    "    - Gain insight from manual error analysis. (why did a person get this right?)\n",
    "    - Better analysis of bias/variance.\n",
    "    \n",
    "## Avoidable bias\n",
    "\n",
    "- Human error as a proxy for bays error.\n",
    "- Gap between human and training error: avoidable bias.\n",
    "- Gap between training and dev error: variance.\n",
    "\n",
    "## Two fundamental assumptions of supervised learning\n",
    "\n",
    "- You can fit the training set pretty well ~ avoidable bias.\n",
    "- Training set performance generalizes pretty well to dev/test set ~ variance.\n",
    "- Avoidable bias\n",
    "    - Traing bigger model.\n",
    "    - Train longer / use better optimization algorithms.\n",
    "    - NN architecture / hyperparameters search.\n",
    "- Dev error\n",
    "    - More data.\n",
    "    - Regularization.\n",
    "    - NN architecture / hyperparameters search.\n",
    "- Increasing $\\lambda$ decrease variance, decreasing $\\lambda$ decrease bias.\n",
    "- More features decrease bias but increases variance. Less features decreases variance but increases bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dc128-5503-4f06-bc38-7b3037ef5ce4",
   "metadata": {},
   "source": [
    "# Sparse data\n",
    "\n",
    "- L1 regularization.\n",
    "- Linear regression if linear relationship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
