{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcbd6788-c1db-4917-9d84-795ae73a7866",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d02ac7-ca9f-46cf-bcee-09f52d55c9fb",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "Examples\n",
    "- Home features -> price: **NN**\n",
    "- Ad, user info -> click on ad? (0,1): **NN**\n",
    "- Image -> object$(1\\dots1000)$: **CNN**\n",
    "- Audio -> text transcript: **RNN**\n",
    "- English -> chinese: **RNN**\n",
    "- Image, radar info -> position of other cars: **Hybrid**\n",
    "\n",
    "Data\n",
    "- Structured data\n",
    "- Unstructured data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35706e7-6c12-4efd-9b01-f1949c1dcdd7",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "- $m$ training examples: $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) \\dots (x^{(m)}, y^{(m)})\\}$\n",
    "\n",
    "$X = \n",
    "\\begin{bmatrix}\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\ \n",
    "    x^{(1)} & x^{(2)} \\ldots & x^{(m)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "- $X \\in {\\rm I\\!R^{n_{x}, m}}$\n",
    "- $X$.shape = $(n_{x}, m)$\n",
    "\n",
    "$Y = \n",
    "\\begin{bmatrix}\n",
    "    y^{(1)} & y^{(2)} \\ldots & y^{(m)} \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- $Y \\in {\\rm I\\!R^{1, m}}$\n",
    "- $Y$.shape = $(1, m)$\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "- Given $x$, want $\\hat{y} = P(y=1|x)$ (where $x \\in {\\rm I\\!R^{n_{x}}}$ and $0 \\le \\hat{y} \\le 1$)\n",
    "- Parameters: $w \\in {\\rm I\\!R^{n_{x}}}$, $b \\in {\\rm I\\!R}$\n",
    "- Output $\\hat{y} = \\sigma{(w^{T}x + b)}$\n",
    "\n",
    "$\\sigma{(z)} = \\dfrac{1}{1+e^{-z}}$\n",
    "- If $z$ large positive, $\\sigma{(z)} \\approx 1$\n",
    "- If $z$ large negative, $\\sigma{(z)} \\approx 0$\n",
    "\n",
    "## Loss function\n",
    "\n",
    "- $L(\\hat{y}, y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y}))$\n",
    "- If $y = 1$, $L(\\hat{y}, y) = -log\\hat{y}$ => want $\\hat{y}$ large as possible ($y \\approx 1$)\n",
    "- If $y = 0$, $L(\\hat{y}, y) = -log(1-\\hat{y})$ => want $\\hat{y}$ small as possible ($y \\approx 0$)\n",
    "\n",
    "## Cost function\n",
    "\n",
    "$J(w,b) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)})$ = $\\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}-(y^{(i)}log\\hat{y}^{(i)} + (1-y^{(i)})log(1-\\hat{y}^{(i)}))$\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "- Want $w,b$ that minimizes $J(w,b)$\n",
    "- $w := w - \\alpha\\dfrac{\\partial J(w,b)}{\\partial w}$\n",
    "- $b := b - \\alpha\\dfrac{\\partial J(w,b)}{\\partial b}$\n",
    "\n",
    "## Logistic regression gradient descent\n",
    "\n",
    "- $z = w^{T} + b$\n",
    "- $y = a = \\sigma(z)$\n",
    "- $L(a,y) = -(ylog(a) + (1-y)log(1-a))$\n",
    "\n",
    "Example with 2 features with a single training example\n",
    "\n",
    "- Parameters: $x_{1}, w_{1}, x_{2}, w_{2}, b$\n",
    "- $z = w_{1}x_{1} + w_{2}x_{2} + b$\n",
    "- $a = \\sigma(z)$\n",
    "- $L(a,y)$\n",
    "\n",
    "Derivatives\n",
    "\n",
    "- $da = \\dfrac{\\partial L(a,y)}{\\partial a} = -\\dfrac{y}{a} + \\dfrac{1-y}{1-a}$\n",
    "- $dz = \\dfrac{\\partial L(a,y)}{\\partial z} = \\dfrac{\\partial L(a,y)}{\\partial a}\\dfrac{\\partial a}{\\partial z} = \\left(-\\dfrac{y}{a} + \\dfrac{1-y}{1-a}\\right)a(1-a) = a - z$ \n",
    "- $dw_{1} = x_{1}dz$\n",
    "- $dw_{2} = x_{2}dz$\n",
    "- $db = dz$\n",
    "\n",
    "Gradient updates\n",
    "\n",
    "- $w_{1} := w_{1} - \\alpha dw_{1}$\n",
    "- $w_{2} := w_{2} - \\alpha dw_{2}$\n",
    "- $b := b - \\alpha db$\n",
    "\n",
    "## Logistic regression on m examples with 2 features\n",
    "\n",
    "- $J=0, dw_{1} = 0, dw_{2} = 0, db = 0$\n",
    "- For $i = 1$ to $m$\n",
    "    - $z^{(i)} = w^{T}x^{(i)} + b$\n",
    "    - $a^{(i)} = \\sigma(z^{(i)})$\n",
    "    - $J += -[y^{(i)}loga^{(i)} + (1-y^{(i)})log(1-a^{(i)})]$\n",
    "    - $dz^{(i)} = a^{(i)} - y^{(i)}$\n",
    "    - $dw_{1} += x_{1}^{(i)}dz^{(i)}$\n",
    "    - $dw_{2} += x_{2}^{(i)}dz^{(i)}$\n",
    "    - $db += dz^{(i)}$\n",
    "- $J = J/m$\n",
    "- $dw_{1} = dw_{1}/m$\n",
    "- $dw_{2} = dw_{2}/m$\n",
    "- $db = db/m$\n",
    "\n",
    "## Vectorizing logistic regression\n",
    "\n",
    "$X = \n",
    "\\begin{bmatrix}\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\ \n",
    "    x^{(1)} & x^{(2)} \\ldots & x^{(m)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\\n",
    "    \\vdots & \\vdots & \\vdots \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Z = \n",
    "\\begin{bmatrix}\n",
    "    z^{(1)} & z^{(2)} \\ldots & z^{(m)} \\\\\n",
    "\\end{bmatrix} = w^{T}X + \\begin{bmatrix}\n",
    "    b & b \\ldots & b \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    w^{T}x^{(1)}+b & w^{T}x^{(2)}+b \\ldots & w^{T}x^{(m)}+b \\\\\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "$dZ = \n",
    "\\begin{bmatrix}\n",
    "    dz^{(1)} & dz^{(2)} \\ldots & dz^{(m)} \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$A = \n",
    "\\begin{bmatrix}\n",
    "    a^{(1)} & a^{(2)} \\ldots & A^{(m)} \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Y = \n",
    "\\begin{bmatrix}\n",
    "    y^{(1)} & y^{(2)} \\ldots & y^{(m)} \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$dZ = A - Y =\n",
    "\\begin{bmatrix}\n",
    "    a^{(1)}y^{(1)} & a^{(2)}y^{(2)} \\ldots & a^{(m)}y^{(m)} \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "In summary\n",
    "\n",
    "- $Z$ = np.dot$(wT,X)+b$\n",
    "- $A = \\sigma(Z)$\n",
    "- $dZ = A - Y$\n",
    "- $dw = \\dfrac{1}{m}XdZ^{T}$\n",
    "- $db = \\dfrac{1}{m}$np.sum$(dZ)$ \n",
    "- $w := w - \\alpha dw$\n",
    "- $b := b - \\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7f225-c2f0-4e87-b4eb-7a7d7860990b",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "Example: 2 layers NN \n",
    "- For $i = 1$ to $m$\n",
    "    - $z^{[1](i)} = w^{[1]}x^{(i)} + b^{[1]}$\n",
    "    - $a^{[1](i)} = \\sigma(z^{[1](i)})$\n",
    "    - $z^{[2](i)} = w^{[2]}a^{[1](i)} + b^{[2]}$\n",
    "    - $a^{[2](i)} = \\sigma(z^{[2](i)})$\n",
    "    \n",
    "Vectorizing\n",
    "- $Z^{[1]} = w^{[1]}X + b^{[1]}$\n",
    "- $A^{[1]} = \\sigma(Z^{[1]})$\n",
    "- $Z^{[2]} = w^{[2]}A^{[1]} + b^{[2]}$\n",
    "- $A^{[2]} = \\sigma(Z^{[2]})$\n",
    "\n",
    "Where \n",
    "- $X$: $(n_{x}, m)$ matrix\n",
    "- $Z$: $($number of hidden units, $m)$ matrix\n",
    "- $A$: $($number of hidden units, $m)$ matrix\n",
    "\n",
    "## Activation function\n",
    "\n",
    "- Binary classification? use sigmoid\n",
    "- All other cases? use RELU (rectified linear unit)\n",
    "\n",
    "Why use non-linear activation function?\n",
    "- If use linear activation funciotn, having layers becomes meaningless because combinations of linear function reduce down to a single linear function\n",
    "\n",
    "## Gradient descent for neural networks\n",
    "\n",
    "Parameters\n",
    "- $n_{x} = n^{[0]}$: number of features\n",
    "- $n^{[1]}$: number of hidden units\n",
    "- $n^{[2]} = 1$: number of output units\n",
    "- $w^{[1]}$: $(n^{[1]}, n^{[0]})$ matrix\n",
    "- $b^{[1]}$: $(n^{[1]}, 1)$ matrix\n",
    "- $w^{[2]}$: $(n^{[2]}, n^{[1]})$ matrix\n",
    "- $b^{[2]}$: $(n^{[2]}, 1)$ matrix\n",
    "\n",
    "Cost function\n",
    "- $J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}) = \\dfrac{1}{m}\\displaystyle\\sum_{i=1}^{n}L(\\hat{y}, y)$ where $L(\\hat{y}, y) = a^{[2]}$\n",
    "\n",
    "Gradient descent\n",
    "- Repeat\n",
    "    - Compute prediction $\\hat{y}^{(i)}$ for $i = 1 \\dots m$\n",
    "    - $dw^{[1]} = \\dfrac{\\partial J}{\\partial w^{[1]}}$, $db^{[1]} = \\dfrac{\\partial J}{\\partial b^{[1]}}$, $dw^{[2]} = \\dfrac{\\partial J}{\\partial w^{[2]}}$, $db^{[2]} = \\dfrac{\\partial J}{\\partial b^{[2]}}$\n",
    "    - $w^{[1]} = w^{[1]} - \\alpha dw^{[1]}$, $b^{[1]} = b^{[1]} - \\alpha db^{[1]}$, $w^{[2]} = w^{[2]} - \\alpha dw^{[2]}$, $b^{[2]} = b^{[2]} - \\alpha db^{[2]}$\n",
    "    \n",
    "Backward propagation\n",
    "- $dZ^{[2]} = A^{[2]} - Y$\n",
    "- $dw^{[2]} = \\dfrac{1}{m}dZ^{[2]}A^{[1]^{T}}$\n",
    "- $db^{[2]} = \\dfrac{1}{m}$np.sum$(dZ^{[2]}$, axis=1, keepdims=True$)$\n",
    "- $dZ^{[1]} = w^{[2]^{T}}dZ^{[2]} * g^{[1]^{'}}(Z^{[1]})$\n",
    "- $dw^{[1]} = \\dfrac{1}{m}dZ^{[1]}X^{T}$\n",
    "- $db^{[1]} = \\dfrac{1}{m}$np.sum$(dZ^{[1]}$, axis=1, keepdims=True$)$\n",
    "\n",
    "## Random initialization\n",
    "\n",
    "- If weights are initialized to $0$, all hidden units compute the same function due to symmetry\n",
    "- To break symmetry,\n",
    "    - $w^{[1]}$ = np.random.rand() * 0.01\n",
    "    - $b^{[1]}$ = np.zeros()\n",
    "    - $w^{[2]}$ = np.random.rand() * 0.01\n",
    "    - $b^{[2]}$ = np.zeros()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270adc4a-b2b1-4022-9115-bfe4bf5f32d1",
   "metadata": {},
   "source": [
    "## Deep L-layer neural network\n",
    "\n",
    "Parameters\n",
    "- $n^{[l]}$: number of units in layer $l$\n",
    "- $A^{[l]} = g^{[l]}(Z^{[l]})$: activation in layer $l$\n",
    "- $Z^{[l]} = w^{[l]}A^{[l-1]}+b^{[l]}$ \n",
    "- $w^{[l]}, b^{[l]}$: weights for $Z^{[l]}$\n",
    "\n",
    "Dimensions\n",
    "- $w^{[l]}$: $(n^{[l]}, n^{[l-1]})$\n",
    "- $b^{[l]}$: $(n^{[l]}, 1)$\n",
    "- $dw^{[l]}$: $(n^{[l]}, n^{[l-1]})$\n",
    "- $db^{[l]}$: $(n^{[l]}, 1)$\n",
    "- $Z^{[l]}$: $(n^{[l]}, m)$\n",
    "- $A^{[l]}$: $(n^{[l]}, m)$\n",
    "\n",
    "## Building blocks of deep neural network\n",
    "\n",
    "Forward and backward propagation\n",
    "\n",
    "$a^{[0]} \\rightarrow \\boxed{w^{[1]},b^{[1]}} \\xrightarrow{a^{[1]}} \\boxed{w^{[1]},b^{[1]}} \\xrightarrow{a^{[2]}} \\dots \\xrightarrow{a^{[l-1]}} \\boxed{w^{[l]},b^{[l]}} \\xrightarrow{a^{[l]}} \\hat{y} \\rightarrow L(\\hat{y},y)$\n",
    "\n",
    "Cache all parameters from each block (layers)\n",
    "\n",
    "$\\boxed{w^{[1]},b^{[1]},dz^{[1]}} \\xleftarrow{da^{[1]}} \\boxed{w^{[2]},b^{[2]},dz^{[2]}} \\xleftarrow{da^{[2]}} \\dots \\xleftarrow{da^{[l-1]}} \\boxed{w^{[l]},b^{[l]},dz^{[l]}} \\leftarrow da^{[l]} = -\\dfrac{y}{a}+\\dfrac{1-y}{1-a}$\n",
    "\n",
    "Each block (layer) computes $dw, db$\n",
    "\n",
    "$w^{[l]} = w^{[l]} - \\alpha dw^{[l]}$\n",
    "$b^{[l]} = b^{[l]} - \\alpha db^{[l]}$\n",
    "\n",
    "$dA^{[L]} = -\\dfrac{y^{(1)}}{a^{(1)}}+\\dfrac{1-y^{(1)}}{1-a^{(1)}} \\dots -\\dfrac{y^{(m)}}{a^{(m)}}+\\dfrac{1-y^{(m)}}{1-a^{(m)}}$\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "- Learning rate\n",
    "- Number of iterations\n",
    "- Number of layers\n",
    "- Number of hidden units in each layer\n",
    "- Choice of activation funciton\n",
    "- Momentum\n",
    "- Min-batch size\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab131a5-a41b-4cbd-b471-a3aa5fc9b254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
