{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b1492d-57fa-445f-8077-72a26da1fc8a",
   "metadata": {},
   "source": [
    "# Bayes\n",
    "\n",
    "Bayes' rule\n",
    "- **assumes each word in sentence are independent from one another** \n",
    "\n",
    "Conditional probability\n",
    "- Probability of B given that A happened OR\n",
    "- Looking at elements of A, probability that they also being to B\n",
    "\n",
    "Bayes rule\n",
    "- $P(X|Y) = P(Y|X) \\times \\dfrac{P(X)}{P(Y)}$\n",
    "\n",
    "Table from before but add the total sum\n",
    "<table>\n",
    "<tr>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>PosFreq(1)</td>\n",
    "    <td>NegFreq(0)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>1</td>\n",
    "    <td>2</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>N</td>\n",
    "    <td>13</td>\n",
    "    <td>12</td>\n",
    "<tr>\n",
    "</table>\n",
    "\n",
    "Compute conditional probabilities, for example\n",
    "- $P(I|Pos) = \\dfrac{3}{13}, P(I|Neg) = \\dfrac{3}{12}$\n",
    "\n",
    "Then, fill those conditional probabilties to a new table\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>Pos</td>\n",
    "    <td>Neg</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>0.24</td>\n",
    "    <td>0.25</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>0.24</td>\n",
    "    <td>0.25</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>0.15</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.17</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>0.08</td>\n",
    "    <td>0.17</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Sum</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Naive Bayes binary classification rule\n",
    "- $\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|POS)}{P({w_{i}}|NEG)}$\n",
    "\n",
    "For an example sentence \"I am happy today; I am learning\"\n",
    "- $\\dfrac{0.2}{0.2} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.14}{0.10} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.2}{0.2} \\times \\dfrac{0.1}{0.1} = 1.4$\n",
    "\n",
    "### Laplacian smoothing\n",
    "- Avoid problems of probabilities being $0$\n",
    "- $P(w_{i}|class) = \\dfrac{freq(w_{i},class)+1}{(N_{class}+V)}$\n",
    "    - $N_{class}$ : frequency of all words in class\n",
    "    - $V$ : number of unique words in vocabulary\n",
    "- For example, $P(I|POS) = \\dfrac{3+1}{13+8} = 0.19$\n",
    "- Then, the table becomes\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>Vocabulary</td>\n",
    "    <td>Pos</td>\n",
    "    <td>Neg</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>I</td>\n",
    "    <td>0.19</td>\n",
    "    <td>0.20</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>am</td>\n",
    "    <td>0.19</td>\n",
    "    <td>0.20</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>happy</td>\n",
    "    <td>0.14</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>because</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.05</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>learning</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>NLP</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.10</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>sad</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.15</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>not</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.15</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Sum</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### Log likelihood\n",
    "\n",
    "To do inference, we can compute\n",
    "- $\\dfrac{P(pos)}{P(neg)}\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|pos)}{P({w_{i}}|neg)} \\gt 1$\n",
    "\n",
    "To avoid numerical overflow as $m$ gets larger, we introduce \"log\" such that\n",
    "- $\\log\\left(\\dfrac{P(pos)}{P(neg)}\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|pos)}{P({w_{i}}|neg)}\\right) = \\log\\left(\\dfrac{P(pos)}{P(neg)}\\right) + \\log\\left(\\displaystyle\\sum_{i=1}^{m}\\dfrac{P({w_{i}}|pos)}{P({w_{i}}|neg)}\\right)$\n",
    "\n",
    "We let lambda such that\n",
    "- $\\lambda(w) = \\log\\left(\\dfrac{P(w|pos)}{P(w|neg)}\\right)$\n",
    "\n",
    "Log likelyhood is give by\n",
    "- $\\displaystyle\\sum_{i=1}^{m}\\lambda(w) = \\displaystyle\\sum_{i=1}^{m}\\log\\left(\\dfrac{P(w|pos)}{P(w|neg)}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f216e837-10cf-4c8b-aba1-f6bd14a8e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word, y)\n",
    "\n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deda9eee-469f-40ed-a119-04e3ef6cd275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos and N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "    \n",
    "    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n",
    "    D_pos = 0\n",
    "    for i in train_y:\n",
    "        if i == 1:\n",
    "            D_pos += 1\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "        \n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af6ce90-f426-49f1-99de-e404f32fe77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "    \n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73362239-c604-463d-855d-4c7664e449c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "            \n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796edfa0-655d-41c0-b4c7-846d8a8965f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
