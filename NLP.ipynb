{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network\n",
    "\n",
    "- Useful for NLP due to \"memory\".\n",
    "- Notation\n",
    "    - $[l]$: $l^{th}$ layer\n",
    "    - $(i)$: $i^{th}$ example\n",
    "    - $\\langle t \\rangle$: $t^{th}$ timestamp\n",
    "    - $i$: $i^{th}$ entry of a vector\n",
    "- Input $x$ is represented by a 3-D tensor $(n_{x},m,T_{x})$\n",
    "    - $n_{x}$: number of units (a single timestamp, a single example) For example, a language with 5000 words one-hot coded into a vector.\n",
    "    - $m$: number of training example.\n",
    "    - $T_{x}$: number of time steps.\n",
    "- Hidden state $a$ is similarly represented by a 3-D tensor $(n_{a},m,T_{x})$\n",
    "- Prediction $\\hat{y}$ is similarly represented by a 3-D tensor $(n_{y},m,T_{y})$\n",
    "    - $T_{y}$: number of time steps in the prediction.\n",
    "\n",
    "## RNN forward path\n",
    "\n",
    "- Input\n",
    "    - $x^{\\langle t \\rangle}$: current input.\n",
    "    - $a^{\\langle t-1 \\rangle}$: previous hidden state.\n",
    "- Output\n",
    "    - $a^{\\langle t \\rangle}$: given to next RNN.\n",
    "    - $\\hat{y}^{\\langle t \\rangle}$: current prediction. \n",
    "- Parameters\n",
    "    - The weights and biases $(W_{aa}, b_{a}, W_{ax}, b_{x})$ are re-used each time step.\n",
    "    \n",
    "$$a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$$\n",
    "\n",
    "$$\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of RNN-cell.\n",
    "\n",
    "    Arguments:\n",
    "    xt -- Input data at timestep \"t\", numpy array of shape (n_x, m)\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "        \n",
    "    Returns:\n",
    "    a_next -- Next hidden state, numpy array of shape (n_a, m)\n",
    "    yt_pred -- Prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- Tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)   \n",
    "    \n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of RNN.\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, numpy array of shape (n_x, m, T_x)\n",
    "    a0 -- Initial hidden state, numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        ba -- Bias numpy array of shape (n_a, 1)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- Tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "        \n",
    "    # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_next = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a[:,:,t], parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Append \"cache\" to \"caches\"\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
    "print(\"a[4][1] = \", a[4][1])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
    "print(\"y_pred.shape = \", y_pred.shape)\n",
    "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN backward pass\n",
    "\n",
    "$$a^{\\langle t \\rangle} = \\tanh(W_{ax} x^{\\langle t \\rangle} + W_{aa} a^{\\langle t-1 \\rangle} + b_{a})$$\n",
    " \n",
    "$$\\displaystyle \\frac{\\partial \\tanh(x)} {\\partial x} = 1 - \\tanh^2(x)$$\n",
    " \n",
    "$$\\displaystyle  {dW_{ax}} = da_{next} * ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) ) x^{\\langle t \\rangle T}$$\n",
    "\n",
    "$$\\displaystyle dW_{aa} = da_{next} * (( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) )  a^{\\langle t-1 \\rangle T}$$\n",
    "\n",
    "$$\\displaystyle db_a = da_{next} * \\sum_{batch}( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) )$$\n",
    " \n",
    "$$\\displaystyle dx^{\\langle t \\rangle} = da_{next} * { W_{ax}}^T ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) )$$\n",
    "  \n",
    "$$\\displaystyle da_{prev} = da_{next} * { W_{aa}}^T ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    Implements a single backward step of RNN-cell.\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to next hidden state.\n",
    "    cache -- python dictionary containing useful values. (output of rnn_cell_forward())\n",
    "\n",
    "    Returns:\n",
    "    gradients -- Python dictionary containing:\n",
    "        dx -- Gradients of input data, numpy array of shape (n_x, m)\n",
    "        da_prev -- Gradients of previous hidden state, numpy array of shape (n_a, m)\n",
    "        dWax -- Gradients of input-to-hidden weights, numpy array of shape (n_a, n_x)\n",
    "        dWaa -- Gradients of hidden-to-hidden weights, numpy array of shape (n_a, n_a)\n",
    "        dba -- Gradients of bias vector, numpy array of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve values from cache.\n",
    "    (a_next, a_prev, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve values from parameters.\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    # Compute the gradient of tanh with respect to a_next.\n",
    "    dtanh = (1- a_next**2) * da_next\n",
    "\n",
    "    # Compute the gradient of the loss with respect to Wax.\n",
    "    dxt = np.dot(Wax.T, dtanh)\n",
    "    dWax = np.dot(dtanh, xt.T)\n",
    "\n",
    "    # Compute the gradient with respect to Waa.\n",
    "    da_prev = np.dot(Waa.T, dtanh)\n",
    "    dWaa = np.dot(dtanh, a_prev.T)\n",
    "\n",
    "    # Compute the gradient with respect to b.\n",
    "    dba = np.sum(dtanh, keepdims=True, axis=-1)\n",
    "\n",
    "    # Store the gradients in a python dictionary.\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "b = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    " \n",
    "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    " \n",
    "da_next = np.random.randn(5,10)\n",
    "gradients = rnn_cell_backward(da_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
    "\n",
    "    Arguments:\n",
    "    da -- Upstream gradients of all hidden states, numpy array of shape (n_a, m, T_x)\n",
    "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "        dx -- Gradient w.r.t. the input data, numpy array of shape (n_x, m, T_x)\n",
    "        da0 -- Gradient w.r.t the initial hidden state, numpy array of shape (n_a, m)\n",
    "        dWax -- Gradient w.r.t the input's weight matrix, numpy array of shape (n_a, n_x)\n",
    "        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy array of shape (n_a, n_a)\n",
    "        dba -- Gradient w.r.t the bias, numpy array of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "        \n",
    "    # Retrieve values from the first cache (t=1) of caches.\n",
    "    (caches, x) = caches\n",
    "    (a1, a0, x1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dimensions from da's and x1's shapes.\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # Initialize the gradients with the right sizes.\n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    dWax = np.zeros((n_a, n_x))\n",
    "    dWaa = np.zeros((n_a, n_a))\n",
    "    dba = np.zeros((n_a, 1))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    \n",
    "    # Loop through all the time steps.\n",
    "    for t in reversed(range(T_x)):\n",
    "        \n",
    "        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step.\n",
    "        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])\n",
    "        \n",
    "        # Retrieve derivatives from gradients.\n",
    "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\n",
    "            \"dWaa\"], gradients[\"dba\"]\n",
    "        \n",
    "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t.\n",
    "        dx[:, :, t] = dxt\n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba += dbat\n",
    "        \n",
    "    # Set da0 to the gradient of a which has been backpropagated through all time-steps.\n",
    "    da0 = da_prevt\n",
    "\n",
    "    # Store the gradients in a python dictionary.\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a, y, caches = rnn_forward(x, a0, parameters)\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = rnn_backward(da, caches)\n",
    " \n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Limitations\n",
    "\n",
    "- Vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM forward path\n",
    "\n",
    "$$\\mathbf{\\Gamma}_{f}^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_{f}[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{f})$$\n",
    "\n",
    "- Forget gate\n",
    "    - A tensor containing values between $0$ and $1$. (Sigmoid function $\\sigma$)\n",
    "        - If a unit is close to $0$, LSTM forgets the previous cell state.\n",
    "        - If a unit is close to $1$, LSTM remembers the previous cell state.\n",
    "    - $a^{\\langle t-1 \\rangle}$ and $x^{\\langle t \\rangle}$ are concatenated together, then multiplied by $\\mathbf{W}_{f}$.\n",
    "    - $\\mathbf{\\Gamma}_{f}$ has the same dimension as the previous cell state $c^{\\langle t-1 \\rangle}$.\n",
    "    \n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right)$$\n",
    "\n",
    "- Candidate value\n",
    "    - A tensor containing values between $-1$ and $1$. ($tanh$ function)\n",
    "    - A tensor containing information that may be stored in current cell state $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "    \n",
    "$$\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_{i}[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{i})$$ \n",
    "\n",
    "- Update gate\n",
    "    - A tensor containing values between $0$ and $1$. (Sigmoid function $\\sigma$)\n",
    "        - If a unit is close to $1$, the candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ is passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "        - If a unit is close to $0$, the candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ is not passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "        \n",
    "$$\\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_{f}^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$$\n",
    "\n",
    "- Cell state\n",
    "    - \"memory\" that gets passed to future time steps.\n",
    "    - Previous cell state is weighted by forget gate.\n",
    "    - Candidate value is weighted by update gate.\n",
    "    \n",
    "$$\\mathbf{\\Gamma}_{o}^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_{o}[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})$$ \n",
    "\n",
    "- Output gate\n",
    "    - A tensor containing values between $0$ and $1$. (Sigmoid function $\\sigma$)\n",
    "    - Decides what gets sent to prediction.\n",
    "    \n",
    "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_{o}^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})$$\n",
    "\n",
    "- Hidden state\n",
    "    - Used to determine three gates ($\\mathbf{\\Gamma}_{f}, \\mathbf{\\Gamma}_{u}, \\mathbf{\\Gamma}_{o}$) of the next time step.\n",
    "    - Also used for the prediction $y^{\\langle t \\rangle}$.\n",
    "    \n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$\n",
    "\n",
    "- Prediction\n",
    "    - Since this is classification, softmax is used.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell.\n",
    "\n",
    "    Arguments:\n",
    "    xt -- Input data at timestep \"t\", numpy array of shape (n_x, m)\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a_next -- next hidden state, numpy array of shape (n_a, m)\n",
    "    c_next -- next memory state, numpy array of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    \n",
    "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
    "          c stands for the cell state (memory)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"] # forget gate weight\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"] # update gate weight \n",
    "    bi = parameters[\"bi\"] \n",
    "    Wc = parameters[\"Wc\"] # candidate value weight\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"] # output gate weight\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"] # prediction weight\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    # Concatenate a_prev and xt\n",
    "    concat = np.concatenate((a_prev, xt))\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a :, :] = xt\n",
    "\n",
    "    # Compute values for ft, it, cct, c_next, ot, a_next\n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)        # forget gate\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)        # update gate\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)       # candidate value\n",
    "    c_next = np.multiply(ft, c_prev) + np.multiply(it, cct)    # cell state\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)        # output gate\n",
    "    a_next = np.multiply(ot, np.tanh(c_next))    # hidden state\n",
    "    \n",
    "    # Compute prediction of the LSTM cell\n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", c_next.shape)\n",
    "print(\"c_next[2] = \", c_next[2])\n",
    "print(\"c_next.shape = \", c_next.shape)\n",
    "print(\"yt[1] =\", yt[1])\n",
    "print(\"yt.shape = \", yt.shape)\n",
    "print(\"cache[1][3] =\", cache[1][3])\n",
    "print(\"len(cache) = \", len(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network using an LSTM-cell.\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, numpy array of shape (n_x, m, T_x)\n",
    "    a0 -- Initial hidden state, numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)\n",
    "    caches -- Tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    Wy = parameters['Wy'] # saving parameters['Wy'] in a local variable\n",
    "    # Retrieve dimensions from shapes of x and parameters['Wy']\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wy'].shape\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next and c_next\n",
    "    a_next = a0\n",
    "    c_next = np.zeros(a_next.shape)\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a[:,:,t], c[:,:,t], parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y\n",
    "        y[:,:,t] = yt\n",
    "        # Save the value of the next cell state\n",
    "        c[:,:,t] = c_next\n",
    "        # Append the cache into caches\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "print(\"a[4][3][6] = \", a[4][3][6])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y[1][4][3] =\", y[1][4][3])\n",
    "print(\"y.shape = \", y.shape)\n",
    "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
    "print(\"c[1][2][1]\", c[1][2][1])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM backward pass\n",
    "\n",
    "- Gate derivatives\n",
    "\n",
    "$$d\\gamma_o^{\\langle t \\rangle} = da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*\\left(1-\\Gamma_{o}^{\\langle t \\rangle}\\right)$$\n",
    "\n",
    "$$dp\\widetilde{c}^{\\langle t \\rangle} = \\left(dc_{next}*\\Gamma_{u}^{\\langle t \\rangle}+ \\Gamma_{o}^{\\langle t \\rangle}* (1-\\tanh^2(c_{next})) * \\Gamma_{u}^{\\langle t \\rangle} * da_{next} \\right) * \\left(1-\\left(\\widetilde c^{\\langle t \\rangle}\\right)^2\\right)$$\n",
    "\n",
    "$$d\\gamma_{u}^{\\langle t \\rangle} = \\left(dc_{next}*\\widetilde{c}^{\\langle t \\rangle} + \\Gamma_{o}^{\\langle t \\rangle}* (1-\\tanh^2(c_{next})) * \\widetilde{c}^{\\langle t \\rangle} * da_{next}\\right)*\\Gamma_{u}^{\\langle t \\rangle}*\\left(1-\\Gamma_{u}^{\\langle t \\rangle}\\right)$$\n",
    "\n",
    "$$d\\gamma_{f}^{\\langle t \\rangle} = \\left(dc_{next}* c_{prev} + \\Gamma_{o}^{\\langle t \\rangle} * (1-\\tanh^2(c_{next})) * c_{prev} * da_{next}\\right)*\\Gamma_{f}^{\\langle t \\rangle}*\\left(1-\\Gamma_{f}^{\\langle t \\rangle}\\right)$$\n",
    "\n",
    "- Parameter derivatives\n",
    "\n",
    "$$dW_f = d\\gamma_{f}^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_{t}\\end{bmatrix}^T$$\n",
    "$$dW_u = d\\gamma_{u}^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_{t}\\end{bmatrix}^T$$\n",
    "$$dW_c = dp\\widetilde c^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_{t}\\end{bmatrix}^T$$\n",
    "$$dW_o = d\\gamma_{o}^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_{t}\\end{bmatrix}^T$$\n",
    "\n",
    "$$\\displaystyle db_{f} = \\sum_{batch}d\\gamma_{f}^{\\langle t \\rangle}$$\n",
    "$$\\displaystyle db_{u} = \\sum_{batch}d\\gamma_{u}^{\\langle t \\rangle}$$\n",
    "$$\\displaystyle db_{c} = \\sum_{batch}d\\gamma_{c}^{\\langle t \\rangle}$$\n",
    "$$\\displaystyle db_{o} = \\sum_{batch}d\\gamma_{o}^{\\langle t \\rangle}$$\n",
    "\n",
    "$$da_{prev} = W_{f}^{T} d\\gamma_{f}^{\\langle t \\rangle} + W_{u}^{T} d\\gamma_{u}^{\\langle t \\rangle}+ W_{c}^{T} dp\\widetilde c^{\\langle t \\rangle} + W_{o}^{T} d\\gamma_{o}^{\\langle t \\rangle}$$\n",
    "$$dc_{prev} = dc_{next}*\\Gamma_{f}^{\\langle t \\rangle} + \\Gamma_{o}^{\\langle t \\rangle} * (1- \\tanh^2(c_{next}))*\\Gamma_{f}^{\\langle t \\rangle}*da_{next}$$\n",
    "$$dx^{\\langle t \\rangle} = W_{f}^{T} d\\gamma_{f}^{\\langle t \\rangle} + W_{u}^{T} d\\gamma_{u}^{\\langle t \\rangle}+ W_{c}^{T} dp\\widetilde c^{\\langle t \\rangle} + W_{o}^{T} d\\gamma_{o}^{\\langle t \\rangle}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for the LSTM-cell. (single time-step)\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
    "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
    "    cache -- Cache storing information from the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    gradients -- Python dictionary containing:\n",
    "        dxt -- Gradient of input data at time-step t, numpy array of shape (n_x, m)\n",
    "        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "        dc_prev -- Gradient w.r.t. the previous memory state, numpy array of shape (n_a, m, T_x)\n",
    "        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        dbf -- Gradient w.r.t. biases of the forget gate, numpy array of shape (n_a, 1)\n",
    "        dbi -- Gradient w.r.t. biases of the update gate, numpy array of shape (n_a, 1)\n",
    "        dbc -- Gradient w.r.t. biases of the memory gate, numpy array of shape (n_a, 1)\n",
    "        dbo -- Gradient w.r.t. biases of the output gate, numpy array of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve information from \"cache\".\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from xt's and a_next's shape.\n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = a_next.shape\n",
    "    \n",
    "    # Compute gates related derivatives.\n",
    "    dot = da_next*np.tanh(c_next)*ot*(1-ot)\n",
    "    dcct = (dc_next*it+ot*(1-np.square(np.tanh(c_next)))*it*da_next)*(1-np.square(cct))\n",
    "    dit = (dc_next*cct+ot*(1-np.square(np.tanh(c_next)))*cct*da_next)*it*(1-it)\n",
    "    dft = (dc_next*c_prev+ot*(1-np.square(np.tanh(c_next)))*c_prev*da_next)*ft*(1-ft)\n",
    "    \n",
    "    dit = None\n",
    "    dft = None\n",
    "    dot = None\n",
    "    dcct = None\n",
    "    \n",
    "    # Compute parameters related derivatives.\n",
    "    dWf = np.dot(dft,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWi = np.dot(dit,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWc = np.dot(dcct,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWo = np.dot(dot,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dbf = np.sum(dft,axis=1,keepdims=True)\n",
    "    dbi = np.sum(dit,axis=1,keepdims=True)\n",
    "    dbc = np.sum(dcct,axis=1,keepdims=True)\n",
    "    dbo = np.sum(dot,axis=1,keepdims=True)\n",
    "\n",
    "    # Compute derivatives w.r.t previous hidden state, previous memory state and input.\n",
    "    da_prev = np.dot(parameters['Wf'][:,:n_a].T,dft)+np.dot(parameters['Wi'][:,:n_a].T,dit)+np.dot(parameters['Wc'][:,:n_a].T,dcct)+np.dot(parameters['Wo'][:,:n_a].T,dot)\n",
    "    dc_prev = dc_next*ft+ot*(1-np.square(np.tanh(c_next)))*ft*da_next\n",
    "    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft)+np.dot(parameters['Wi'][:,n_a:].T,dit)+np.dot(parameters['Wc'][:,n_a:].T,dcct)+np.dot(parameters['Wo'][:,n_a:].T,dot)\n",
    "    \n",
    "    # Save gradients in dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    " \n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    " \n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    " \n",
    "da_next = np.random.randn(5,10)\n",
    "dc_next = np.random.randn(5,10)\n",
    "gradients = lstm_cell_backward(da_next, dc_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dc_prev\\\"][2][3] =\", gradients[\"dc_prev\"][2][3])\n",
    "print(\"gradients[\\\"dc_prev\\\"].shape =\", gradients[\"dc_prev\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the backward pass for the RNN with LSTM-cell. (over a whole sequence)\n",
    "\n",
    "    Arguments:\n",
    "    da -- Gradients w.r.t the hidden states, numpy array of shape. (n_a, m, T_x)\n",
    "    caches -- Cache storing information from the forward pass. (lstm_forward)\n",
    "\n",
    "    Returns:\n",
    "    gradients -- Python dictionary containing:\n",
    "        dx -- Gradient of inputs, numpy array of shape (n_x, m, T_x)\n",
    "        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        dbf -- Gradient w.r.t. biases of the forget gate, numpy array of shape (n_a, 1)\n",
    "        dbi -- Gradient w.r.t. biases of the update gate, numpy array of shape (n_a, 1)\n",
    "        dbc -- Gradient w.r.t. biases of the memory gate, numpy array of shape (n_a, 1)\n",
    "        dbo -- Gradient w.r.t. biases of the save gate, numpy array of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve values from the first cache (t=1) of caches.\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dimensions from da's and x1's shapes.\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # initialize the gradients with the right sizes.\n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    dc_prevt = np.zeros((n_a, m))\n",
    "    dWf = np.zeros((n_a, n_a + n_x))\n",
    "    dWi = np.zeros((n_a, n_a + n_x))\n",
    "    dWc = np.zeros((n_a, n_a + n_x))\n",
    "    dWo = np.zeros((n_a, n_a + n_x))\n",
    "    dbf = np.zeros((n_a, 1))\n",
    "    dbi = np.zeros((n_a, 1))\n",
    "    dbc = np.zeros((n_a, 1))\n",
    "    dbo = np.zeros((n_a, 1))\n",
    "    \n",
    "    # loop back over the whole sequence\n",
    "    for t in reversed(range(T_x)):\n",
    "        \n",
    "        # Compute all gradients using lstm_cell_backward.\n",
    "        gradients = lstm_cell_backward(da[:,:,t]+da_prevt,dc_prevt,caches[t])\n",
    "        \n",
    "        # Store or add the gradient to the parameters' previous step's gradient.\n",
    "        dx[:, :, t] = gradients['dxt']\n",
    "        dWf = dWf+gradients['dWf']\n",
    "        dWi = dWi+gradients['dWi']\n",
    "        dWc = dWc+gradients['dWc']\n",
    "        dWo = dWo+gradients['dWo']\n",
    "        dbf = dbf+gradients['dbf']\n",
    "        dbi = dbi+gradients['dbi']\n",
    "        dbc = dbc+gradients['dbc']\n",
    "        dbo = dbo+gradients['dbo']\n",
    "        \n",
    "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
    "    da0 = gradients['da_prev']\n",
    "    \n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    " \n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    " \n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    " \n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = lstm_backward(da, caches)\n",
    " \n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient clipping\n",
    "\n",
    "- Every element of the gradient vector is clipped to lie between some range $[-N, N]$. \n",
    "    - For example, if the $N=10$\n",
    "        - If any component of the gradient vector is greater than $10$, it is set to $10$.\n",
    "        - If any component of the gradient vector is less than $-10$, it is set to $-10$. \n",
    "        - If any components are between $-10$ and $10$, they keep their original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- A dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\".\n",
    "    maxValue -- Everything above this number is set to this number, and everything less than -maxValue is set to -maxValue.\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- A dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    # Clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        gradient = np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "- Assuming model is already trained, generate new texts\n",
    "    - 1) Pass in $x^{\\langle 1 \\rangle} = \\vec{0}$ (vector of zeros) and also set $a^{\\langle 0 \\rangle} = \\vec{0}$\n",
    "    - 2) Run one step of forward propagation to get $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$\n",
    "        - $a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)$\n",
    "        - $z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_{y}$\n",
    "        - $\\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })$\n",
    "            - $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character\n",
    "    - 3) Pick the next character's index according to the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$\n",
    "    - 4) overwrite the variable `x`, which currently stores $x^{\\langle t \\rangle }$, with the value of $x^{\\langle t + 1 \\rangle }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- Python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- Python dictionary mapping each character to an index.\n",
    "    \n",
    "    Returns:\n",
    "    indices -- A list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary.\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Create the one-hot vector x for the first character. (initializing the sequence generation)\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # Initialize a_prev as zeros.\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate.\n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1.\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "\n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Forward propagate x using the equations.\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by \n",
    "        y = softmax(z)\n",
    "        \n",
    "        # Sample the index of a character within the vocabulary from the probability distribution y.\n",
    "        idx = index = np.random.choice(list(range(vocab_size)), p = y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\".\n",
    "        indices.append(index)\n",
    "        \n",
    "        # Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1 \n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\".\n",
    "        a_prev = a\n",
    "        \n",
    "        counter +=1\n",
    "        \n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/nlp/dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n",
    "\n",
    "chars = sorted(chars)\n",
    "print(chars)\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(ix_to_char)\n",
    "\n",
    "np.random.seed(2)\n",
    "_, n_a = 20, 100\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "### One step of attention\n",
    "\n",
    "<img src=\"img/nlp/attention1.png\" style=\"width:500;height:500px;\">\n",
    "\n",
    "- Copy $s^{\\langle t-1 \\rangle}$'s value $T_{x}$ times.\n",
    "- Then, concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ to compute $e^{\\langle t, t'\\rangle}$.\n",
    "- Then, $e^{\\langle t, t' \\rangle}$ is passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "\n",
    "### Entire model using attention\n",
    "\n",
    "<img src=\"img/nlp/attention2.png\" style=\"width:500;height:500px;\">\n",
    "\n",
    "- The pre-attention Bi-LSTM goes through $T_{x}$ time steps\n",
    "- The post-attention LSTM goes through $T_{y}$ time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "- Encoder-decoder structure\n",
    "    - Encoder maps an input sequence $(x_{1} \\dots x_{n})$ to a sequence of continuous representation $(z_{1} \\dots z_{n})$ \n",
    "    - Given $\\vec{z}$, decoder generates an output sequence $(y_{1} \\dots y_{n})$, one element at a time.\n",
    "    - Each step of model consumes previous output as an additional input when generating the next.\n",
    "    \n",
    "<img src=\"img/nlp/transformer1.png\" style=\"width:500;height:500px;\">\n",
    "\n",
    "- Encoder (left side of the picture. There are Six $N=6$ of them)\n",
    "    - Sub-layer #1: multi-head self-attention.\n",
    "    - Sub-layer #2: fully connected feed-forward.\n",
    "    - There are also residual connection and layer normalization.\n",
    "\n",
    "- Decoder (right side of the picture. There are Six $N=6$ of them)\n",
    "    - Sub-layer #1: multi-head self-attention.\n",
    "    - Sub-layer #2: fully connected feed-forward.\n",
    "    - Sub-layer #4: multi-head attention over the output of encoder stack.\n",
    "    - There are also residual connection and layer normalization.\n",
    "    - Modify the self-attention sub-layer in the decoder stack. (masking)\n",
    "    \n",
    "<img src=\"img/nlp/transformer2.png\" style=\"width:500;height:500px;\">\n",
    "\n",
    "- Multi-Head attention\n",
    "    - Linearly project the queries, keys, and values $Q, K, V$ to $d_{k}, d_{k}, d_{v}$ $h$ times.\n",
    "    - Perform attention function in parallel, yielding $d_{v}$ dimensional output values.\n",
    "    - These are concatenated and projected to produce the final value.\n",
    "    \n",
    "- Feed forward network\n",
    "    - $FFN(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "- Need pretrained models (PTM) to overcome small data sets in NLP.\n",
    "    - Word embedding PTM: Word2vec, GloVe \n",
    "    - Contextual workd embedding PTM: GPT, BERT, XLNet\n",
    "    \n",
    "### Autoregressive language modelling\n",
    "\n",
    "- Predict the next word in sentence.\n",
    "- Example: GPT\n",
    "\n",
    "### Masked language modelling\n",
    "\n",
    "- Mask out some tokens from input sentence.\n",
    "- Train the model to predict the masked tokens.\n",
    "- Solved as classification problem.\n",
    "- Example: BERT\n",
    "\n",
    "### Contrastive learing\n",
    "\n",
    "- Train the model to distinguish whether two input sentences are continuous segments from training corpus.\n",
    "- Example: BERT\n",
    "\n",
    "### BERT\n",
    "\n",
    "- Input\n",
    "    - [CLS]: special classification token. \n",
    "    - [SEP]: separation token. (separates the sentences)\n",
    "    - Two sentences with masked words.\n",
    "- Token embedding: words with 30k vocabulary.\n",
    "- Segment embedding: encodes the sentence of the word.\n",
    "- Position embedding: enchodes the position of the word.\n",
    "\n",
    "<img src=\"img/nlp/bert1.png\" style=\"width:500;height:500px;\">\n",
    "\n",
    "- Use the final hidden state of masked words to perform classification.\n",
    "- 30k vocabularies = 30k classes.\n",
    "- Cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
