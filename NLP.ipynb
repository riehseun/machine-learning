{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network\n",
    "\n",
    "- Useful for NLP due to \"memory\".\n",
    "- Notation\n",
    "    - $[l]$: $l^{th}$ layer\n",
    "    - $(i)$: $i^{th}$ example\n",
    "    - $\\langle t \\rangle$: $t^{th}$ timestamp\n",
    "    - $i$: $i^{th}$ entry of a vector\n",
    "- Input $x$ is represented by a 3-D tensor $(n_{x},m,T_{x})$\n",
    "    - $n_{x}$: number of units (a single timestamp, a single example) For example, a language with 5000 words one-hot coded into a vector.\n",
    "    - $m$: number of training example.\n",
    "    - $T_{x}$: number of time steps.\n",
    "- Hidden state $a$ is similarly represented by a 3-D tensor $(n_{a},m,T_{x})$\n",
    "- Prediction $\\hat{y}$ is similarly represented by a 3-D tensor $(n_{y},m,T_{y})$\n",
    "    - $T_{y}$: number of time steps in the prediction.\n",
    "\n",
    "## Single RNN cell\n",
    "\n",
    "- Input\n",
    "    - $x^{\\langle t \\rangle}$: current input.\n",
    "    - $a^{\\langle t-1 \\rangle}$: previous hidden state.\n",
    "- Output\n",
    "    - $a^{\\langle t \\rangle}$: given to next RNN.\n",
    "    - $\\hat{y}^{\\langle t \\rangle}$: current prediction. \n",
    "- Parameters\n",
    "    - The weights and biases $(W_{aa}, b_{a}, W_{ax}, b_{x})$ are re-used each time step.\n",
    "    \n",
    "$$a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$$\n",
    "\n",
    "$$\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of RNN-cell.\n",
    "\n",
    "    Arguments:\n",
    "    xt -- Input data at timestep \"t\", numpy array of shape (n_x, m)\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "        \n",
    "    Returns:\n",
    "    a_next -- Next hidden state, numpy array of shape (n_a, m)\n",
    "    yt_pred -- Prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- Tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)   \n",
    "    \n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN forward path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of RNN.\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, numpy array of shape (n_x, m, T_x)\n",
    "    a0 -- Initial hidden state, numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        ba -- Bias numpy array of shape (n_a, 1)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- Tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "        \n",
    "    # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_next = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a[:,:,t], parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Append \"cache\" to \"caches\"\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Limitations\n",
    "\n",
    "- Vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM \n",
    "\n",
    "$$\\mathbf{\\Gamma}_{f}^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_{f}[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{f})$$\n",
    "\n",
    "- Forget gate\n",
    "    - A tensor containing values between $0$ and $1$. (Sigmoid function $\\sigma$)\n",
    "        - If a unit is close to $0$, LSTM forgets the previous cell state.\n",
    "        - If a unit is close to $1$, LSTM remembers the previous cell state.\n",
    "    - $a^{\\langle t-1 \\rangle}$ and $x^{\\langle t \\rangle}$ are concatenated together, then multiplied by $\\mathbf{W}_{f}$.\n",
    "    - $\\mathbf{\\Gamma}_{f}$ has the same dimension as the previous cell state $c^{\\langle t-1 \\rangle}$.\n",
    "    \n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right)$$\n",
    "\n",
    "- Candidate value\n",
    "    - A tensor containing values between $-1$ and $1$. ($tanh$ function)\n",
    "    - A tensor containing information that may be stored in current cell state $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "    \n",
    "$$\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_{i}[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{i})$$ \n",
    "\n",
    "- Update gate\n",
    "    - A tensor containing values between $0$ and $1$. (Sigmoid function $\\sigma$)\n",
    "        - If a unit is close to $1$, the candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ is passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "        - If a unit is close to $0$, the candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ is not passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "        \n",
    "$$\\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_{f}^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$$\n",
    "\n",
    "- Cell state\n",
    "    - \"memory\" that gets passed to future time steps.\n",
    "    - Previous cell state is weighted by forget gate.\n",
    "    - Candidate value is weighted by update gate.\n",
    "    \n",
    "$$\\mathbf{\\Gamma}_{o}^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_{o}[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})$$ \n",
    "\n",
    "- Output gate\n",
    "    - A tensor containing values between $0$ and $1$. (Sigmoid function $\\sigma$)\n",
    "    - Decides what gets sent to prediction.\n",
    "    \n",
    "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_{o}^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})$$\n",
    "\n",
    "- Hidden state\n",
    "    - Used to determine three gates ($\\mathbf{\\Gamma}_{f}, \\mathbf{\\Gamma}_{u}, \\mathbf{\\Gamma}_{o}$) of the next time step.\n",
    "    - Also used for the prediction $y^{\\langle t \\rangle}$.\n",
    "    \n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$\n",
    "\n",
    "- Prediction\n",
    "    - Since this is classification, softmax is used.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell.\n",
    "\n",
    "    Arguments:\n",
    "    xt -- Input data at timestep \"t\", numpy array of shape (n_x, m)\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a_next -- next hidden state, numpy array of shape (n_a, m)\n",
    "    c_next -- next memory state, numpy array of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    \n",
    "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
    "          c stands for the cell state (memory)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"] # forget gate weight\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"] # update gate weight \n",
    "    bi = parameters[\"bi\"] \n",
    "    Wc = parameters[\"Wc\"] # candidate value weight\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"] # output gate weight\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"] # prediction weight\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    # Concatenate a_prev and xt\n",
    "    concat = np.concatenate((a_prev, xt))\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a :, :] = xt\n",
    "\n",
    "    # Compute values for ft, it, cct, c_next, ot, a_next\n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)        # forget gate\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)        # update gate\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)       # candidate value\n",
    "    c_next = np.multiply(ft, c_prev) + np.multiply(it, cct)    # cell state\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)        # output gate\n",
    "    a_next = np.multiply(ot, np.tanh(c_next))    # hidden state\n",
    "    \n",
    "    # Compute prediction of the LSTM cell\n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network using an LSTM-cell.\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, numpy array of shape (n_x, m, T_x)\n",
    "    a0 -- Initial hidden state, numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)\n",
    "    caches -- Tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    Wy = parameters['Wy'] # saving parameters['Wy'] in a local variable\n",
    "    # Retrieve dimensions from shapes of x and parameters['Wy']\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wy'].shape\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next and c_next\n",
    "    a_next = a0\n",
    "    c_next = np.zeros(a_next.shape)\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a[:,:,t], c[:,:,t], parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y\n",
    "        y[:,:,t] = yt\n",
    "        # Save the value of the next cell state\n",
    "        c[:,:,t] = c_next\n",
    "        # Append the cache into caches\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
