{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network\n",
    "\n",
    "- Useful for NLP due to \"memory\".\n",
    "- Notation\n",
    "    - $[l]$: $l^{th}$ layer\n",
    "    - $(i)$: $i^{th}$ example\n",
    "    - $\\langle t \\rangle$: $t^{th}$ timestamp\n",
    "    - $i$: $i^{th}$ entry of a vector\n",
    "- Input $x$ is represented by a 3-D tensor $(n_{x},m,T_{x})$\n",
    "    - $n_{x}$: number of units (a single timestamp, a single example) For example, a language with 5000 words one-hot coded into a vector.\n",
    "    - $m$: number of training example.\n",
    "    - $T_{x}$: number of time steps.\n",
    "- Hidden state $a$ is similarly represented by a 3-D tensor $(n_{a},m,T_{x})$\n",
    "- Prediction $\\hat{y}$ is similarly represented by a 3-D tensor $(n_{y},m,T_{y})$\n",
    "    - $T_{y}$: number of time steps in the prediction.\n",
    "\n",
    "## Single RNN cell\n",
    "\n",
    "- Input\n",
    "    - $x^{\\langle t \\rangle}$: current input.\n",
    "    - $a^{\\langle t-1 \\rangle}$: previous hidden state.\n",
    "- Output\n",
    "    - $a^{\\langle t \\rangle}$: given to next RNN.\n",
    "    - $\\hat{y}^{\\langle t \\rangle}$: current prediction. \n",
    "- Parameters\n",
    "    - The weights and biases $(W_{aa}, b_{a}, W_{ax}, b_{x})$ are re-used each time step.\n",
    "    \n",
    "$$a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$$\n",
    "\n",
    "$$\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of RNN-cell.\n",
    "\n",
    "    Arguments:\n",
    "    xt -- Input data at timestep \"t\", numpy array of shape (n_x, m)\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "        \n",
    "    Returns:\n",
    "    a_next -- Next hidden state, numpy array of shape (n_a, m)\n",
    "    yt_pred -- Prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- Tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)   \n",
    "    \n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN forward path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of RNN.\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, numpy array of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, numpy array of shape (n_a, m)\n",
    "    parameters -- Python dictionary containing:\n",
    "        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "        ba --  Bias numpy array of shape (n_a, 1)\n",
    "        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- Tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "        \n",
    "    # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_next = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a[:,:,t], parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Append \"cache\" to \"caches\"\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Limitations\n",
    "\n",
    "- Vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM \n",
    "\n",
    "- Forget gate\n",
    "    - A tensor containing values between $0$ and $1$.\n",
    "        - If a unit is close to $0$, LSTM forgets the previous cell state.\n",
    "        - If a unit is close to $1$, LSTM remembers the previous cell state.\n",
    "\n",
    "$$\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
