{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e494bd-6bff-4cd6-a9f5-f92336354d7f",
   "metadata": {},
   "source": [
    "# Entity linking\n",
    "\n",
    "## Scope\n",
    "- Named entity recognition\n",
    "    - Detect person, organization, location, etc.\n",
    "- Disambiguation\n",
    "    - Map each detected entity to corresponding entity in knowledge base.\n",
    "    - For example, “Michael Jordan is a machine learning professor at UC Berkeley.”\n",
    "        - Michael Jordan linked to the professor at the University of California, Berkeley entity in the knowledge base.\n",
    "        - UC Berkeley is linked to the University of California entity in the knowledge base.\n",
    "\n",
    "## Metric\n",
    "- There will be separate metric for each of three components.\n",
    "    - Named entity recognition\n",
    "    - Disambiguation\n",
    "    - Entity linking as a whole\n",
    "    \n",
    "### Named entity recognition (Offline)\n",
    "\n",
    "<img src=\"img/entity-linking1.png\" style=\"width:800px;height:800px;\">\n",
    "\n",
    "- Precision = # of correctly recognized named entities / # of total recognized named entitied\n",
    "- Recall = # of correctly recognized named entities / # of named entities in corpus\n",
    "- F1 score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "### Disambiguation (Offline)\n",
    "\n",
    "<img src=\"img/entity-linking2.png\" style=\"width:400px;height:200px;\">\n",
    "<img src=\"img/entity-linking3.png\" style=\"width:400px;height:200px;\">\n",
    "<img src=\"img/entity-linking4.png\" style=\"width:400px;height:200px;\">\n",
    "\n",
    "- Recall doesn't make sense.\n",
    "- Precision = # of mentions correctly linked / # of total mentions\n",
    "\n",
    "### Micro average (Offline)\n",
    "- Aggregates contributions of all documents to compute average.\n",
    "- Precision = sum of TP / (sum of TP + sum of FP)\n",
    "- Recall = sum of TP / (sum of TP + sum of FN)\n",
    "- Micro-averaged F1-score is computed using above.\n",
    "\n",
    "### Macro average (Offline)\n",
    "- Computes metrics independently for each document and takes the average.\n",
    "- Precision = sum of Precision over documents / n \n",
    "- Recall = sum of Recall over documents / n \n",
    "- Macro-averaged F1-score is computed using above.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "<img src=\"img/entity-linking5.png\" style=\"width:1000px;height:600px;\">\n",
    "\n",
    "### Model\n",
    "- Traditional word embedding like Word2vec does not understand the context.\n",
    "\n",
    "### ELMo (Embeddings from Language Models)\n",
    "- Starts with something like Word2vec.\n",
    "- Raw vectors are fed into bidirectional LSTM layer.\n",
    "- Forward and backward LSTMs are trained independently.\n",
    "- Word representations cannot take advantage of left and right context simultaneously.\n",
    "\n",
    "### BERT (Bidirectional encoder representations from transformers)\n",
    "- Take input sentenses, which can be multiple sentences separated by SEP tag.\n",
    "- Each word is converted to embedding and fed into transformer encoder layer.\n",
    "- All words are processed simultaneously in the layer.\n",
    "- Final transformer layer outputs the contextualized representation of each word.\n",
    "\n",
    "### NER modelling\n",
    "- Option 1. Use embeddings generated by BERT as features in NER modelling.\n",
    "- Option 2. Take pre-trained models and fine-tune them based on NER dataset.\n",
    "\n",
    "### Disambiguation modeling\n",
    "\n",
    "#### Candidate generation\n",
    "- Build an index where terms are mapped to knowledge base entities.\n",
    "- Index should include all terms that could possibly refer to an entity.\n",
    "\n",
    "#### Linking\n",
    "- Build a model that gives the probability of a candidate being true match for a recognized entity.\n",
    "- Inputs to this model should be represented by BERT/ELMo embeddings.\n",
    "\n",
    "## Training data generation\n",
    "\n",
    "### Open dataset\n",
    "- Named entity recognition\n",
    "    - CoNLL-2003\n",
    "- Disambiguation\n",
    "    - AIDA CoNLL-YAGO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c143b70c-6439-4960-a137-95f317919b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
