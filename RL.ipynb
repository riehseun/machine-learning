{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c\n",
    "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.2402&rep=rep1&type=pdf\n",
    "\n",
    "# Imitation learning\n",
    "\n",
    "$S$: set of states<br>\n",
    "$A$: set of actions<br>\n",
    "$P(s^{'}|s,a)$: probability that action $a$ in state $s$ can lead to state $s^{'}$<br>\n",
    "$R(s,a)$: reward function<br>\n",
    "$\\pi$: policy where the actions are based on<br>\n",
    "$\\tau = (s_{0},a_{0},s_{1},a_{1},\\dots)$: expert demonstartions or tragectories<br>\n",
    "$\\pi^{*}$: optimal policy where actions given by $\\tau$\n",
    "\n",
    "## Behavioral cloning\n",
    "\n",
    "- Collect $\\pi^{*}$ from experts\n",
    "- Assume demonstrations are i.i.d. $(s_{0}^{*},a_{0}^{*}), (s_{1}^{*},a_{1}^{*}),\\dots$\n",
    "- Learn $\\pi_{\\theta}$ by minimizing $L(a^{*},\\pi_{\\theta}(s))$ \n",
    "\n",
    "### Limitations\n",
    "\n",
    "- An action in a given state induces the next one in Markov Decision Process (MDP), which breaks i.i.d. assumption\n",
    "- If agent makes a mistake, it can land on a state that expert never visited, causing catastrophic failure.\n",
    "\n",
    "## Options framework\n",
    "\n",
    "- Define expected discounted future reward\n",
    "    - $Q^{\\pi}(s,a) = E_{\\pi}\\left[r_{t+1} + \\gamma r_{t+2} + \\dots | s_{t}=s,a_{t}=a\\right]$\n",
    "        - Where $r_{t}$ represents the reward at time step $t$ \n",
    "- Q-learning\n",
    "    - $Q(s_{t},a_{t}) \\leftarrow Q\\left(s_{t},a_{t} + \\alpha(r_{t}+1) + \\gamma \\underset{{a^{'} \\in A}}{max}Q(s_{t+1},a^{'}) - Q(s_{t},a_{t})\\right)$\n",
    "- Options\n",
    "    - Policy $\\pi$\n",
    "    - Termination condition $\\beta$\n",
    "    - Input set $I \\subseteq S$\n",
    "    - An option is available in state $s$ iif $s \\in I$\n",
    "    - If option is taken, then actions are selected according to $\\pi$, until the option terminates according to $\\beta$ \n",
    "- Define the value of taking option $o$ in state $s \\in I$ under policy $\\mu$\n",
    "    - $Q^{\\mu}(s,o) = E\\left[r_{t+1} + \\gamma r_{t+2} + \\dots | E[o\\mu,s,t]\\right]$\n",
    "        - $o\\mu$ denotes the policy that first follows $o$ until it terminates, then initiates $\\mu$ in the resulting state\n",
    "        - $E[o\\mu,s,t]$ denotes the execution of $o\\mu$ in state $s$ at starting time $t$\n",
    "- Any MDP with a set of fixed options is SMDP (Semi-MDP)\n",
    "- Q-learning (SMDP version)\n",
    "    - $Q(s,o) \\leftarrow Q(s,o) + \\alpha\\left(r + \\gamma^{k} \\underset{{a \\in O}}{max}Q(s^{'},a) - Q(s,o)\\right)$\n",
    "        - $k$ denotes the number of time steps elapsing between $s$ and $s^{'}$\n",
    "        - $r$ denotes the cumulative discounted reward over $k$\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
