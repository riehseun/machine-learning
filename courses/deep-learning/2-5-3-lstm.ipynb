{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvise a Jazz Solo with an LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import groupby, zip_longest\n",
    "import IPython\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Problem statement\n",
    "\n",
    "You would like to create a jazz music piece specially for a friend's birthday. However, you don't know any instruments or music composition. Fortunately, you know deep learning and will solve this problem using an LSTM netwok.  \n",
    "\n",
    "You will train a network to generate novel jazz solos in a style representative of a body of performed work.\n",
    "\n",
    "<img src=\"images/recurrent-neural-network/jazz.jpg\" style=\"width:450;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Dataset\n",
    "\n",
    "We have taken care of the preprocessing of the musical data to render it in terms of musical \"values.\" You can informally think of each \"value\" as a note, which comprises a pitch and a duration. For example, if you press down a specific piano key for 0.5 seconds, then you have just played a note. In music theory, a \"value\" is actually more complicated than this--specifically, it also captures the information needed to play multiple notes at the same time. For example, when playing a music piece, you might press down two piano keys at the same time (playing multiple notes at the same time generates what's called a \"chord\"). But we don't need to worry about the details of music theory for this assignment. For the purpose of this assignment, all you need to know is that we will obtain a dataset of values, and will learn an RNN model to generate sequences of values. \n",
    "\n",
    "Our music generation system will use 78 unique values. Run the following code to load the raw music data and preprocess it into values. This might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __parse_midi(data_fn):\n",
    "    # Parse the MIDI data for separate melody and accompaniment parts.\n",
    "    midi_data = converter.parse(data_fn)\n",
    "    # Get melody part, compress into single voice.\n",
    "    melody_stream = midi_data[5]     # For Metheny piece, Melody is Part #5.\n",
    "    melody1, melody2 = melody_stream.getElementsByClass(stream.Voice)\n",
    "    for j in melody2:\n",
    "        melody1.insert(j.offset, j)\n",
    "    melody_voice = melody1\n",
    "\n",
    "    for i in melody_voice:\n",
    "        if i.quarterLength == 0.0:\n",
    "            i.quarterLength = 0.25\n",
    "\n",
    "    # Change key signature to adhere to comp_stream (1 sharp, mode = major).\n",
    "    # Also add Electric Guitar. \n",
    "    melody_voice.insert(0, instrument.ElectricGuitar())\n",
    "    melody_voice.insert(0, key.KeySignature(sharps=1))\n",
    "\n",
    "    # The accompaniment parts. Take only the best subset of parts from\n",
    "    # the original data. Maybe add more parts, hand-add valid instruments.\n",
    "    # Should add least add a string part (for sparse solos).\n",
    "    # Verified are good parts: 0, 1, 6, 7 '''\n",
    "    partIndices = [0, 1, 6, 7]\n",
    "    comp_stream = stream.Voice()\n",
    "    comp_stream.append([j.flat for i, j in enumerate(midi_data) \n",
    "        if i in partIndices])\n",
    "\n",
    "    # Full stream containing both the melody and the accompaniment. \n",
    "    # All parts are flattened. \n",
    "    full_stream = stream.Voice()\n",
    "    for i in range(len(comp_stream)):\n",
    "        full_stream.append(comp_stream[i])\n",
    "    full_stream.append(melody_voice)\n",
    "\n",
    "    # Extract solo stream, assuming you know the positions ..ByOffset(i, j).\n",
    "    # Note that for different instruments (with stream.flat), you NEED to use\n",
    "    # stream.Part(), not stream.Voice().\n",
    "    # Accompanied solo is in range [478, 548)\n",
    "    solo_stream = stream.Voice()\n",
    "    for part in full_stream:\n",
    "        curr_part = stream.Part()\n",
    "        curr_part.append(part.getElementsByClass(instrument.Instrument))\n",
    "        curr_part.append(part.getElementsByClass(tempo.MetronomeMark))\n",
    "        curr_part.append(part.getElementsByClass(key.KeySignature))\n",
    "        curr_part.append(part.getElementsByClass(meter.TimeSignature))\n",
    "        curr_part.append(part.getElementsByOffset(476, 548, \n",
    "                                                  includeEndBoundary=True))\n",
    "        cp = curr_part.flat\n",
    "        solo_stream.insert(cp)\n",
    "\n",
    "    # Group by measure so you can classify. \n",
    "    # Note that measure 0 is for the time signature, metronome, etc. which have\n",
    "    # an offset of 0.0.\n",
    "    melody_stream = solo_stream[-1]\n",
    "    measures = OrderedDict()\n",
    "    offsetTuples = [(int(n.offset / 4), n) for n in melody_stream]\n",
    "    measureNum = 0 # for now, don't use real m. nums (119, 120)\n",
    "    for key_x, group in groupby(offsetTuples, lambda x: x[0]):\n",
    "        measures[measureNum] = [n[1] for n in group]\n",
    "        measureNum += 1\n",
    "\n",
    "    # Get the stream of chords.\n",
    "    # offsetTuples_chords: group chords by measure number.\n",
    "    chordStream = solo_stream[0]\n",
    "    chordStream.removeByClass(note.Rest)\n",
    "    chordStream.removeByClass(note.Note)\n",
    "    offsetTuples_chords = [(int(n.offset / 4), n) for n in chordStream]\n",
    "\n",
    "    # Generate the chord structure. Use just track 1 (piano) since it is\n",
    "    # the only instrument that has chords. \n",
    "    # Group into 4s, just like before. \n",
    "    chords = OrderedDict()\n",
    "    measureNum = 0\n",
    "    for key_x, group in groupby(offsetTuples_chords, lambda x: x[0]):\n",
    "        chords[measureNum] = [n[1] for n in group]\n",
    "        measureNum += 1\n",
    "\n",
    "    # Fix for the below problem.\n",
    "    #   1) Find out why len(measures) != len(chords).\n",
    "    #   ANSWER: resolves at end but melody ends 1/16 before last measure so doesn't\n",
    "    #           actually show up, while the accompaniment's beat 1 right after does.\n",
    "    #           Actually on second thought: melody/comp start on Ab, and resolve to\n",
    "    #           the same key (Ab) so could actually just cut out last measure to loop.\n",
    "    #           Decided: just cut out the last measure. \n",
    "    del chords[len(chords) - 1]\n",
    "    assert len(chords) == len(measures)\n",
    "\n",
    "    return measures, chords\n",
    "\n",
    "\n",
    "def __is_scale_tone(chord, note):\n",
    "    # Method: generate all scales that have the chord notes th check if note is\n",
    "    # in names\n",
    "\n",
    "    # Derive major or minor scales (minor if 'other') based on the quality\n",
    "    # of the chord.\n",
    "    scaleType = scale.DorianScale() # i.e. minor pentatonic\n",
    "    if chord.quality == 'major':\n",
    "        scaleType = scale.MajorScale()\n",
    "    # Can change later to deriveAll() for flexibility. If so then use list\n",
    "    # comprehension of form [x for a in b for x in a].\n",
    "    scales = scaleType.derive(chord) # use deriveAll() later for flexibility\n",
    "    allPitches = list(set([pitch for pitch in scales.getPitches()]))\n",
    "    allNoteNames = [i.name for i in allPitches] # octaves don't matter\n",
    "\n",
    "    # Get note name. Return true if in the list of note names.\n",
    "    noteName = note.name\n",
    "    return (noteName in allNoteNames)\n",
    "\n",
    "\n",
    "def parse_melody(fullMeasureNotes, fullMeasureChords):\n",
    "    # Remove extraneous elements.x\n",
    "    measure = copy.deepcopy(fullMeasureNotes)\n",
    "    chords = copy.deepcopy(fullMeasureChords)\n",
    "    measure.removeByNotOfClass([note.Note, note.Rest])\n",
    "    chords.removeByNotOfClass([chord.Chord])\n",
    "\n",
    "    # Information for the start of the measure.\n",
    "    # 1) measureStartTime: the offset for measure's start, e.g. 476.0.\n",
    "    # 2) measureStartOffset: how long from the measure start to the first element.\n",
    "    measureStartTime = measure[0].offset - (measure[0].offset % 4)\n",
    "    measureStartOffset  = measure[0].offset - measureStartTime\n",
    "\n",
    "    # Iterate over the notes and rests in measure, finding the grammar for each\n",
    "    # note in the measure and adding an abstract grammatical string for it. \n",
    "\n",
    "    fullGrammar = \"\"\n",
    "    prevNote = None # Store previous note. Need for interval.\n",
    "    numNonRests = 0 # Number of non-rest elements. Need for updating prevNote.\n",
    "    for ix, nr in enumerate(measure):\n",
    "        # Get the last chord. If no last chord, then (assuming chords is of length\n",
    "        # >0) shift first chord in chords to the beginning of the measure.\n",
    "        try: \n",
    "            lastChord = [n for n in chords if n.offset <= nr.offset][-1]\n",
    "        except IndexError:\n",
    "            chords[0].offset = measureStartTime\n",
    "            lastChord = [n for n in chords if n.offset <= nr.offset][-1]\n",
    "\n",
    "        # FIRST, get type of note, e.g. R for Rest, C for Chord, etc.\n",
    "        # Dealing with solo notes here. If unexpected chord: still call 'C'.\n",
    "        elementType = ' '\n",
    "        # R: First, check if it's a rest. Clearly a rest --> only one possibility.\n",
    "        if isinstance(nr, note.Rest):\n",
    "            elementType = 'R'\n",
    "        # C: Next, check to see if note pitch is in the last chord.\n",
    "        elif nr.name in lastChord.pitchNames or isinstance(nr, chord.Chord):\n",
    "            elementType = 'C'\n",
    "        # L: (Complement tone) Skip this for now.\n",
    "        # S: Check if it's a scale tone.\n",
    "        elif __is_scale_tone(lastChord, nr):\n",
    "            elementType = 'S'\n",
    "        # A: Check if it's an approach tone, i.e. +-1 halfstep chord tone.\n",
    "        elif __is_approach_tone(lastChord, nr):\n",
    "            elementType = 'A'\n",
    "        # X: Otherwise, it's an arbitrary tone. Generate random note.\n",
    "        else:\n",
    "            elementType = 'X'\n",
    "\n",
    "        # SECOND, get the length for each element. e.g. 8th note = R8, but\n",
    "        # to simplify things you'll use the direct num, e.g. R,0.125\n",
    "        if (ix == (len(measure)-1)):\n",
    "            # formula for a in \"a - b\": start of measure (e.g. 476) + 4\n",
    "            diff = measureStartTime + 4.0 - nr.offset\n",
    "        else:\n",
    "            diff = measure[ix + 1].offset - nr.offset\n",
    "\n",
    "        # Combine into the note info.\n",
    "        noteInfo = \"%s,%.3f\" % (elementType, nr.quarterLength) # back to diff\n",
    "\n",
    "        # THIRD, get the deltas (max range up, max range down) based on where\n",
    "        # the previous note was, +- minor 3. Skip rests (don't affect deltas).\n",
    "        intervalInfo = \"\"\n",
    "        if isinstance(nr, note.Note):\n",
    "            numNonRests += 1\n",
    "            if numNonRests == 1:\n",
    "                prevNote = nr\n",
    "            else:\n",
    "                noteDist = interval.Interval(noteStart=prevNote, noteEnd=nr)\n",
    "                noteDistUpper = interval.add([noteDist, \"m3\"])\n",
    "                noteDistLower = interval.subtract([noteDist, \"m3\"])\n",
    "                intervalInfo = \",<%s,%s>\" % (noteDistUpper.directedName, \n",
    "                    noteDistLower.directedName)\n",
    "                # print \"Upper, lower: %s, %s\" % (noteDistUpper,\n",
    "                #     noteDistLower)\n",
    "                # print \"Upper, lower dnames: %s, %s\" % (\n",
    "                #     noteDistUpper.directedName,\n",
    "                #     noteDistLower.directedName)\n",
    "                # print \"The interval: %s\" % (intervalInfo)\n",
    "                prevNote = nr\n",
    "\n",
    "        # Return. Do lazy evaluation for real-time performance.\n",
    "        grammarTerm = noteInfo + intervalInfo \n",
    "        fullGrammar += (grammarTerm + \" \")\n",
    "\n",
    "    return fullGrammar.rstrip()\n",
    "\n",
    "\n",
    "def __get_abstract_grammars(measures, chords):\n",
    "    # extract grammars\n",
    "    abstract_grammars = []\n",
    "    for ix in range(1, len(measures)):\n",
    "        m = stream.Voice()\n",
    "        for i in measures[ix]:\n",
    "            m.insert(i.offset, i)\n",
    "        c = stream.Voice()\n",
    "        for j in chords[ix]:\n",
    "            c.insert(j.offset, j)\n",
    "        parsed = parse_melody(m, c)\n",
    "        abstract_grammars.append(parsed)\n",
    "\n",
    "    return abstract_grammars\n",
    "\n",
    "\n",
    "def get_musical_data(data_fn):\n",
    "    \n",
    "    measures, chords = __parse_midi(data_fn)\n",
    "    abstract_grammars = __get_abstract_grammars(measures, chords)\n",
    "\n",
    "    return chords, abstract_grammars\n",
    "\n",
    "\n",
    "def get_corpus_data(abstract_grammars):\n",
    "    corpus = [x for sublist in abstract_grammars for x in sublist.split(' ')]\n",
    "    values = set(corpus)\n",
    "    val_indices = dict((v, i) for i, v in enumerate(values))\n",
    "    indices_val = dict((i, v) for i, v in enumerate(values))\n",
    "\n",
    "    return corpus, values, val_indices, indices_val\n",
    "\n",
    "\n",
    "def data_processing(corpus, values_indices, m = 60, Tx = 30):\n",
    "    # cut the corpus into semi-redundant sequences of Tx values\n",
    "    Tx = Tx \n",
    "    N_values = len(set(corpus))\n",
    "    np.random.seed(0)\n",
    "    X = np.zeros((m, Tx, N_values), dtype=np.bool)\n",
    "    Y = np.zeros((m, Tx, N_values), dtype=np.bool)\n",
    "    for i in range(m):\n",
    "        random_idx = np.random.choice(len(corpus) - Tx)\n",
    "        corp_data = corpus[random_idx:(random_idx + Tx)]\n",
    "        for j in range(Tx):\n",
    "            idx = values_indices[corp_data[j]]\n",
    "            if j != 0:\n",
    "                X[i, j, idx] = 1\n",
    "                Y[i, j-1, idx] = 1\n",
    "    \n",
    "    Y = np.swapaxes(Y,0,1)\n",
    "    Y = Y.tolist()\n",
    "    return np.asarray(X), np.asarray(Y), N_values \n",
    "\n",
    "\n",
    "def load_music_utils():\n",
    "    chords, abstract_grammars = get_musical_data('datasets/recurrent-neural-network/original_metheny.mid')\n",
    "    corpus, tones, tones_indices, indices_tones = get_corpus_data(abstract_grammars)\n",
    "    N_tones = len(set(corpus))\n",
    "    X, Y, N_tones = data_processing(corpus, tones_indices, 60, 30)   \n",
    "    return (X, Y, N_tones, indices_tones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, n_values, indices_values = load_music_utils()\n",
    "print('shape of X:', X.shape)\n",
    "print('number of training examples:', X.shape[0])\n",
    "print('Tx (length of sequence):', X.shape[1])\n",
    "print('total # of unique values:', n_values)\n",
    "print('Shape of Y:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just loaded the following:\n",
    "\n",
    "- `X`: This is an (m, $T_x$, 78) dimensional array. We have m training examples, each of which is a snippet of $T_x =30$ musical values. At each time step, the input is one of 78 different possible values, represented as a one-hot vector. Thus for example, X[i,t,:] is a one-hot vector representating the value of the i-th example at time t. \n",
    "\n",
    "- `Y`: This is essentially the same as `X`, but shifted one step to the left (to the past). Similar to the dinosaurus assignment, we're interested in the network using the previous values to predict the next value, so our sequence model will try to predict $y^{\\langle t \\rangle}$ given $x^{\\langle 1\\rangle}, \\ldots, x^{\\langle t \\rangle}$. However, the data in `Y` is reordered to be dimension $(T_y, m, 78)$, where $T_y = T_x$. This format makes it more convenient to feed to the LSTM later. \n",
    "\n",
    "- `n_values`: The number of unique values in this dataset. This should be 78. \n",
    "\n",
    "- `indices_values`: python dictionary mapping from 0-77 to musical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of our model\n",
    "\n",
    "Here is the architecture of the model we will use. This is similar to the Dinosaurus model you had used in the previous notebook, except that in you will be implementing it in Keras. The architecture is as follows: \n",
    "\n",
    "<img src=\"images/recurrent-neural-network/music_generation.png\" style=\"width:600;height:400px;\">\n",
    "\n",
    "<!--\n",
    "<img src=\"images/djmodel.png\" style=\"width:600;height:400px;\">\n",
    "<br>\n",
    "<caption><center> **Figure 1**: LSTM model. $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a window of size $T_x$ scanned over the musical corpus. Each $x^{\\langle t \\rangle}$ is an index corresponding to a value (ex: \"A,0.250,< m2,P-4 >\") while $\\hat{y}$ is the prediction for the next value  </center></caption>\n",
    "!--> \n",
    "\n",
    "We will be training the model on random snippets of 30 values taken from a much longer piece of music. Thus, we won't bother to set the first input $x^{\\langle 1 \\rangle} = \\vec{0}$, which we had done previously to denote the start of a dinosaur name, since now most of these snippets of audio start somewhere in the middle of a piece of music. We are setting each of the snippts to have the same length $T_x = 30$ to make vectorization easier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building the model\n",
    "\n",
    "In this part you will build and train a model that will learn musical patterns. To do so, you will need to build a model that takes in X of shape $(m, T_x, 78)$ and Y of shape $(T_y, m, 78)$. We will use an LSTM with 64 dimensional hidden states. Lets set `n_a = 64`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can create a Keras model with multiple inputs and outputs. If you're building an RNN where even at test time entire input sequence $x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\ldots, x^{\\langle T_x \\rangle}$ were *given in advance*, for example if the inputs were words and the output was a label, then Keras has simple built-in functions to build the model. However, for sequence generation, at test time we don't know all the values of $x^{\\langle t\\rangle}$ in advance; instead we generate them one at a time using $x^{\\langle t\\rangle} = y^{\\langle t-1 \\rangle}$. So the code will be a bit more complicated, and you'll need to implement your own for-loop to iterate over the different time steps. \n",
    "\n",
    "The function `djmodel()` will call the LSTM layer $T_x$ times using a for-loop, and it is important that all $T_x$ copies have the same weights. I.e., it should not re-initiaiize the weights every time---the $T_x$ steps should have shared weights. The key steps for implementing layers with shareable weights in Keras are: \n",
    "1. Define the layer objects (we will use global variables for this).\n",
    "2. Call these objects when propagating the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapor = Reshape((1, 78))                        # Used in Step 2.B of djmodel(), below\n",
    "LSTM_cell = LSTM(n_a, return_state = True)         # Used in Step 2.C\n",
    "densor = Dense(n_values, activation='softmax')     # Used in Step 2.D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of `reshapor`, `LSTM_cell` and `densor` are now layer objects, and you can use them to implement `djmodel()`. In order to propagate a Keras tensor object X through one of these layers, use `layer_object(X)` (or `layer_object([X,Y])` if it requires multiple inputs.). For example, `reshapor(X)` will propagate X through the `Reshape((1,78))` layer defined above.\n",
    "\n",
    "**Exercise**: Implement `djmodel()`. You will need to carry out 2 steps:\n",
    "\n",
    "1. Create an empty list \"outputs\" to save the outputs of the LSTM Cell at every time step.\n",
    "2. Loop for $t \\in 1, \\ldots, T_x$:\n",
    "\n",
    "    A. Select the \"t\"th time-step vector from X. The shape of this selection should be (78,). To do so, create a custom [Lambda](https://keras.io/layers/core/#lambda) layer in Keras by using this line of code:\n",
    "```    \n",
    "           x = Lambda(lambda x: x[:,t,:])(X)\n",
    "``` \n",
    "Look over the Keras documentation to figure out what this does. It is creating a \"temporary\" or \"unnamed\" function (that's what Lambda functions are) that extracts out the appropriate one-hot vector, and making this function a Keras `Layer` object to apply to `X`. \n",
    "\n",
    "    B. Reshape x to be (1,78). You may find the `reshapor()` layer (defined below) helpful.\n",
    "\n",
    "    C. Run x through one step of LSTM_cell. Remember to initialize the LSTM_cell with the previous step's hidden state $a$ and cell state $c$. Use the following formatting:\n",
    "```python\n",
    "a, _, c = LSTM_cell(input_x, initial_state=[previous hidden state, previous cell state])\n",
    "```\n",
    "\n",
    "    D. Propagate the LSTM's output activation value through a dense+softmax layer using `densor`. \n",
    "    \n",
    "    E. Append the predicted value to the list of \"outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def djmodel(Tx, n_a, n_values):\n",
    "    \"\"\"\n",
    "    Implement the model\n",
    "    \n",
    "    Arguments:\n",
    "    Tx -- length of the sequence in a corpus\n",
    "    n_a -- the number of activations used in our model\n",
    "    n_values -- number of unique values in the music data \n",
    "    \n",
    "    Returns:\n",
    "    model -- a keras model with the \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    X = Input(shape=(Tx, n_values))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    \n",
    "    # Step 1: Create empty list to append the outputs while you iterate (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop\n",
    "    for t in range(Tx):\n",
    "        \n",
    "        # Step 2.A: select the \"t\"th time step vector from X. \n",
    "        x = Lambda(lambda x: x[:,t,:])(X)\n",
    "        # Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)\n",
    "        x = reshapor(x)\n",
    "        # Step 2.C: Perform one step of the LSTM_cell\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a, c])\n",
    "        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell\n",
    "        out =  densor(a)\n",
    "        # Step 2.E: add the output to \"outputs\"\n",
    "        outputs.append(out)\n",
    "        \n",
    "    # Step 3: Create model instance\n",
    "    model = Model(inputs=[X, a0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to define your model. We will use `Tx=30`, `n_a=64` (the dimension of the LSTM activations), and `n_values=78`. This cell may take a few seconds to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = djmodel(Tx = 30 , n_a = 64, n_values = 78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now need to compile your model to be trained. We will Adam and a categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets initialize `a0` and `c0` for the LSTM's initial state to be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 60\n",
    "a0 = np.zeros((m, n_a))\n",
    "c0 = np.zeros((m, n_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now fit the model! We will turn `Y` to a list before doing so, since the cost function expects `Y` to be provided in this format (one list item per time-step). So `list(Y)` is a list with 30 items, where each of the list items is of shape (60,78). Lets train for 100 epochs. This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X, a0, c0], list(Y), epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Generating music\n",
    "\n",
    "You now have a trained model which has learned the patterns of the jazz soloist. Lets now use this model to synthesize new music. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Predicting & Sampling\n",
    "\n",
    "<img src=\"images/recurrent-neural-network/music_gen.png\" style=\"width:600;height:400px;\">\n",
    "\n",
    "At each step of sampling, you will take as input the activation `a` and cell state `c` from the previous state of the LSTM, forward propagate by one step, and get a new output activation as well as cell state. The new activation `a` can then be used to generate the output, using `densor` as before. \n",
    "\n",
    "To start off the model, we will initialize `x0` as well as the LSTM activation and and cell value `a0` and `c0` to be zeros. \n",
    "\n",
    "\n",
    "<!-- \n",
    "You are about to build a function that will do this inference for you. Your function takes in your previous model and the number of time steps `Ty` that you want to sample. It will return a keras model that would be able to generate sequences for you. Furthermore, the function takes in a dense layer of `78` units and the number of activations. \n",
    "!--> \n",
    "\n",
    "\n",
    "**Exercise:** Implement the function below to sample a sequence of musical values. Here are some of the key steps you'll need to implement inside the for-loop that generates the $T_y$ output characters: \n",
    "\n",
    "Step 2.A: Use `LSTM_Cell`, which inputs the previous step's `c` and `a` to generate the current step's `c` and `a`. \n",
    "\n",
    "Step 2.B: Use `densor` (defined previously) to compute a softmax on `a` to get the output for the current step. \n",
    "\n",
    "Step 2.C: Save the output you have just generated by appending it to `outputs`.\n",
    "\n",
    "Step 2.D: Sample x to the be \"out\"'s one-hot version (the prediction) so that you can pass it to the next LSTM's step.  We have already provided this line of code, which uses a [Lambda](https://keras.io/layers/core/#lambda) function. \n",
    "```python\n",
    "x = Lambda(one_hot)(out) \n",
    "```\n",
    "[Minor technical note: Rather than sampling a value at random according to the probabilities in `out`, this line of code actually chooses the single most likely note at each step using an argmax.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    x = K.argmax(x)\n",
    "    x = tf.one_hot(x, 78) \n",
    "    x = RepeatVector(1)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 100):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    n_values -- integer, umber of unique values\n",
    "    n_a -- number of units in the LSTM_cell\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        \n",
    "        # Step 2.A: Perform one step of LSTM_cell (≈1 line)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a, c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "        out = densor(a)\n",
    "\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, 78) (≈1 line)\n",
    "        outputs.append(out)\n",
    "        \n",
    "        # Step 2.D: Select the next value according to \"out\", and set \"x\" to be the one-hot representation of the\n",
    "        #           selected value, which will be passed as the input to LSTM_cell on the next step. We have provided \n",
    "        #           the line of code you need to do this. \n",
    "        x = Lambda(one_hot)(out)\n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "    inference_model = Model(inputs=[x0, a0, c0], outputs=outputs)\n",
    "    \n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define your inference model. This model is hard coded to generate 50 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this creates the zero-valued vectors you will use to initialize `x` and the LSTM state variables `a` and `c`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_initializer = np.zeros((1, 1, 78))\n",
    "a_initializer = np.zeros((1, n_a))\n",
    "c_initializer = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement `predict_and_sample()`. This function takes many arguments including the inputs [x_initializer, a_initializer, c_initializer]. In order to predict the output corresponding to this input, you will need to carry-out 3 steps:\n",
    "1. Use your inference model to predict an output given your set of inputs. The output `pred` should be a list of length $T_y$ where each element is a numpy-array of shape (1, n_values).\n",
    "2. Convert `pred` into a numpy array of $T_y$ indices. Each index corresponds is computed by taking the `argmax` of an element of the `pred` list. [Hint](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html).\n",
    "3. Convert the indices into their one-hot vector representations. [Hint](https://keras.io/utils/#to_categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, \n",
    "                       c_initializer = c_initializer):\n",
    "    \"\"\"\n",
    "    Predicts the next value of values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated\n",
    "    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])\n",
    "    # Step 2: Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "    indices = np.argmax(pred, axis=-1)\n",
    "    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)\n",
    "    results = to_categorical(indices)\n",
    "    \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)\n",
    "print(\"np.argmax(results[12]) =\", np.argmax(results[12]))\n",
    "print(\"np.argmax(results[17]) =\", np.argmax(results[17]))\n",
    "print(\"list(indices[12:18]) =\", list(indices[12:18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Generate music \n",
    "\n",
    "Finally, you are ready to generate music. Your RNN generates a sequence of values. The following code generates music by first calling your `predict_and_sample()` function. These values are then post-processed into musical chords (meaning that multiple values or notes can be played at the same time). \n",
    "\n",
    "Most computational music algorithms use some post-processing because it is difficult to generate music that sounds good without such post-processing. The post-processing does things such as clean up the generated audio by making sure the same sound is not repeated too many times, that two successive notes are not too far from each other in pitch, and so on. One could argue that a lot of these post-processing steps are hacks; also, a lot the music generation literature has also focused on hand-crafting post-processors, and a lot of the output quality depends on the quality of the post-processing and not just the quality of the RNN. But this post-processing does make a huge difference, so lets use it in our implementation as well. \n",
    "\n",
    "Lets make some music! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to generate music and record it into your `out_stream`. This can take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(inference_model, corpus = corpus, abstract_grammars = abstract_grammars, tones = tones, tones_indices = tones_indices, indices_tones = indices_tones, T_y = 10, max_tries = 1000, diversity = 0.5):\n",
    "    \"\"\"\n",
    "    Generates music using a model trained to learn musical patterns of a jazz soloist. Creates an audio stream\n",
    "    to save the music and play it.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- Keras model Instance, output of djmodel()\n",
    "    corpus -- musical corpus, list of 193 tones as strings (ex: 'C,0.333,<P1,d-5>')\n",
    "    abstract_grammars -- list of grammars, on element can be: 'S,0.250,<m2,P-4> C,0.250,<P4,m-2> A,0.250,<P4,m-2>'\n",
    "    tones -- set of unique tones, ex: 'A,0.250,<M2,d-4>' is one element of the set.\n",
    "    tones_indices -- a python dictionary mapping unique tone (ex: A,0.250,< m2,P-4 >) into their corresponding indices (0-77)\n",
    "    indices_tones -- a python dictionary mapping indices (0-77) into their corresponding unique tone (ex: A,0.250,< m2,P-4 >)\n",
    "    Tx -- integer, number of time-steps used at training time\n",
    "    temperature -- scalar value, defines how conservative/creative the model is when generating music\n",
    "    \n",
    "    Returns:\n",
    "    predicted_tones -- python list containing predicted tones\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up audio stream\n",
    "    out_stream = stream.Stream()\n",
    "    \n",
    "    # Initialize chord variables\n",
    "    curr_offset = 0.0                                     # variable used to write sounds to the Stream.\n",
    "    num_chords = int(len(chords) / 3)                     # number of different set of chords\n",
    "    \n",
    "    print(\"Predicting new values for different set of chords.\")\n",
    "    # Loop over all 18 set of chords. At each iteration generate a sequence of tones\n",
    "    # and use the current chords to convert it into actual sounds \n",
    "    for i in range(1, num_chords):\n",
    "        \n",
    "        # Retrieve current chord from stream\n",
    "        curr_chords = stream.Voice()\n",
    "        \n",
    "        # Loop over the chords of the current set of chords\n",
    "        for j in chords[i]:\n",
    "            # Add chord to the current chords with the adequate offset, no need to understand this\n",
    "            curr_chords.insert((j.offset % 4), j)\n",
    "        \n",
    "        # Generate a sequence of tones using the model\n",
    "        _, indices = predict_and_sample(inference_model)\n",
    "        indices = list(indices.squeeze())\n",
    "        pred = [indices_tones[p] for p in indices]\n",
    "        \n",
    "        predicted_tones = 'C,0.25 '\n",
    "        for k in range(len(pred) - 1):\n",
    "            predicted_tones += pred[k] + ' ' \n",
    "        \n",
    "        predicted_tones +=  pred[-1]\n",
    "                \n",
    "        #### POST PROCESSING OF THE PREDICTED TONES ####\n",
    "        # We will consider \"A\" and \"X\" as \"C\" tones. It is a common choice.\n",
    "        predicted_tones = predicted_tones.replace(' A',' C').replace(' X',' C')\n",
    "\n",
    "        # Pruning #1: smoothing measure\n",
    "        predicted_tones = prune_grammar(predicted_tones)\n",
    "        \n",
    "        # Use predicted tones and current chords to generate sounds\n",
    "        sounds = unparse_grammar(predicted_tones, curr_chords)\n",
    "\n",
    "        # Pruning #2: removing repeated and too close together sounds\n",
    "        sounds = prune_notes(sounds)\n",
    "\n",
    "        # Quality assurance: clean up sounds\n",
    "        sounds = clean_up_notes(sounds)\n",
    "\n",
    "        # Print number of tones/notes in sounds\n",
    "        print('Generated %s sounds using the predicted values for the set of chords (\"%s\") and after pruning' % (len([k for k in sounds if isinstance(k, note.Note)]), i))\n",
    "        \n",
    "        # Insert sounds into the output stream\n",
    "        for m in sounds:\n",
    "            out_stream.insert(curr_offset + m.offset, m)\n",
    "        for mc in curr_chords:\n",
    "            out_stream.insert(curr_offset + mc.offset, mc)\n",
    "\n",
    "        curr_offset += 4.0\n",
    "        \n",
    "    # Initialize tempo of the output stream with 130 bit per minute\n",
    "    out_stream.insert(0.0, tempo.MetronomeMark(number=130))\n",
    "\n",
    "    # Save audio stream to fine\n",
    "    mf = midi.translate.streamToMidiFile(out_stream)\n",
    "    mf.open(\"output/my_music.midi\", 'wb')\n",
    "    mf.write()\n",
    "    print(\"Your generated music is saved in output/my_music.midi\")\n",
    "    mf.close()\n",
    "    \n",
    "    # Play the final stream through output (see 'play' lambda function above)\n",
    "    # play = lambda x: midi.realtime.StreamPlayer(x).play()\n",
    "    # play(out_stream)\n",
    "    \n",
    "    return out_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_stream = generate_music(inference_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
