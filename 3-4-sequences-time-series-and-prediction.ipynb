{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequences, Time Series and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None, label=None):\n",
    "    plt.plot(time[start:end], series[start:end], format, label=label)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    if label:\n",
    "        plt.legend(fontsize=14)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    \n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "\n",
    "# Create a time series that just trends upward\n",
    "\n",
    "time = np.arange(4 * 365 + 1)\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Generate a time series with a seasonal pattern\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "series = seasonality(time, period=365, amplitude=amplitude)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a time series with both trend and seasonality\n",
    "\n",
    "slope = 0.05\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Noise\n",
    "\n",
    "def white_noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "noise_level = 5\n",
    "noise = white_noise(time, noise_level, seed=42)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, noise)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Add this white noise to the time series\n",
    "\n",
    "series += noise\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Forecast\n",
    "\n",
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "def autocorrelation(time, amplitude, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    φ1 = 0.5\n",
    "    φ2 = -0.1\n",
    "    ar = rnd.randn(len(time) + 50)\n",
    "    ar[:50] = 100\n",
    "    for step in range(50, len(time) + 50):\n",
    "        ar[step] += φ1 * ar[step - 50]\n",
    "        ar[step] += φ2 * ar[step - 33]\n",
    "    return ar[50:] * amplitude\n",
    "\n",
    "def autocorrelation(time, amplitude, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    φ = 0.8\n",
    "    ar = rnd.randn(len(time) + 1)\n",
    "    for step in range(1, len(time) + 1):\n",
    "        ar[step] += φ * ar[step - 1]\n",
    "    return ar[1:] * amplitude\n",
    "\n",
    "series = autocorrelation(time, 10, seed=42)\n",
    "plot_series(time[:200], series[:200])\n",
    "plt.show()\n",
    "\n",
    "series = autocorrelation(time, 10, seed=42) + trend(time, 2)\n",
    "plot_series(time[:200], series[:200])\n",
    "plt.show()\n",
    "\n",
    "series = autocorrelation(time, 10, seed=42) + seasonality(time, period=50, amplitude=150) + trend(time, 2)\n",
    "plot_series(time[:200], series[:200])\n",
    "plt.show()\n",
    "\n",
    "series = autocorrelation(time, 10, seed=42) + seasonality(time, period=50, amplitude=150) + trend(time, 2)\n",
    "series2 = autocorrelation(time, 5, seed=42) + seasonality(time, period=50, amplitude=2) + trend(time, -1) + 550\n",
    "series[200:] = series2[200:]\n",
    "#series += noise(time, 30)\n",
    "plot_series(time[:300], series[:300])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def impulses(time, num_impulses, amplitude=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    impulse_indices = rnd.randint(len(time), size=10)\n",
    "    series = np.zeros(len(time))\n",
    "    for index in impulse_indices:\n",
    "        series[index] += rnd.rand() * amplitude\n",
    "    return series    \n",
    "\n",
    "series = impulses(time, 10, seed=42)\n",
    "plot_series(time, series)\n",
    "plt.show()\n",
    "\n",
    "def autocorrelation(source, φs):\n",
    "    ar = source.copy()\n",
    "    max_lag = len(φs)\n",
    "    for step, value in enumerate(source):\n",
    "        for lag, φ in φs.items():\n",
    "            if step - lag > 0:\n",
    "              ar[step] += φ * ar[step - lag]\n",
    "    return ar\n",
    "\n",
    "signal = impulses(time, 10, seed=42)\n",
    "series = autocorrelation(signal, {1: 0.99})\n",
    "plot_series(time, series)\n",
    "plt.plot(time, signal, \"k-\")\n",
    "plt.show()\n",
    "\n",
    "signal = impulses(time, 10, seed=42)\n",
    "series = autocorrelation(signal, {1: 0.70, 50: 0.2})\n",
    "plot_series(time, series)\n",
    "plt.plot(time, signal, \"k-\")\n",
    "plt.show()\n",
    "\n",
    "series_diff1 = series[1:] - series[:-1]\n",
    "plot_series(time[1:], series_diff1)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(series)\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "model = ARIMA(series, order=(5, 1, 0))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"sunspots.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "series = df[\"Monthly Mean Total Sunspot Number\"].asfreq(\"1M\")\n",
    "series.head()\n",
    "\n",
    "series.plot(figsize=(12, 5))\n",
    "\n",
    "series[\"1995-01-01\":].plot()\n",
    "\n",
    "series.diff(1).plot()\n",
    "plt.axis([0, 100, -50, 50])\n",
    "\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(series)\n",
    "\n",
    "autocorrelation_plot(series.diff(1)[1:])\n",
    "\n",
    "autocorrelation_plot(series.diff(1)[1:].diff(11 * 12)[11*12+1:])\n",
    "plt.axis([0, 500, -0.1, 0.1])\n",
    "\n",
    "autocorrelation_plot(series.diff(1)[1:])\n",
    "plt.axis([0, 50, -0.1, 0.1])\n",
    "\n",
    "# 116.7 - 104.3\n",
    "\n",
    "[series.autocorr(lag) for lag in range(1, 50)]\n",
    "\n",
    "pd.read_csv(filepath_or_buffer, sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)\n",
    "# Read a comma-separated values (csv) file into DataFrame.\n",
    "\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "series_diff = series\n",
    "for lag in range(50):\n",
    "  series_diff = series_diff[1:] - series_diff[:-1]\n",
    "\n",
    "autocorrelation_plot(series_diff)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "series_diff1 = pd.Series(series[1:] - series[:-1])\n",
    "autocorrs = [series_diff1.autocorr(lag) for lag in range(1, 60)]\n",
    "plt.plot(autocorrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the time series with seasonality, trend and a bit of noise\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)  \n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.05\n",
    "noise_level = 5\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Split it so we can start forecasting\n",
    "\n",
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_train, x_train)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Naive forecast\n",
    "\n",
    "naive_forecast = series[split_time - 1:-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, naive_forecast)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid, start=0, end=150)\n",
    "plot_series(time_valid, naive_forecast, start=1, end=151)\n",
    "\n",
    "\n",
    "# Compute the mean squared error and the mean absolute error between the forecasts and the predictions in the validation period\n",
    "\n",
    "print(keras.metrics.mean_squared_error(x_valid, naive_forecast).numpy())\n",
    "print(keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy())\n",
    "\n",
    "\n",
    "# Try a moving average\n",
    "\n",
    "def moving_average_forecast(series, window_size):\n",
    "  \"\"\"Forecasts the mean of the last few values.\n",
    "     If window_size=1, then this is equivalent to naive forecast\"\"\"\n",
    "  forecast = []\n",
    "  for time in range(len(series) - window_size):\n",
    "    forecast.append(series[time:time + window_size].mean())\n",
    "  return np.array(forecast)\n",
    "\n",
    "moving_avg = moving_average_forecast(series, 30)[split_time - 30:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, moving_avg)\n",
    "\n",
    "print(keras.metrics.mean_squared_error(x_valid, moving_avg).numpy())\n",
    "print(keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy())\n",
    "\n",
    "\n",
    "# The moving average does not anticipate trend or seasonality, so let's try to remove them by using differencing\n",
    "\n",
    "diff_series = (series[365:] - series[:-365])\n",
    "diff_time = time[365:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(diff_time, diff_series)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Trend and seasonality seem to be gone, so now we can use the moving average\n",
    "\n",
    "diff_moving_avg = moving_average_forecast(diff_series, 50)[split_time - 365 - 50:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, diff_series[split_time - 365:])\n",
    "plot_series(time_valid, diff_moving_avg)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Bring back the trend and seasonality by adding the past values from t – 365\n",
    "\n",
    "diff_moving_avg_plus_past = series[split_time - 365:-365] + diff_moving_avg\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, diff_moving_avg_plus_past)\n",
    "plt.show()\n",
    "\n",
    "print(keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_past).numpy())\n",
    "print(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_past).numpy())\n",
    "\n",
    "\n",
    "# Use a moving averaging on past values to remove some of the noise\n",
    "\n",
    "diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - 370:-360], 10) + diff_moving_avg\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, diff_moving_avg_plus_smooth_past)\n",
    "plt.show()\n",
    "\n",
    "print(keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())\n",
    "print(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural networks for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "for val in dataset:\n",
    "   print(val.numpy())\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1)\n",
    "for window_dataset in dataset:\n",
    "  for val in window_dataset:\n",
    "    print(val.numpy(), end=\" \")\n",
    "  print()\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "for window_dataset in dataset:\n",
    "  for val in window_dataset:\n",
    "    print(val.numpy(), end=\" \")\n",
    "  print()\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "for window in dataset:\n",
    "  print(window.numpy())\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "for x,y in dataset:\n",
    "  print(x.numpy(), y.numpy())\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "for x,y in dataset:\n",
    "  print(x.numpy(), y.numpy())\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "dataset = dataset.batch(2).prefetch(1)\n",
    "for x,y in dataset:\n",
    "  print(\"x = \", x.numpy())\n",
    "  print(\"y = \", y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)  \n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.05\n",
    "noise_level = 5\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "print(dataset)\n",
    "l0 = tf.keras.layers.Dense(1, input_shape=[window_size])\n",
    "model = tf.keras.models.Sequential([l0])\n",
    "\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
    "model.fit(dataset,epochs=100,verbose=0)\n",
    "\n",
    "print(\"Layer weights {}\".format(l0.get_weights()))\n",
    "\n",
    "\n",
    "forecast = []\n",
    "\n",
    "for time in range(len(series) - window_size):\n",
    "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "forecast = forecast[split_time-window_size:]\n",
    "results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, results)\n",
    "\n",
    "\n",
    "tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)  \n",
    "baseline = 10\n",
    "amplitude = 20\n",
    "slope = 0.09\n",
    "noise_level = 5\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid)\n",
    "\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=[window_size], activation=\"relu\"), \n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"), \n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
    "model.fit(dataset,epochs=100,verbose=0)\n",
    "\n",
    "\n",
    "forecast = []\n",
    "for time in range(len(series) - window_size):\n",
    "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "forecast = forecast[split_time-window_size:]\n",
    "results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, results)\n",
    "\n",
    "\n",
    "tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()\n",
    "\n",
    "\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=[window_size], activation=\"relu\"), \n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"), \n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "history = model.fit(dataset, epochs=100, callbacks=[lr_schedule], verbose=0)\n",
    "\n",
    "\n",
    "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
    "plt.semilogx(lrs, history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1e-3, 0, 300])\n",
    "\n",
    "\n",
    "window_size = 30\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=\"relu\", input_shape=[window_size]),\n",
    "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=8e-6, momentum=0.9)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "history = model.fit(dataset, epochs=500, verbose=0)\n",
    "\n",
    "\n",
    "loss = history.history['loss']\n",
    "epochs = range(len(loss))\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot all but the first 10\n",
    "loss = history.history['loss']\n",
    "epochs = range(10, len(loss))\n",
    "plot_loss = loss[10:]\n",
    "print(plot_loss)\n",
    "plt.plot(epochs, plot_loss, 'b', label='Training Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "forecast = []\n",
    "for time in range(len(series) - window_size):\n",
    "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "forecast = forecast[split_time-window_size:]\n",
    "results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, results)\n",
    "\n",
    "\n",
    "tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)  \n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.05\n",
    "noise_level = 5\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "train_set = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "                      input_shape=[None]),\n",
    "  tf.keras.layers.SimpleRNN(40, return_sequences=True),\n",
    "  tf.keras.layers.SimpleRNN(40),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
    "])\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])\n",
    "\n",
    "\n",
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1e-4, 0, 30])\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "                      input_shape=[None]),\n",
    "  tf.keras.layers.SimpleRNN(40, return_sequences=True),\n",
    "  tf.keras.layers.SimpleRNN(40),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=5e-5, momentum=0.9)\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "history = model.fit(dataset,epochs=400)\n",
    "\n",
    "\n",
    "forecast=[]\n",
    "for time in range(len(series) - window_size):\n",
    "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "forecast = forecast[split_time-window_size:]\n",
    "results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, results)\n",
    "\n",
    "\n",
    "tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()\n",
    "\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "mae=history.history['mae']\n",
    "loss=history.history['loss']\n",
    "\n",
    "epochs=range(len(loss)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot MAE and Loss\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, mae, 'r')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "plt.title('MAE and Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"MAE\", \"Loss\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "epochs_zoom = epochs[200:]\n",
    "mae_zoom = mae[200:]\n",
    "loss_zoom = loss[200:]\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot Zoomed MAE and Loss\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs_zoom, mae_zoom, 'r')\n",
    "plt.plot(epochs_zoom, loss_zoom, 'b')\n",
    "plt.title('MAE and Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"MAE\", \"Loss\"])\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)  \n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.05\n",
    "noise_level = 5\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "                      input_shape=[None]),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
    "])\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])\n",
    "\n",
    "\n",
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1e-4, 0, 30])\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "                      input_shape=[None]),\n",
    "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9),metrics=[\"mae\"])\n",
    "history = model.fit(dataset,epochs=500,verbose=0)\n",
    "\n",
    "\n",
    "forecast = []\n",
    "results = []\n",
    "for time in range(len(series) - window_size):\n",
    "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "forecast = forecast[split_time-window_size:]\n",
    "results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, results)\n",
    "\n",
    "\n",
    "tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()\n",
    "\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "mae=history.history['mae']\n",
    "loss=history.history['loss']\n",
    "\n",
    "epochs=range(len(loss)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot MAE and Loss\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, mae, 'r')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "plt.title('MAE and Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"MAE\", \"Loss\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "epochs_zoom = epochs[200:]\n",
    "mae_zoom = mae[200:]\n",
    "loss_zoom = loss[200:]\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot Zoomed MAE and Loss\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs_zoom, mae_zoom, 'r')\n",
    "plt.plot(epochs_zoom, loss_zoom, 'b')\n",
    "plt.title('MAE and Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"MAE\", \"Loss\"])\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)  \n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.05\n",
    "noise_level = 5\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "def model_forecast(model, series, window_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(32).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "window_size = 30\n",
    "train_set = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n",
    "                      strides=1, padding=\"causal\",\n",
    "                      activation=\"relu\",\n",
    "                      input_shape=[None, 1]),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Lambda(lambda x: x * 200)\n",
    "])\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])\n",
    "\n",
    "\n",
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1e-4, 0, 30])\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "#batch_size = 16\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n",
    "                      strides=1, padding=\"causal\",\n",
    "                      activation=\"relu\",\n",
    "                      input_shape=[None, 1]),\n",
    "  tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Lambda(lambda x: x * 200)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "history = model.fit(dataset,epochs=500)\n",
    "\n",
    "\n",
    "rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
    "rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, rnn_forecast)\n",
    "\n",
    "\n",
    "tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
    "\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "mae=history.history['mae']\n",
    "loss=history.history['loss']\n",
    "\n",
    "epochs=range(len(loss)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot MAE and Loss\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, mae, 'r')\n",
    "plt.plot(epochs, loss, 'b')\n",
    "plt.title('MAE and Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"MAE\", \"Loss\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "epochs_zoom = epochs[200:]\n",
    "mae_zoom = mae[200:]\n",
    "loss_zoom = loss[200:]\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot Zoomed MAE and Loss\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs_zoom, mae_zoom, 'r')\n",
    "plt.plot(epochs_zoom, loss_zoom, 'b')\n",
    "plt.title('MAE and Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"MAE\", \"Loss\"])\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv and store it in tmp/sunspots.csv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    \n",
    "import csv\n",
    "time_step = []\n",
    "sunspots = []\n",
    "\n",
    "with open('tmp/sunspots.csv') as csvfile:\n",
    "  reader = csv.reader(csvfile, delimiter=',')\n",
    "  next(reader)\n",
    "  for row in reader:\n",
    "    sunspots.append(float(row[2]))\n",
    "    time_step.append(int(row[0]))\n",
    "\n",
    "series = np.array(sunspots)\n",
    "time = np.array(time_step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "\n",
    "\n",
    "series = np.array(sunspots)\n",
    "time = np.array(time_step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time, series)\n",
    "\n",
    "\n",
    "split_time = 3000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "window_size = 30\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)\n",
    "\n",
    "def model_forecast(model, series, window_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(32).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "window_size = 64\n",
    "batch_size = 256\n",
    "train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
    "print(train_set)\n",
    "print(x_train.shape)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n",
    "                      strides=1, padding=\"causal\",\n",
    "                      activation=\"relu\",\n",
    "                      input_shape=[None, 1]),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "  tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Lambda(lambda x: x * 400)\n",
    "])\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])\n",
    "\n",
    "\n",
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1e-4, 0, 60])\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "train_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
    "                      strides=1, padding=\"causal\",\n",
    "                      activation=\"relu\",\n",
    "                      input_shape=[None, 1]),\n",
    "  tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "  tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Lambda(lambda x: x * 400)\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "history = model.fit(train_set,epochs=500)\n",
    "\n",
    "\n",
    "rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
    "rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, rnn_forecast)\n",
    "\n",
    "\n",
    "tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
    "\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "loss=history.history['loss']\n",
    "\n",
    "epochs=range(len(loss)) # Get number of epochs\n",
    "\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "\n",
    "zoomed_loss = loss[200:]\n",
    "zoomed_epochs = range(200,500)\n",
    "\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(zoomed_epochs, zoomed_loss, 'r')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\"])\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
