{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "## Dot product\n",
    "\n",
    "- $cos(\\theta) = \\dfrac{x^{T}y}{\\|x\\|\\|y\\|} = \\dfrac{<x,y>}{\\|x\\|\\|y\\|}$\n",
    "\n",
    "## Inner product\n",
    "\n",
    "- $x,y \\in V$\n",
    "- $<,>: V x V -> R$\n",
    "\n",
    "### Symmetric\n",
    "\n",
    "- $<x,y> = <y,x>$\n",
    "\n",
    "### Positive definite\n",
    "\n",
    "- $<x,x> \\ge 0$ and $<x,x> = 0$ iff $x = 0$\n",
    "\n",
    "### Bilinear\n",
    "\n",
    "- $x,y,z \\in V, \\lambda \\in R$\n",
    "- $<\\lambda x + z, y> = \\lambda<x,y> + <z,y>$\n",
    "- $<x, \\lambda y + z> = \\lambda<x,y> + <x,z>$\n",
    "\n",
    "## Length / Norm of a vector\n",
    "\n",
    "- $\\|x\\| = \\sqrt{<x,x>}$\n",
    "\n",
    "## Triangle inequality\n",
    "\n",
    "- $\\|x+y\\| \\le \\|x\\| + \\|y\\|$\n",
    "\n",
    "## Distance\n",
    "\n",
    "- $d(x,y) = \\|x-y\\| = \\sqrt{<x-y,x-y>}$\n",
    "\n",
    "## Projection onto 1D subspace\n",
    "\n",
    "Using two conditions\n",
    "- $\\Pi_{u}(x) \\in u => \\exists \\lambda \\in R: \\Pi_{u}(x) = \\lambda b$\n",
    "- $<b, \\Pi_{u}(x)-x>$ = 0\n",
    "\n",
    "Then\n",
    "- $\\Pi_{u}(x) = \\dfrac{bb^{T}}{\\|b\\|^{2}}x$\n",
    "\n",
    "## Projection onto higher dimensional subspace\n",
    "\n",
    "- $\\lambda = \\begin{pmatrix} \\lambda_{1} \\\\ \\vdots \\\\ \\lambda_{M}  \\end{pmatrix} (M \\times 1)$ \n",
    "- $B = \\begin{pmatrix} B_{1} | \\dots | B_{M} \\end{pmatrix} (D \\times M)$\n",
    "\n",
    "Then\n",
    "- $\\Pi_{u}(x) = B(B^{T}B)^{-1}B^{T}X$\n",
    "\n",
    "## PCA\n",
    "\n",
    "- Given $X = \\{x_{1} \\dots x_{n}\\}$ where $x_{i} \\in {\\rm I\\!R}^{D}$\n",
    "\n",
    "We have\n",
    "- $x_{n} = \\displaystyle\\sum_{i=1}^{D}B_{in}b_{i}, B_{in} = x_{n}^{T}b_{i}, B = (b_{1} \\dots b_{n})$\n",
    "- $\\tilde{x} = BB^{T}X$\n",
    "\n",
    "From above\n",
    "- $x_{n} = \\displaystyle\\sum_{i=1}^{M}B_{in}b_{i} + \\displaystyle\\sum_{i=M+1}^{D}B_{in}b_{i}$ (PCA ingores the second term. Also $b_{1} \\dots b_{M}$ span the principal subspace)\n",
    "\n",
    "Loss function is given by\n",
    "- $J = \\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\|x_{n}-\\tilde{x_{n}}\\|^{2}$\n",
    "\n",
    "Then, derivates are\n",
    "- $\\dfrac{\\partial J}{\\partial \\{B_{in}b_{i}\\}} = \\dfrac{\\partial J}{\\partial \\tilde{x_{n}}}\\dfrac{\\partial \\tilde{x_{n}}}{\\partial \\{B_{in}b_{i}\\}} = -\\dfrac{2}{N}(x_{n}-\\tilde{x_{n}})^{T}\\dfrac{\\partial \\tilde{x_{n}}}{\\partial \\{B_{in}b_{i}\\}}$\n",
    "- $\\dfrac{\\partial \\tilde{x_{n}}}{\\partial B_{in}} = b_{i}$\n",
    "- $\\dfrac{\\partial J}{\\partial B_{in}} = \\dfrac{\\partial J}{\\partial \\tilde{x_{n}}}\\dfrac{\\partial \\tilde{x_{n}}}{\\partial B_{in}} = -\\dfrac{2}{N}(x_{n}-\\tilde{x_{n}})^{T}b_{i} = -\\dfrac{2}{N}(x_{n}-\\displaystyle\\sum_{j=1}^{M}B_{jn}b_{j})^{T}b_{i} = -\\dfrac{2}{N}(x_{n}^{T}b_{i}-B_{in}b_{i}^{T}b_{i}) = -\\dfrac{2}{N}(x_{n}^{T}b_{i}-B_{in}) = 0$\n",
    "\n",
    "Thus\n",
    "- $B_{in} = x_{n}^{T}b_{i}$\n",
    "\n",
    "Now\n",
    "- $\\tilde{x_{n}} = \\displaystyle\\sum_{j=1}^{M}B_{jn}b_{j} = \\displaystyle\\sum_{j=1}^{M}(x_{n}^{T}b_{j})b_{j} = \\displaystyle\\sum_{j=1}^{M}b_{j}(b_{j}^{T}x_{n}) = \\displaystyle\\sum_{j=1}^{M}(b_{j}b_{j}^{T})x_{n}$\n",
    "\n",
    "Note\n",
    "- $x_{n} = \\left(\\displaystyle\\sum_{j=1}^{M}b_{j}b_{j}^{T}\\right)x_{n} + \\left(\\displaystyle\\sum_{j=M+1}^{D}b_{j}b_{j}^{T}\\right)x_{n}$\n",
    "\n",
    "Then\n",
    "- $x_{n} - \\tilde{x_{n}} = \\left(\\displaystyle\\sum_{j=M+1}^{D}b_{j}b_{j}^{T}\\right)x_{n} = \\displaystyle\\sum_{j=M+1}^{D}(b_{j}^{T}x_{n})b_{j}$\n",
    "\n",
    "Loss function becomes\n",
    "- $J = \\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\|x_{n}-\\tilde{x_{n}}\\|^{2} = \\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\|\\displaystyle\\sum_{j=M+1}^{D}(b_{j}^{T}x_{n})b_{j}\\|^{2} = \\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\displaystyle\\sum_{j=M+1}^{D}(b_{j}^{T}x_{n})^{2} = \\dfrac{1}{N}\\displaystyle\\sum_{n}\\displaystyle\\sum_{J}b_{j}^{T}x_{n}x_{n}^{T}b_{j} = \\displaystyle\\sum_{j=M+1}^{D}b_{j}^{T}\\left(\\dfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}x_{n}x_{n}^{T}\\right)b_{j} = \\displaystyle\\sum_{j=M+1}^{D}b_{j}^{T}Sb_{j} = trace\\left[\\left(\\displaystyle\\sum_{j=M+1}^{D}b_{j}^{T}b_{j}\\right)S\\right]$\n",
    "\n",
    "Example\n",
    "- $J = b_{2}^{T}b_{2}, b_{2}^{T}b_{2} = 1$\n",
    "- Lagrange $L = b_{2}^{T}Sb_{2} + \\lambda(1 - b_{2}^{T}b_{2})$\n",
    "- $\\dfrac{\\partial L}{\\partial \\lambda} = 1 - b_{2}^{T}b_{2} = 0$\n",
    "- $\\dfrac{\\partial L}{\\partial b_{2}} = 2b_{2}^{T}S - 2\\lambda b_{2}^{T} = 0$\n",
    "- $Sb_{2} = \\lambda b_{2}$\n",
    "- Now, $J = b_{2}^{T}Sb_{2} = b_{2}^{T}b_{2}\\lambda = \\lambda$\n",
    "- Thus, $J = \\displaystyle\\sum_{j=M+1}^{D}\\lambda$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
